Contexto engenharia de prompt




Engenharia de Prompts para Ciência de Dados Instituto InfNet Institucional Reitor Prof. Eduardo Ramos eduardo.ramos@infnet.edu.br
 Coordenador do Curso Prof. Fernando Ferreira fernando.gferreira@prof.infnet.edu.br
 Institucional DSc. Thiago Ciodaro Xavier Pós doutor em engenharia elétrica, ênfase em inteligência computacional pela COPPE/UFRJ. Mais de 10 anos de experiência em desenvolvi- mento de projetos de IA em pesquisa e consultorias (MJV, EY). Líder de pesquisa em IA pelo NetLAB-UFRJ e professor de advanced analytics do Instituto Infnet. https://www.linkedin.com/in/tciodaro/
 Como vai ser o curso? ? Competências ? Explicar o que é Inteligência Artificial Generativa e os Modelos Grandes de Linguagem (LLMs) ? Gerar textos a partir de técnicas com LLMs usando Prompt Engineering ? Utilizar técnicas avançadas de Prompt Engineering ? Criar soluções a partir de Prompt Engineering ? Utilizar técnicas Prompt Engineering para gerar imagens ? Total de 9 etapas e 18 aulas ? Cada aula possui 1.5h ? Total de 2 aulas por semana ? Estudo extra por semana ? Leitura de capítulos da bibliografia ou; ? Implementação de tutoriais ? Assessment ? Desenvolvimento de trabalho prático com os conhecimentos adquiridos no curso. Setup do Ambiente ? Anaconda: distribuição científica para Python ? Git: controle e versionamento de códigos ? JupyterLab: desenvolvimento de notebooks ? Streamlit: desenvolvimento de apps ? LangChain: desenvolvimento de aplicações com LLM ? HuggingFace: comunidade de LLMS ETAPA 1 Introdução à inteligência artificial generativa e modelos de linguagem grande para Ciência de Dados O que é Inteligência Artificial Generativa O que é Inteligência Artificial Generativa Criatividade Mensagens de textos criadas segundo as características especificadas pelo usuário, podendo resumir arquivos e artigos em tópicos. Flexibilidade para caracterizar o texto desejado para uso geral em diversas aplicações. Imagens Mensagens de textos criadas segundo as características especificadas pelo usuário, podendo resumir arquivos e artigos em tópicos. Flexibilidade para caracterizar o texto desejado para uso geral em diversas aplicações. Contextos Elaboração de textos baseados em bibliografia específica de textos e imagens, contextualizando, assim, perguntas, assuntos e tópicos que serão apresentados a diferentes públicos. Moderação Análise de sentimento sobre as respostas e textos gerados segundo as políticas de conteúdo da openAI, auxiliando na implantação de regras de governança e compliance. 1943: Warren McCulloch e Walter Pitts - "A Logical Calculus of the Ideas Immanent in Nervous Activity" O artigo introduziu o conceito de neurônio artificial, um modelo computacional básico que lançou as bases para redes neurais. O modelo é conhecido por representar operações lógicas com circuitos neurais simples. 1958: Frank Rosenblatt - "The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain" Rosenblatt apresentou o Perceptron, um dos primeiros algoritmos a tentar aprendizado de máquina com base no modelo de neurônio de McCulloch-Pitts. O Perceptron demonstrou como as máquinas poderiam classificar entradas de forma linear, mas não conseguia resolver problemas não lineares, como o problema do XOR. 1969: Marvin Minsky e Seymour Papert - "Perceptrons" O livro destacou as limitações do Perceptron em resolver problemas não lineares, como o problema do XOR, o que levou a uma diminuição do interesse por redes neurais, um período conhecido como "o inverno da IA". 1986: David E. Rumelhart, Geoffrey E. Hinton e Ronald J. Williams - "Learning Representations by Back-propagating Errors" Este artigo seminal introduziu o algoritmo de retropropagação (backpropagation), que permitiu o treinamento de redes neurais com múltiplas camadas, reativando o interesse pela IA e resolvendo o problema do XOR. 1998: Yann LeCun, Léon Bottou, Yoshua Bengio e Patrick Haffner - "Gradient-Based Learning Applied to Document Recognition" Introduziu as Redes Neurais Convolucionais (CNNs) com a arquitetura LeNet, utilizada para reconhecimento de escrita manual, marcando um marco importante no aprendizado profundo para dados visuais. 2006: Geoffrey Hinton, Simon Osindero e Yee-Whye Teh - "A Fast Learning Algorithm for Deep Belief Nets" Este artigo impulsionou a era moderna do aprendizado profundo ao demonstrar como o aprendizado não supervisionado poderia ser usado para treinar redes neurais profundas de forma eficiente. 2012: Alex Krizhevsky, Ilya Sutskever e Geoffrey Hinton - "ImageNet Classification with Deep Convolutional Neural Networks" Introduziu a arquitetura AlexNet, que superou significativamente os métodos anteriores na competição ImageNet e demonstrou o poder das GPUs no treinamento de modelos de aprendizado profundo. 2014: Ian Goodfellow et al. - "Generative Adversarial Nets" Propôs as Redes Adversárias Generativas (GANs), uma estrutura em que duas redes neurais (geradora e discriminadora) competem, levando à criação de dados sintéticos realistas. 2014: Dzmitry Bahdanau, Kyunghyun Cho e Yoshua Bengio - "Neural Machine Translation by Jointly Learning to Align and Translate" Introduziu o mecanismo de atenção, permitindo que modelos se concentrem em partes relevantes da sequência de entrada, o que melhorou significativamente as tarefas de tradução automática. 2017: Ashish Vaswani et al. - "Attention is All You Need" Introduziu a arquitetura Transformer, que usa mecanismos de auto-atenção e tornou-se a base para muitos dos modelos de última geração em PLN, incluindo o BERT e o GPT. 2018: Alec Radford et al. - "Improving Language Understanding by Generative Pre-Training (GPT-1)" Apresentou a primeira versão do modelo Generative Pre-trained Transformer (GPT), estabelecendo uma nova abordagem para compreensão de linguagem não supervisionada. 2020: Tom B. Brown et al. - "Language Models are Few-Shot Learners (GPT-3)" O GPT-3, um modelo com 175 bilhões de parâmetros, demonstrou o poder do aprendizado com poucos exemplos (few-shot learning), mostrando que grandes modelos de linguagem podem generalizar bem em várias tarefas de PLN sem treinamento específico para cada tarefa. 2023: OpenAI - Lançamento do GPT-4 O modelo GPT-4 apresentou desempenho aprimorado na compreensão de instruções complexas, geração de texto coerente e suporte para entradas multimodais (texto e imagem). O que é Inteligência Artificial Generativa BENEFÍCIOS Criação de Conteúdo: A IA generativa facilita a produção de textos, imagens, vídeos e músicas de maneira rápida e escalável. Automação Criativa: Permite que empresas e indivíduos automatizem processos criativos, otimizando tempo e recursos. Prototipagem Rápida: Acelera o desenvolvimento de ideias, como design de produtos e narrativas, antes de implementar versões finais. Personalização: Gera conteúdo adaptado às necessidades e preferências dos usuários, oferecendo experiências únicas e mais engajantes. Exploração de Novos Domínios: Auxilia na descoberta de novos padrões e tendências em áreas como arte, ciência e negócios. O que é Inteligência Artificial Generativa DESAFIOS Bias e Preconceitos: Algoritmos podem reproduzir e amplificar vieses existentes, resultando em conteúdo tendencioso ou discriminatório. Uso Indevido: Pode ser utilizado para criar deepfakes ou disseminar informações falsas e manipulação digital. Impacto no Trabalho: Potencial substituição de certas funções e profissões criativas. Controle e Transparência: Falta de clareza sobre como as IAs tomam decisões e geram conteúdo. Privacidade: Risco de utilização inadequada de dados pessoais para treinamento de modelos. O que é Inteligência Artificial Generativa O interesse por IA generativa explodiu desde outubro de 2022, graças ao lançamento do ChatGPT. Além disso, a Gartner prevê que, até 2025, o percentual de dados gerados por IA generativa representará 10% de todos os dados gerados. O que é Inteligência Artificial Generativa Modelos grandes de linguagem (LLMs) LFM (Large Foundation Models): ? Modelos de fundação que servem como base para várias tarefas de aprendizado de máquina e inteligência artificial. ? São treinados com grandes quantidades de dados de forma genérica e podem ser adaptados (finetuning) para resolver problemas específicos. ? Exemplos: modelos multimodais que entendem texto, imagem, e até áudio simultaneamente. LLM (Large Language Models): ? Subconjunto dos LFMs especializado em Processamento de Linguagem Natural (NLP). ? Projetados para lidar com texto e gerar linguagem natural. ? Capazes de compreender, gerar e interagir em linguagem humana. ? Exemplos: GPT-3, GPT-4, BERT, T5. Modelos grandes de linguagem (LLMs) LFM (Large Foundation Models): ? Treinado para múltiplas modalidades e propósitos. ? Capaz de ser utilizado como um ponto de partida para diversas aplicações (visão computacional, NLP, aprendizado multimodal). ? Flexível e genérico. LLM (Large Language Models): ? Focado exclusivamente em texto. ? Excelente em compreensão e geração de linguagem natural. ? Pode ser utilizado como um componente de um LFM em tarefas específicas de linguagem. Modelos grandes de linguagem (LLMs) Os Modelos de Linguagem Grande (LLMs) são redes neurais profundas especializadas na compreensão e geração da linguagem humana, fundamentais em áreas como criação de conteúdo e Processamento de Linguagem Natural (PLN), onde o objetivo é desenvolver algoritmos capazes de entender e gerar texto em linguagem natural. Esses modelos são treinados principalmente através de aprendizado não supervisionado em grandes quantidades de dados textuais, permitindo-lhes aprender padrões e estruturas linguísticas de forma eficaz. Ilustração: Brasil Escola. O que é um Neurônio? Modelos grandes de linguagem (LLMs) A atual geração de LLMs, como o GPT-4, utiliza arquiteturas de rede neural baseadas no modelo de transformadores. A força notável desses modelos reside na sua capacidade de funcionar como interfaces conversacionais, como chatbots, gerando respostas coerentes e contextualmente apropriadas em conversas abertas. Este avanço na modelagem de linguagem e no PLN é amplamente dependente da qualidade do aprendizado de representações, onde o modelo codifica informações sobre os textos nos quais foi treinado e gera novos textos baseando-se no que aprendeu. Modelos grandes de linguagem (LLMs) Avaliação de LLMs ? Importância dos Benchmarks: ? Os benchmarks que capturam o desempenho em tarefas de diferentes domínios são fundamentais para o desenvolvimento de LLMs. ? Eles oferecem uma maneira padronizada de avaliar o desempenho multitarefa e as capacidades amplas dos LLMs. Modelos grandes de linguagem (LLMs) O que é MMLU? ? Massive Multitask Language Understanding (MMLU): ? É um conjunto abrangente composto por 57 tarefas que abrangem diversos domínios, como matemática, história, ciência da computação e direito. ? Avalia LLMs em configurações de zero-shot (sem exemplos prévios) e few-shot (com poucos exemplos). Modelos grandes de linguagem (LLMs) Exemplos de bases de dados públicas para MMLU GLUE (General Language Understanding Evaluation) ? GLUE é um benchmark composto por uma coleção de recursos de dados que testam as capacidades de compreensão de linguagem de um modelo em múltiplas tarefas, como análise de sentimento e inferência textual. SuperGLUE ? SuperGLUE foi projetado como um benchmark mais difícil e abrangente que o GLUE original, incluindo tarefas mais desafiadoras como o raciocínio de causa e efeito, compreensão de leitura mais complexa e raciocínio lógico. CoLA Modelos grandes de linguagem (LLMs) Exemplos de bases de dados públicas para MMLU SQuAD (Stanford Question Answering Dataset) ? SQuAD é um conjunto de dados popular para avaliar o desempenho de modelos de compreensão de leitura. Contém perguntas feitas por humanos baseadas em um conjunto de artigos da Wikipedia, onde o modelo deve fornecer a resposta correta extraída do texto. https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Amazon\_rainforest.html
 Processo de pré-treinamento de LLMs. Modelos Auto-regressivos (AR): são usados para modelar e prever sequências de dados onde valores futuros são assumidos como funções lineares dos valores anteriores na série. Exemplos comuns incluem previsões de demanda de produtos, meteorológicas e preços de ações. Aplicação em Linguagem: Em linguagem, modelos auto-regressivos preveem a próxima palavra ou token com base nas palavras anteriores na sequência de texto. Processo de pré-treinamento de LLMs. Modelos Sequenciais: processam e geram dados considerando a ordem em que os dados aparecem. São amplamente usados em tarefas que envolvem dependências temporais ou sequenciais como reconhecimento de fala, tradução automática e geração de texto. Aplicações em IA Generativa: Modelos sequenciais são fundamentais para IA generativa, onde a criação de conteúdo novo depende das informações precedentes. Processo de pré-treinamento de LLMs. Modelos Autoencoders: são uma classe de redes neurais usadas para aprendizado não supervisionado. Eles aprendem a codificar os dados de entrada em uma representação compacta e depois decodificar essa representação de volta ao formato original. O Encoder comprime os dados de entrada em um espaço latente menor (codificado), cuja função é capturar as características essenciais dos dados, enquanto o Decoder reconstrói os dados a partir da representação comprimida. https://medium.com/@etorezone/differences-between-autoencoders-and-principal-component-analysis-pca-in-dimensionality-reduction-ca5f24364054
 https://www.researchgate.net/figure/Comparison-between-PCA-and-Autoencoder-15\_fig1\_340049776
 Codificando texto Corpus: um grande conjunto de textos que será utilizado como fonte de dados para análise ou treinamento de modelos de linguagem. Um corpus pode ser formado por milhares de artigos de notícias, livros ou conversas de redes sociais. Serve como base para a construção do vocabulário e a compreensão das relações entre palavras. Vocabulário: coleção de todas as palavras distintas presentes no corpus. Define quais palavras serão reconhecidas e codificadas pelo modelo. Processo de pré-treinamento de LLMs. corpus vocabulário vetores Machado de Assis Word2Vec é um modelo de aprendizado de palavras que transforma cada palavra em um vetor de números em um espaço vetorial contínuo. Ele utiliza redes neurais para aprender representações distribuídas de palavras com base em seu contexto. Ideia Principal: Palavras com significados semelhantes estão mais próximas no espaço vetorial. Por exemplo, "gato" e "cachorro" terão representações vetoriais mais próximas do que "gato" e "carro". Processo de pré-treinamento de LLMs. https://swatimeena989.medium.com/training-word2vec-using-gensim-14433890e8e4
 word2vec EXPLICAR Processo de pré-treinamento de LLMs. MIKOLOV et al. Distributed Representations of Words and Phrases and their Compositionality. Principais Parâmetros de Treinamento: Vector Size: Determina o número de dimensões do vetor de palavras (embedding). Window Size: Define quantas palavras à esquerda e à direita da palavra-alvo o modelo deve considerar para o contexto. Min Count: Filtra palavras raras que aparecem menos vezes do que o limite definido. SG (Skip-Gram ou CBOW):Escolhe o método de treinamento: ? Skip-Gram (prever contexto a partir da palavra-alvo). ? CBOW (Continuous Bag of Words, prever a palavra-alvo a partir do contexto). Processo de pré-treinamento de LLMs. Skip-gram é eficaz para capturar informações sobre palavras menos frequentes, pois ele treina diretamente as relações entre a palavra-alvo e cada palavra do contexto. CBOW tende a ser mais eficiente em termos de treinamento e funciona bem para palavras mais frequentes, já que ele faz uma predição a partir de um conjunto de palavras em vez de fazer predições separadas para cada palavra de contexto. Processo de pré-treinamento de LLMs. Transformers: são modelos neurais de arquitetura complexa, estruturados em um codificador e um decodificador capazes de identificar palavras segundo o seu contexto. Se diferenciam em relação a memória e entendimento de contexto devido a como cada arquitetura lida com o contexto sequencial e as dependências a longo prazo. Enquanto o codificador aprende aspectos da linguagem e dos contextos das palavras, considerando os termos antes e depois, o decodificador aplica uma máscara nos termos para garantir que a amostra prevista dependa somente das amostras anteriores. Processo de pré-treinamento de LLMs. LSTMs têm uma dependência sequencial que, embora a informação de estados anteriores seja considerada para a previsão futura, longas sequências sofrem com problemas como gradient vanishing. Diferente do LSTM, que processa a sequência passo a passo, o Transformer processa todas as palavras da sequência simultaneamente (paralelamente), o que melhora significativamente a eficiência e permite capturar melhor as relações entre palavras distantes. Processo de pré-treinamento de LLMs. Codificação Posicional: como o transformer não processa a informação de forma sequencial, a posição dos termos na sequência é codificada no próprio embeddings. Normalização por Camada: Estabiliza o aprendizado normalizando as entradas ao longo das características, melhorando a velocidade e a estabilidade do processo de otimização. Atenção Multi-Head (MHA): Aplica o mecanismo de atenção várias vezes em paralelo, capturando diferentes tipos de informações para uma representação mais rica. Mecanismo de Atenção: Calcula uma soma ponderada (vetor de contexto) dos valores nas posições de entrada, com base na similaridade, permitindo o foco seletivo em partes relevantes da entrada. Processo de pré-treinamento de LLMs. Mecanismo de atenção: permite que o modelo "preste atenção" em diferentes palavras (ou tokens) da sequência, dando mais ou menos peso a elas, dependendo de sua relevância para a tarefa em questão. Isso significa que o modelo pode identificar quais palavras ou partes do texto são mais importantes para entender o significado global ou para prever a próxima palavra. Query (Q): Representa a palavra/tokens para a qual estamos calculando a atenção. Key (K): Representa todas as palavras/tokens na sequência de entrada, que servirão como "chaves" para comparação com a query. Value (V): Representa os valores das palavras/tokens que serão usados para calcular o "content vector", ou seja, as informações que o modelo vai usar de fato. Processo de pré-treinamento de LLMs. Saída do LLM: o modelo transformer estima a probabilidade de cada token dado uma sequência de tokens de entrada. A decisão de qual token utilizar é estocástica, impedindo que somente os tokens mais prováveis sejam selecionados. Temperatura: parâmetro que controla a estocasticidade da resposta. Quanto menor for a temperatura, mais determinística é a resposta, enquanto valores elevados auxiliam na criatividade do modelo, aumentando a chance de selecionarmos termos menos prováveis. Processo de pré-treinamento de LLMs. “O rei do futebol se chama:“ Pelé Maradona Zidane … Biro-Biro Temperatura 1 Pelé Maradona Zidane … Biro-Biro Temperatura 2 Probabilidades https://www.youtube.com/watch?v=wjZofJX0v4Mhttps://www.youtube.com/watch?v=eMlx5fFNoYc
 3Blue1Brown Embeddings em LLMs Embeddings são representações vetoriais densas de palavras ou itens, usadas para capturar relações semânticas. Ao contrário de representações esparsas (one-hot), embeddings mapeiam palavras para um espaço de menor dimensão, preservando a similaridade semântica. Palavras que aparecem em contextos semelhantes terão embeddings semelhantes, mesmo que não sejam sinônimos exatos. Esses modelos são capazes de identificar: ? Relações semânticas amplas (ex. “rei” e “rainha”), ? Hierarquias conceituais (ex. “fruta” e “maçã”), ? Dependências contextuais (ex. “corre” e “rápido”). https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526
 Embeddings em LLMs Transferência de Conhecimento: Embeddings treinados em tarefas gerais podem ser ajustados para aplicações mais específicas através de fine-tuning. Um modelo genérico de linguagem pode ser adaptado para entender termos médicos ou jurídicos. Contextos Específicos: Ao adaptar embeddings para contextos particulares, como análises financeiras ou diagnósticos médicos, as representações genéricas podem ser especializadas para focar em padrões e termos específicos dessas áreas.






ETAPA 2 Explorando modelos grandes de linguagem (LLMs) Explorando modelos grandes de linguagem (LLMs) Objetivos da Etapa ? Tokenização em LLMs ? Explicar critérios de escolha de LLMs ? Comparar LLMs proprietários e open-source ? Analisar trade-offs de tamanho x desempenho ? Avaliar desafios éticos e limitações Tokenização e LLMs Tokenização de Textos: Processo de dividir um texto em unidades menores (tokens), como palavras, subpalavras ou caracteres, para processamento em modelos de aprendizado de máquina. Tipos de Tokenização por Palavra: cada palavra do texto é considerada um token. Útil para análise de palavras, como frequência de palavras e modelos de bag-of-words. ? Aplicações: Modelos clássicos de NLP, como TF-IDF ou n-grams. “O gato está dormindo” ["O", "gato", "está", "dormindo"] Tokenização e LLMs Tokenização por Caracteres: cada caractere do texto é tratado como um token. Funciona bem com textos curtos, emojis ou linguagens artificiais. ? Aplicações: processamento de texto em linguagens com sistema de escrita complexo (como chinês). Útil para tarefas que envolvem correção ortográfica ou quando palavras não estão claramente definidas. "caminhando" [‘c’,’a’,’m’,’i’,’n’,’h’,a’,’’n’,’d’,’o’] Tokenização e LLMs Tokenização por Sentença: descrição: Cada frase é tratada como um token. Captura o contexto de uma frase completa, útil para análises de entonação e estrutura de texto. ? Aplicações: modelos de sumarização, análise de coesão textual, e sistemas que precisam processar sentenças inteiras, como na tradução automática. Um dia fomos. No outro, não! [‘Um dia fomos.’, ‘No outro, não!’] Byte-Pair Encoding - BPE: divide palavras em pedaços menores, como prefixos e sufixos, formando subpalavras. Reduz o tamanho do vocabulário, capturando morfologia de palavras, e é eficiente para lidar com palavras desconhecidas ou raras. ? Aplicações: amplamente usada em modelos de linguagem avançados, como o BERT e o GPT. Indicada para traduções automáticas, geração de texto e qualquer tarefa que lide com palavras compostas. Tokenização e LLMs “O gato está dormindo” ["O", " g”, ”ato", " est”, ”á", " dorm”, “ind”, “o"] https://platform.openai.com/tokenizer
 Critérios para Escolher um LLM Requisitos de Tarefa: o critério essencial ao escolher um LLM é entender qual tarefa específica o modelo precisará executar pois, a depender do objetivo, diferentes modelos podem ser mais adequados. ? Processamento de Linguagem Natural: Modelos como GPT-4 ou BERT são amplamente usados para tarefas como análise de sentimentos, classificação de textos e extração de informações. ? Resumo de Textos: Se a tarefa é resumir grandes quantidades de texto, como relatórios ou artigos, modelos treinados para compressão de informações são ideais. ? Tradução: Modelos como M2M-100 ou T5 são otimizados para tradução automática e oferecem precisão elevada para múltiplos idiomas. ? Geração de Código: Codex (baseado em GPT) é um exemplo de modelo especializado em geração e correção de código de programação. GPT-3 https://ar5iv.labs.arxiv.org/html/2107.03374
 Critérios para Escolher um LLM Capacidade de Personalização: dependendo da aplicação, pode ser necessário ajustar um LLM pré-treinado (ou fine-tuned) para otimizar a performance em tarefas específicas. ? Modelos Base: são treinados em grandes volumes de dados gerais, o que os torna versáteis e prontos para uso imediato. Podem lidar com uma variedade de tarefas sem necessidade de ajustes adicionais. ? Modelos Fine-Tuned: aplicação exige resultados muito específicos, comum em cenários onde a linguagem utilizada é técnica ou especializada, como em áreas médicas ou jurídicas. Fine-tuning permite que o modelo aprenda características particulares de um domínio ou estilo, aumentando sua precisão e relevância. https://www.researchgate.net/figure/Top-five-languages-included-in-GPT-3-training-data-compared-against-other-measures-of-the\_tbl1\_359256884
 Orçamento: o custo de utilizar um LLM pode variar bastante dependendo do modelo e da infraestrutura escolhida. ? APIs pagas: paga-se por chamadas ao modelo, ideal para aplicações que precisam de grande poder computacional sem a necessidade de construir e manter a infraestrutura própria (GPT-4 e Gemini). Custos podem ser significativos dependendo do volume de uso. ? Treinamento Próprio: tarefas que necessitem de um LLM altamente customizado, o que requer poder computacional elevado e infraestrutura para lidar com o treinamento, possivelmente com a aquisição de hardware específico (GPUs) e consumo de energia. ? Equilíbrio de Custos: modelos open-source podem ser mais econômicos a longo prazo, porém com um custo inicial relativamente alto para treinar e manter o modelo. Critérios para Escolher um LLM https://llmpricecheck.com
 Modelos com Arquitetura MoE https://www.researchgate.net/figure/The-standard-MoE-architecture-The-outputs-classifications-from-the-classifier-networks\_fig10\_309296341
 Mixture of Experts - MoE: é uma técnica de aprendizado de máquina em que um modelo seleciona dinamicamente diferentes subconjuntos de "especialistas" (submodelos menores e especializados) para processar partes específicas de uma entrada, ponderadas através de um roteador para gerar a resposta final. Escalabilidade: permite a criação de modelos massivos sem aumentar proporcionalmente o custo computacional, pois apenas uma parte do modelo é ativada em cada momento. Eficiência Computacional: o modelo pode ser muito maior sem exigir um aumento linear nos recursos de processamento, permitindo o uso de LLMs com menor custo por inferência. Complexidade: requer um roteador eficiente para selecionar os especialistas corretos, o que aumenta a complexidade do treinamento. Balanceamento de carga: garantir que todos os especialistas sejam ativados de maneira equilibrada, evitando que o aprendizado seja enviesado. Modelos Open-Source (LLaMA, Falcon) ? Transparência: oferecem acesso total ao código-fonte, permitindo que desenvolvedores compreendam como o modelo foi treinado, ajustem a arquitetura e investiguem potenciais problemas de viés ou desempenho. ? Personalização: é possível ajustar o modelo exatamente para a sua aplicação, o que inclui treinamento adicional em dados específicos ? Custo-efetividade: o custo inicial de treinamento pode ser diluído em aplicações de longo prazo com alto volume de dados. ? Manutenção: usar e manter um modelo open-source requer mais esforço técnico, como a configuração de infraestrutura e ajustes constantes do modelo. ? Suporte Limitado: correções e fixes podem depender de comunidades de desenvolvedores que contribuem nos códigos, o que pode ser um desafio em situações críticas. LLM proprietários e open-source. Modelos Proprietários (GPT-4, Gemini) ? Performance Superior: tendem a ser líderes em benchmarks de desempenho, oferecendo alta precisão e melhor generalização em diversas tarefas. ? Suporte Técnico: empresas que desenvolvem esses modelos oferecem suporte técnico e atualizações constantes, garantindo que as soluções implementadas sejam robustas e seguras. ? Segurança: modelos proprietários geralmente têm medidas de segurança e alinhamento embutidas, como mitigação de vieses e controle sobre geração de conteúdo nocivo. ? Caixa-Secreta: código e dados usados para treinar esses modelos não são divulgados, o que os torna menos transparentes. ? Custos Elevados: custos significativos para aplicações em larga escala ou com altos volumes de chamadas. LLM proprietários e open-source. Modelos Proprietários Modelos Open-Source Desempenho Geral Suporte Técnico Custo Transparência Específico e Flexível Transparência Conhecimento Técnico Infraestrutura LLaMA - Large Language Model Meta AI foi projetado para ser uma alternativa aberta e altamente personalizável a modelos proprietários. ? Performance: suas versões mais recentes demonstraram excelente desempenho em benchmarks (MMLU), mostrando uma sólida compreensão de múltiplos idiomas e uma capacidade forte em raciocínio e análise de contexto. LLaMA 3 oferece um desempenho superior em tarefas de geração de código e tradução ? Transparência: O código-fonte e os pesos do modelo são acessíveis, permitindo personalização e adaptação do modelo para necessidades específicas ? Escalabilidade: o LLaMA pode ser ajustado de acordo com as necessidades, desde pequenas tarefas até implementações corporativas com modelos de até 70 bilhões de parâmetros Principais modelos de LLMs disponíveis Open-source https://datasciencedojo.com/blog/meta-llama-3-1/
 Falcon (TII AI): desenvolvido pelo Instituto de Inovação em IA (TII), foi projetado para ser um modelo de alto desempenho em tarefas de NLP, com foco em eficiência e rapidez, se destacando em contextos menores e de recursos limitados. ? Performance: supera muitos modelos maiores em benchmarks como commonsense reasoning e compreensão de leitura. Mesmo com um tamanho de modelo menor, compete diretamente com modelos maiores como LLaMA 2 e Mistral 7B. ? Eficiência Computacional: é otimizado para consumir menos recursos computacionais, permitindo que seja executado com eficiência em infraestruturas leves. ? Personalização e Flexibilidade: permite ajustes finos e pode ser adaptado para diferentes tarefas, tornando-o ideal para empresas e desenvolvedores que buscam um modelo balanceado entre custo e performance. Principais modelos de LLMs disponíveis - Open-source https://dataforest.ai/blog/falcon-ai-180-billion-parameters
 GPT-4 - modelo com alto desempenho em benchmarks de raciocínio, compreensão de texto e tarefas complexas, fornecendo respostas mais precisas e criativas em comparação com modelos anteriores. ? Capacidades Multimodais: versão avançada, o GPT-4o, também pode processar entradas de imagem, ampliando suas aplicações em áreas como análise visual e geração de legendas. ? Exemplos de Benchmark: ótimo desempenho em exames padronizados como o Bar Exam e o GRE. ? Aplicações Comuns: usado amplamente em chatbots, sistemas de recomendação e outras tarefas que exigem entendimento profundo e geração de texto. ? Desafios e Limitações: ainda pode apresentar alucinações (respostas incorretas) em cenários complexos e informações de eventos recentes. Principais modelos de LLMs disponíveis - Pagos https://arxiv.org/html/2303.08774v6
 GPT-4 https://arxiv.org/html/2303.08774v6
 Claude AI: é um LLM baseado em transformers, desenvolvido pela Anthropic, com foco em segurança e alinhamento da IA. Foi treinado com dados públicos da internet e dados proprietários, usando aprendizado não supervisionado, Reinforcement Learning with Human Feedback (RLHF), e uma técnica inovadora chamada Constitutional AI (CAI). CAI é uma técnica única desenvolvida pela Anthropic para melhorar a confiabilidade e a segurança da IA, com o objetivo de fazer com que o modelo seja útil, honesto e inofensivo, prevenindo respostas tóxicas, discriminatórias ou ilegais. Treinamento baseado em princípios: fontes como a Declaração Universal dos Direitos Humanos e melhores práticas de segurança. Principais modelos de LLMs disponíveis - pagos https://arxiv.org/abs/2212.08073
 Fase 1: O modelo aprende a criticar e revisar suas próprias respostas usando princípios de alinhamento, sem depender de feedback humano direto. Fase 2: O modelo utiliza feedback gerado por IA, baseado nos princípios, para selecionar saídas mais seguras e inofensivas. Gemini - da Google, é um dos modelos mais avançados em termos de multimodalidade, lidando não apenas com texto, mas também com imagens e som, permitindo sua aplicação em uma ampla gama de tarefas. ? Capacidades Multimodais: se destaca pela capacidade de processar dados de diversas fontes simultaneamente, oferecendo soluções em tarefas que combinam imagem, áudio e texto. ? Performance e Eficiência: oferece desempenho de alta qualidade em benchmarks relacionados à compreensão de linguagem e análise de conteúdo visual. ? Aplicações: assistentes virtuais avançados, processamento de imagens médicas e sistemas de análise multimídia. ? Desafios: o custo computacional e a complexidade para treinamento e manutenção. . Principais modelos de LLMs disponíveis - Pagos https://arxiv.org/pdf/2312.11805
 Trade-offs de tamanho x desempenho Tamanho de modelos: são medido principalmente pelo número de coeficientes ajustáveis durante o treinamento que determinam como o modelo processa as entradas. Esses parâmetros são responsáveis pela complexidade e capacidade de aprendizado do modelo. ? Generalização: LLMs grandes têm maior chance de lidar bem com tarefas de zero-shot e few-shot learning, ou seja, conseguem generalizar melhor mesmo com pouca ou nenhuma amostra específica de uma tarefa. ? Custo Computacional: o aumento no número de parâmetros resulta em um consumo muito maior de recursos computacionais, como memória e poder de processamento. ? Latência: modelos maiores tendem a ser mais lentos para gerar respostas em tempo real, o que pode impactar negativamente sua aplicação em sistemas que exigem respostas rápidas. https://learning.oreilly.com/library/view/generative-ai-with/9781835083468/Text/Chapter\_1.xhtml#\_idParaDest-26
 Trade-offs de tamanho x desempenho https://artificialanalysis.ai/
 Falta de Verdadeira Compreensão LLMs não têm entendimento real do que estão dizendo; sua geração de texto é baseada em padrões estatísticos. LLMs alucinam em matemática. Limitações dos modelos: Desafios éticos e limitações no uso LLMs Capacidade de Raciocínio e Factualidade Frequentemente "alucinam": em benchmarks como o TruthfulQA, o GPT-3 apresenta dificuldade em distinguir entre informações verdadeiras e falsas, gerando respostas factualmente incorretas. Viés e Discriminação Podem reproduzir vieses de gênero, raça e outros preconceitos, pois são treinados em grandes volumes de dados da web Alto Custo Computacional Modelos com bilhões de parâmetros demandam uma enorme quantidade de recursos computacionais para treinar e utilizar (consumo de energia e aquecimento global). Memória de Curto Prazo Têm uma janela limitada de contexto (tokens). Em aplicações como tradução de documentos longos ou respostas contextuais em conversas complexas, o modelo perde informações de interações anteriores, prejudicando a coerência (GPT3: 4096). Falta de Transparência: Seus processos internos são complexos, opacos e difíceis de interpretar, proibitivo, em áreas sensíveis, como saúde e justiça. Desafios éticos e limitações no uso de LLMs Alinhamento e Viés: os dados utilizados no treinamento dos modelos, frequentemente, contêm vieses históricos e sociais. Esses vieses podem ser reproduzidos nas saídas dos modelos, levando a resultados prejudiciais ou discriminatórios. Podemos caracterizar 5 fontes de vieses em NLP: ? Design de Pesquisa ? Processo de anotação humana ? Dados ? Modelos ? Representação dos dados https://compass.onlinelibrary.wiley.com/doi/10.1111/lnc3.12432
 Desafios éticos e limitações no uso de LLMs Curadoria de Dados: seleção cuidadosa de dados é essencial para evitar que vieses presentes nos dados de treinamento sejam replicados nos modelos. Reduzir esses vieses implica revisar e editar grandes volumes de dados para garantir que eles sejam representativos e justos. Ajuste Fino com Feedback Humano: Reinforcement Learning with Human Feedback (RLHF) é uma estratégia de recompensa que usa preferências humanas para guiar o modelo na direção de saídas mais alinhadas aos valores humanos, evitando resultados prejudiciais ou inadequados. Supervisão Humana: a IA deve complementar o julgamento humano, e não substituí-lo, especialmente em contextos de sensíveis, como saúde e justiça. A confiança cega em modelos de IA pode levar a decisões automatizadas erradas ou enviesadas, com consequências graves (maiores riscos). Impacto Ambiental: treinar grandes modelos de IA consome enormes quantidades de energia, o que pode contribuir para o aquecimento global (carbon footprint). IA deve ser desenvolvida de forma responsável, explorando alternativas mais eficientes em termos de energia, como otimização de hardware e técnicas de treinamento mais econômicas. Desafios éticos e limitações no uso de LLMs Desafios éticos e limitações no uso de LLMs Responsible AI: envolve o desenvolvimento e uso de sistemas de IA de forma ética e transparente, garantindo segurança, equidade, e privacidade. Visa minimizar riscos, como vieses, falta de transparência e danos sociais, promovendo IA que seja confiável e centrada em humanos. ? Nível de Modelo: utilização de dados livres de vieses para evitar saídas discriminatórias e ajustes finos para garantir que suas saídas sigam princípios éticos e normas de segurança. ? Nível de Metaprompt: definir prompts e regras explícitas para moldar as respostas da IA, promovendo alinhamento com valores éticos e regulatórios. ? Nível da Interface com o Usuário: incorporar mecanismos que permitam aos usuários fornecer feedback e ajustar as saídas do modelo em tempo real, garantindo transparência e controle humano. Desafios éticos e limitações no uso de LLMs Metaprompt: é a mensagem ou instrução base associada ao LLM, que guia o comportamento do modelo em uma aplicação, definindo como o modelo deve operar para cumprir tarefas específicas de maneira adequada. ? Diretivas claras: seja prolixo com o LLM ? Transparência: franqueza em relação às limitações ? Embasamentos: confrontar as respostas do LLM com fatos Prompt injection: ataques maliciosos para manipular ou explorar o comportamento de um LLM, alterando o conteúdo dos prompts para ignorar as diretrizes de segurança, induzindo o sistema a gerar saídas mal intencionadas, incorretas ou perigosas. ? Vazamento de prompt (direto): o meta prompt é alterado ? Sequestro de Objetivo (indireto): prompts específicos que conseguem contornar a diretivas de segurança. (DAN) https://www.lakera.ai/blog/guide-to-prompt-injectionhttps://github.com/0xk1h0/ChatGPT\_DAN
 Desafios éticos e limitações no uso de LLMs https://www.lakera.ai/blog/guide-to-prompt-injection
 Desafios éticos e limitações no uso de LLMs https://www.reddit.com/r/LocalLLaMA/comments/186inwr/chatgpt\_leaks\_training\_data\_when\_asked\_to\_repeat/?rdt=51969
 Desafios éticos e limitações no uso de LLMs https://arxiv.org/html/2303.08774v6https://arxiv.org/pdf/2403.05530
 Gemini GPT-4 Desafios éticos e limitações no uso LLMs https://craigclouditpro.wordpress.com/2024/03/06/microsoft-responsible-ai-principles/
 Microsoft Responsible AI: um dos frameworks mais avançados, promovendo a construção de IA de forma justa, segura, transparente e privada., de acordo com alguns princípios ? Justiça: Garantir que a IA não cause discriminação ou viéses. ? Confiabilidade e Segurança: Proteger sistemas de IA contra falhas e ataques. ? Privacidade e Segurança: Manter a confidencialidade dos dados dos usuários. ? Inclusão: Assegurar que a IA seja acessível a todos os públicos. ? Transparência: Explicar como e por que as decisões da IA são tomadas. ? Responsabilidade: Definir claramente quem é responsável pelos resultados gerados por sistemas de IA




ETAPA 3 Introdução à criação e manipulação de Prompts Prompt para interação com LLMs Engenharia de Prompt é o processo de construção de instruções eficazes para LLMs, que dependem de como são formuladas para fornecer respostas precisas e relevantes. Um prompt bem definido pode guiar o modelo a realizar tarefas complexas. Prompts são a interface que permite interações dinâmicas com os LLMs, e pequenas mudanças na formulação de um prompt podem alterar drasticamente o resultado. A construção adequada desses prompts é crucial para garantir que o modelo atenda às expectativas e realize a tarefa. ? LLMs e sua dependência de prompts. ? Impacto de prompts bem estruturados. ? Importância da clareza e objetividade nos prompts. Prompt para interação com LLMs A forma como o prompt é estruturado tem um impacto direto na qualidade e relevância da resposta gerada. Prompts bem formulados são a chave para maximizar o potencial dos LLMs. ? Prompts como interface de instrução. ? Impacto da clareza e especificidade nos resultados. ? Exemplos de bons e maus prompts. Prompt para interação com LLMs Elementos de um Prompt Descrição da Tarefa (Instruções) A instrução, ou objetivo de um prompt, precisa ser bem definida para guiar o LLM na realização correta da tarefa. O objetivo deve ser explícito, permitindo que o modelo saiba exatamente o que você quer que ele faça. Isso pode variar de uma tarefa simples como "listar" a uma mais complexa, como "analisar" ou "classificar". Especificar o objetivo claramente melhora a eficiência da interação com o modelo, minimizando respostas erradas ou imprecisas. O LLM pode realizar múltiplas tarefas, e o objetivo do prompt garante que ele siga a direção correta. Exemplos: 1. Retail: "Liste três estratégias de marketing digital que poderiam aumentar as vendas de produtos eletrônicos durante a Black Friday." 2. Travel: "Resuma as principais atrações turísticas de Paris que os turistas costumam visitar em menos de 100 palavras." 3. Payments: "Classifique os principais métodos de pagamento online de acordo com sua popularidade na Europa." Elementos de um Prompt Exemplos: 1. “Como um especialista em educação, descreva o impacto da inteligência artificial nas escolas” Informação de Fundo (Persona, Contexto) O contexto e a persona fornecem informações de fundo para o LLM, ajudando a situar a resposta dentro de um cenário ou perspectiva específica. O contexto define o ambiente ou a situação, enquanto a persona pode ajustar o tom ou estilo da resposta com base em quem está respondendo. ? Contexto: Define o ambiente. ? Persona: Ajusta o estilo e tom da resposta. Elementos de um Prompt Exemplos: 1. ‘“A IA está transformando a educação ao permitir aulas personalizadas…’. Agora, faça um resumo semelhante sobre saúde.” Exemplos O uso de exemplos é uma técnica poderosa em prompting, fornecendo ao modelo uma referência clara do tipo de resposta esperada. Ao incluir exemplos de boa resposta, o LLM pode gerar respostas mais consistentes e alinhadas às expectativas do usuário. ? Referências claras para respostas ideais. Elementos de um Prompt Output Specifications (Formato, Tom, Restrições) Especificar o formato, tom e restrições da resposta garante que o modelo produza uma saída no estilo correto. Você pode pedir ao modelo para responder em um formato de lista, em um tom formal ou dentro de certas limitações, como "resposta curta" ou "até 150 palavras". ? Formato: Estrutura da saída (lista, parágrafo). ? Tom: Formal, informal, técnico. ? Restrições: Limitar tamanho ou detalhes da resposta. Exemplos: 1. "Liste em três pontos os benefícios da IA na saúde, usando um tom formal" Elementos de um Prompt Syntax (Delimitadores) O uso de delimitadores no prompt ajuda a estruturar o texto e definir partes da instrução. Delimitadores como aspas, colchetes ou linhas específicas podem restringir ou organizar o conteúdo, permitindo que o modelo compreenda melhor a divisão de dados e áreas de foco. ? Delimitadores: Organizadores de texto no prompt. Exemplos: 1. "Resuma o seguinte texto: 'A IA está revolucionando...'" Elementos de um Prompt Input Data (Dados) A entrada de dados é fundamental para que o modelo processe informações específicas. Ao fornecer dados detalhados ou referências, o prompt pode ser focado em uma fonte de dados específica, permitindo uma resposta mais precisa. Isso é importante em tarefas que envolvem análises de informações específicas. ? Dados de entrada: Informações específicas a serem analisadas. Exemplos: 1. " ### Ano; Categoria; Vendas 2022; Roupas; 20023 2022; Mercado; 2312 2022; Banho; 19231 ### Baseado nos dados de vendas de 2022, quais são as três categorias de produtos que mais cresceram no varejo?" Elementos de um Prompt Approach (Técnica) O método ou técnica utilizados no prompt influenciam como o modelo deve abordar a solução. Métodos como análise, comparação ou classificação podem ser explicitamente mencionados, orientando o modelo a seguir uma estratégia específica na geração da resposta. ? Técnica: Método para realizar a tarefa (análise, comparação). Exemplos: 1. "Compare os prós e contras de usar IA em diagnósticos médicos" Prompt para interação com LLMs Prompts simples são comandos curtos e diretos que permitem uma interação rápida com os LLMs, resultando em respostas instantâneas e objetivas. Esses prompts são úteis para tarefas mais básicas, como listar itens, responder perguntas diretas ou fornecer descrições curtas. ? Prompt direto vs. prompt detalhado. ? Como especificidade pode melhorar a resposta. Exercícios 1) Testar exemplos com diferentes elementos de prompt 2) Comparar resposta com e sem os elementos. a) Dados de entrada e Formato b) Contexto e Exemplo c) Delimitador e Exemplo d) Persona, Restrições, Tom, e Técnica 3) Usar o Poe.com https://llmnanban.akmmusai.pro/Introductory/Prompt-Elements/
 https://medium.com/@Ivan\_S/prompt-engineering-mastering-the-art-of-guiding-ai-responses-d75072d549e9
 https://blog.tobiaszwingmann.com/p/5-principles-for-writing-effective-prompts
 https://www.theaioptimist.com/p/what-is-ais-perfect-question-prompting
 https://www.aiforeducation.io/ai-resources/the-five-s-model-students
 Princípios do prompting para IA Diversos princípios devem ser aplicados para a produção de prompts eficientes que gerem respostas precisas. Diferentes formas de organizar esses princípios são propostas por engenheiros e autores, com informações comuns e critérios sobrepostos. Princípios do prompting para IA 1: Direcione Um dos principais fatores para gerar uma resposta precisa é fornecer uma direção clara no prompt. Isso significa especificar com detalhes o que se espera da resposta do modelo. ? Instrução clara define as expectativas. ? Maior precisão e relevância nas respostas. ? Especificidade melhora a qualidade das saídas. Exemplos: 1. "Sugira um nome para um produto" 2. "Sugira um nome criativo para um sapato de corrida unissex, confortável e moderno, voltado para o público jovem que pratica esportes ao ar livre" Princípios do prompting para IA 2: Formate A especificação de formato em um prompt é essencial para garantir que a resposta do modelo seja gerada no estilo correto e no formato esperado. ? Formatos comuns: listas, parágrafos, JSON, YAML. ? Controle sobre a estrutura da resposta para melhor integração e processamento. Exemplos: 1. "Liste 5 frutas tropicais" 2. "Liste 5 frutas tropicais em formato JSON" 3. “Enumere os princípios básicos de IA” 4. “Retorne sobre quais pessoas a matéria está mencionando em formato JSON” Princípios do prompting para IA 3: Exemplifique O uso de exemplos em prompts, conhecido como few-shot prompting, é uma técnica que permite ao modelo entender melhor o padrão de resposta esperado, guiando-o a gerar saídas mais consistentes e alinhadas ao que o usuário deseja. Em vez de apenas dar uma instrução, o prompt também fornece exemplos claros de respostas corretas ou desejáveis, para que o modelo siga essa estrutura e formato. ? Few-shot prompting melhora a consistência das saídas. Exemplos: 1. "Aqui está um exemplo de nome de produto: ‘iPrompt'. Agora, gere três nomes de produtos tecnológicos inovadores" 4: Avalie Avaliar o resultado da resposta de cada prompt segundo sua coerência, precisão e relevância para o objetivo da tarefa. Isso pode ser feito tanto de forma subjetiva, através da análise humana, quanto com o uso de métricas automáticas: ? BLEU (Bilingual Evaluation Understudy) mede a similaridade entre a resposta gerada pelo modelo e uma resposta de referência. Quanto mais próximo o texto gerado estiver da referência, maior será a pontuação BLEU. ? ROUGE (Recall-Oriented Understudy for Gisting Evaluation) mede a recuperação de informações na resposta gerada. É amplamente utilizada para avaliar resumos de texto, medindo a quantidade de sobreposição entre as palavras e frases do resumo gerado e o de referência. Princípios do prompting para IA https://clementbm.github.io/theory/2021/12/23/rouge-bleu-scores.html
 https://ai.plainenglish.io/when-to-use-bleu-score-evaluating-text-generation-with-n-gram-precision-3431829a641e
 BLEU ROUGE BLEU Princípios do prompting para IA ROUGE-1 Princípios do prompting para IA ROUGE-L 5: Divida Quando um prompt solicita uma tarefa muito complexa, há o risco de o modelo gerar uma resposta vaga ou imprecisa. Para lidar com isso, é útil dividir tarefas complexas em partes menores, criando subtarefas que o modelo pode resolver em etapas. ? Divisão de tarefas em partes gerenciáveis. Princípios do prompting para IA Exemplos: 1. "Gere seções de um relatório (introdução, corpo, conclusão) em etapas sobre aquecimento global”. Listas hierárquicas utilizando prompts Prompts que criam dados complexos em múltiplos níveis, gerando listas hierárquicas. Destaque para a importância de prompts bem estruturados, utilizando palavras como "hierárquico" ou "detalhado" para guiar o modelo a produzir respostas mais organizadas e profundas. ? Use descrições claras e específicas nos prompts para listas complexas. ? Combine instruções como "detalhado" ou "hierárquico" com o contexto desejado. ? Essa técnica é útil em contextos como planos de negócios, estratégias de marketing e organização de projetos. ? Passo intermediário para criação de estruturas e projetos mais complexos. Dados em formatos JSON e YAML Em ciência de dados, gerar saídas estruturadas, como JSON (JavaScript Object Notation), é essencial para muitas aplicações, incluindo APIs e manipulação de dados. Gerar respostas em formato JSON via prompts permite a automação de tarefas como o preenchimento de formulários ? Utilidade de JSON em integração de sistemas. ? Como prompts específicos geram saídas estruturadas e personalizadas. O YAML (YAML Ain't Markup Language) é um formato de dados estruturado amplamente utilizado para configurações de software e gerenciamento de infraestrutura em DevOps, popular em ferramentas como Kubernetes ? YAML como formato essencial para configuração e automação. ? Como o prompting pode ser utilizado para gerar YAML automaticamente. Dados em formatos JSON e YAML





ETAPA 4 Prompts avançados Relatórios Executivos e Tomada de Decisão: geração de relatórios executivos que sintetizam informações de relatórios longos, facilitando a tomada de decisões. Análise de Notícias e Artigos: Sumarizar notícias e artigos para leitores, oferecendo uma visão geral das informações mais importantes. Resumir textos utilizando prompts O resumo de textos é uma das aplicações mais populares e úteis de LLMs, pois permite extrair informações principais de grandes quantidades de conteúdo de forma eficiente. Vamos classificar os tipos de atividades de resumo em: ? Resumo Extrativo ? Resumo Abstrativo ? Resumo Híbrido Pesquisa Acadêmica e Científica: resumo de artigos e papers científicos para pesquisadores, permitindo uma rápida revisão de literatura. Assistentes de Atendimento ao Cliente: resumo de interações passadas para contexto antes do atendimento. “Reescreva as principais ideias deste texto em um resumo conciso de até 3 frases.” Resumir textos utilizando prompts No resumo extrativo, o modelo seleciona frases ou trechos importantes diretamente do texto original, mantendo a estrutura e as palavras originais. Esse tipo de resumo é útil em situações que exigem fidelidade ao texto fonte, como notícias ou relatórios financeiros. No resumo abstrativo, o modelo reformula o conteúdo, criando novas frases que sintetizam as ideias do texto original de forma mais concisa e criativa. Esse método é mais flexível, porém depende da capacidade do modelo de interpretar e reescrever a informação. Extrativo “Resuma este texto extraindo as frases mais importantes.” Leia o texto a seguir e extraia apenas as frases mais relevantes que sintetizam as principais informações. Mantenha a linguagem original e destaque as ideias centrais. Abstrativo “Resuma o texto a seguir usando uma combinação de frases extraídas do texto original e novas formulações. Destaque as informações principais de forma concisa.” “Resuma o texto a seguir usando uma combinação de frases extraídas do texto original e novas formulações. Destaque as informações principais de forma concisa.” Híbrido Manutenção do Contexto LLMs podem perder o fio de pensamento ao longo de textos extensos, especialmente se houver múltiplos tópicos ou mudanças de contexto. Limitação de Tokens Muitos LLMs possuem um limite de tokens por entrada e saída, o que restringe a quantidade de texto que pode ser resumida em uma única iteração. Coerência e Coesão A divisão de textos em chunks pode gerar resumos que não são coesos ou que apresentam quebra na fluidez entre as partes resumidas. Resumir textos utilizando prompts Risco de Omissão de Informações Importantes LLMs podem deixar de lado detalhes essenciais, especialmente quando orientados a fazer resumos mais concisos. Tendência de Alucinação Modelos podem gerar informações irrelevantes ou incorretas, criando conteúdo que não estava presente no texto original. Dificuldade de Balancear Generalidade e Especificidade Encontrar um equilíbrio entre ser conciso e manter detalhes importantes é um desafio, especialmente em textos complexos ou técnicos. Estratégias de Prompts para Resumo de Textos Longos Para resumir textos longos com LLMs, é necessário dividir o conteúdo em partes menores, chamadas de chunks. Esse processo facilita a manutenção de contexto e precisão durante o resumo. Resumo Sequencial: dividir o texto em seções sequenciais e aplicar o mesmo prompt a cada uma. Apesar de manter a coerência dentro de cada chunk, pode perder o contexto geral ao consolidar as partes resumidas. Estratégias de Prompts para Resumo de Textos Longos Formatos de Chunking para Resumo de Textos Longos Existem diversas estratégias de chunking de textos, que aplicam cortes no texto segundo diferentes estruturas. Divisão por Parágrafos: cada parágrafo é tratado como um chunk independente, o que facilita a manutenção de coesão dentro de tópicos, porém pode haver perda de continuidade entre parágrafos. Divisão por Tópicos ou Seções: separação baseada na estrutura do documento, como introdução, metodologia, etc. Mantém o contexto específico de cada seção,porém pode deixar lacunas de informação entre as seções. Estratégias de Prompts para Resumo de Textos Longos EXERCÍCIOS Tarefa 1: sumarização de episódios Utilizar técnicas de prompt engineering para sumarizar um episódio do programa. Tarefa 2: sumarização da temporada Quebrar a temporada em chunks de episódios para sumarização dividida. Tarefa 3: listar a interação com a família Para cada temporada, listar quais foram os personagens que mais interagiram com cada membro da família. O conceito de token é essencial para o processamento de textos longos. Quanto maior o número de tokens, mais complexa e custosa se torna a tarefa de processamento. Limites de Tokens afetam diretamente o desempenho e o custo dos LLMs. Quando o texto excede esse limite, ele precisa ser dividido em chunks menores, o que pode afetar a fluidez e o contexto do texto. Além disso, como muitos LLMs têm custos atrelados ao uso de tokens, é importante otimizar prompts e textos para minimizar o consumo e garantir que o modelo entregue respostas completas. Quantidade de tokens necessários para processar textos longos https://www.edenai.co/post/understanding-llm-billing-from-characters-to-tokens
 Gerenciamento do consumo de tokens em LLMs Fundamental prever o número de tokens que um texto consumirá para estimativa de custos e otimização de prompts. O BPE (Byte Pair Encoding) ajuda a minimizar o número total de tokens, dividindo palavras em partes reutilizáveis, amplamente usado em todos os LLMs. A biblioteca tiktoken oferece métodos para calcular o número de tokens com base no texto e no modelo utilizado, simulando o processo de tokenização. Quantidade de tokens necessários para processar textos longos Role Prompting é uma técnica de prompting que orienta o LLM a assumir um papel específico ao responder. Ao definir um "papel" para o modelo, conseguimos guiar o tom e o conteúdo da resposta, melhorando a coerência e adequação ao contexto. Benefícios: útil em aplicações que requerem respostas com um estilo específico ou que precisam incorporar conhecimentos especializados. Role prompting ajuda o LLM a adaptar seu vocabulário. Role prompting para otimizar respostas de LLMs "Seja um especialista em marketing e dê sugestões para aumentar o engajamento do público nas redes sociais." "Finja ser um historiador e explique a importância da Revolução Industrial." Quando usar Role Prompting? Personalizar o estilo de resposta: atribuir um papel pode orientar o LLM a gerar respostas com um tom, estilo ou perspectiva específicos, como uma resposta formal, casual ou humorística. Elicitar conhecimento especializado: quando a resposta exige conhecimento específico de um domínio ou área, o Role Prompting ajuda o LLM a gerar respostas mais precisas e informadas. Estimular respostas criativas: Role Prompting pode ser utilizado para criar cenários fictícios ou gerar respostas imaginativas ao atribuir papéis como contador de histórias, personagens de um romance ou figuras históricas. Explorar perspectivas diversas: Para explorar diferentes pontos de vista sobre um tema, o Role Prompting pode ajudar ao solicitar que o LLM assuma várias perspectivas ou personagens, proporcionando uma visão mais completa do assunto. Aumentar o engajamento do usuário: Role Prompting torna as interações mais envolventes e divertidas ao permitir que o LLM assuma personagens ou papéis que se conectem com o usuário. Especialização Personalização Criatividade Perspectivas Engajamento Role prompting para otimizar respostas de LLMs Desafios: risco de introduzir viés ou estereótipos com base no papel atribuído, o que pode levar a respostas enviesadas, prejudicando a usabilidade e, em alguns casos, podendo ofender os usuários. Risco de perda de consistência no papel após longas interações com o usuário, bem como o risco de prompt injection, alterando o Role do sistema. Definição de Estilo e Formato: Especifique o tom e o estilo desejados, como “formal”, “informativo” ou “conversacional”. Isso ajuda o modelo a ajustar o estilo de escrita. Uso de Exemplos: Apresente exemplos de respostas esperadas para guiar o modelo. Isso aumenta a coerência e a qualidade das respostas ao alinhar o output ao estilo desejado. Direcionamento Claro: Defina o papel de forma explícita, incluindo instruções sobre a perspectiva que o LLM deve assumir (ex.: "Responda como um especialista financeiro"). Boas práticas em Role Prompting EXERCÍCIOS Tarefa 1: Role prompting para agentes personalizados Criar Agentes de IA que possam interpretar os personagens na interação com o usuário. Tarefa 2: Recriação de episódios Simular os editores dos Simpsons, criando novos diálogos e fim para os episódios. Tarefa 3: Assistente de conteúdo Interface para QA com um assistente especialista em Simpsons.




ETAPA 5 Técnicas de engenharia de Prompt Least-to-Most Prompting: segue o princípio da divisão de problemas complexos em subtarefas, facilitando que o modelo resolva etapas sequencialmente. Essa técnica oferece maior controle e clareza no processo, ideal para problemas multi-etapa. No entanto, exige planejamento cuidadoso para sequências corretas. ? Vantagens: Melhora a precisão, facilita a resolução de tarefas complexas e permite flexibilidade entre LLMs para diferentes partes do problema. ? Limitações: Pode ser menos eficiente em questões que não requerem múltiplas etapas, bem como dependência e de prompts anteriores Técnicas de Engenharia de Prompt Tarefa principal: escrever um código em Python para expor uma API com um serviço de LLM. ? Prompt 1: escreva um código “hello world” de uma aplicação Flask simples; ? Prompt 2: escreva o código da aplicação Flask para receber uma string chamada prompt. ? Prompt 3: adicione à aplicação um código simples para receber a string prompt e utilizar com a biblioteca google.generative_ai e LLM Gemini-Flash. ? Prompt 4: escreva um código em separado para testar essa aplicação com o prompt = “Olá”. Self-Ask é uma abordagem onde o LLM, ao responder uma pergunta complexa, divide-a em subperguntas resolvidas sequencialmente. Isso permite quebrar a questão em partes menores e mais gerenciáveis, produzindo uma resposta detalhada e estruturada. Cada subpergunta pode ser respondida com fontes confiáveis de dados, deixando o raciocínio final ao LLM. ? Vantagens: capacidade de gerar respostas mais precisas e detalhadas e de melhorar o entendimento de questões ambíguas ou complexas. ? Desafios: o aumento no tempo de processamento e coerência em perguntas com múltiplas interpretações. Prompt: Pergunta Principal: "Quais foram os principais fatores que levaram à queda do Império Romano?" Você precisa considerar os seguintes questionamentos antes de responder: - "Para entender a queda do Império Romano, quais foram os principais desafios econômicos que enfrentaram?" - "Quais fatores sociais e culturais influenciaram a desestabilização do império?" - "Que eventos militares e invasões contribuíram diretamente para a queda?" - "Como as divisões internas de poder e a corrupção afetaram o império?" Técnicas de Engenharia de Prompt A técnica ReAct combina raciocínio e ação em prompts, permitindo que LLMs respondam a perguntas complexas, decompondo-as em etapas de raciocínio e busca ativa de informações. ? Integração de Ações e Respostas: ao combinar raciocínio com ações (como consultas de dados ou cálculos), o LLM pode ajustar suas respostas com base em novos dados ou processos iterativos. ? Redução de Erros: Como o modelo verifica etapas ao longo do processo, ReAct pode minimizar erros em problemas complexos, garantindo que a resposta final seja bem fundamentada e construída de forma lógica. ? Maior Tempo de Resposta: envolve múltiplas etapas de raciocínio e ação, aumentando o tempo de processamento. ? Consumo de Recursos: envolve uma série de prompts e respostas intermediárias para completar a tarefa. Técnicas de Engenharia de Prompt https://medium.com/@bryan.mckenney/teaching-llms-to-think-and-act-react-prompt-engineering-eef278555a2e
 ReAct ReAct https://www.promptingguide.ai/techniques/react
 Symbolic Reasoning e PAL (Program-Aided Language Model) aprimoram a capacidade dos modelos de linguagem em resolver problemas complexos que envolvem lógica formal, cálculos simbólicos e manipulação de variáveis. Symbolic Reasoning: utiliza representações simbólicas para resolver equações e aplicar regras lógicas. Problemas onde o raciocínio passo a passo é fundamental, como manipulação algébrica e deduções matemáticas. PAL: vai um passo além, permitindo que o LLM gere e execute código (Python) para resolver problemas. Em vez de apenas descrever a solução, o modelo cria programas que executam cálculos ou validam resultados. Isso é útil para tarefas de análise de dados, cálculos científicos, e outros problemas computacionais. Técnicas de Engenharia de Prompt Modelos de classificação utilizando LLMs através de few-shot learning Few-shot learning: o modelo de linguagem é capaz de classificar novos dados a partir de poucos exemplos anotados. Essa técnica é especialmente útil quando há limitação de dados para o treinamento do modelo, pois permite que ele aprenda padrões a partir de um número mínimo de exemplos. ? Selecionar exemplos representativos da tarefa para guiar o LLM. ? Formular o prompt com clareza, incluindo as instruções e exemplos. ? Testar e ajustar os exemplos até que o modelo apresente respostas confiáveis e consistentes. Prompt: Gere cinco sugestões de títulos para um artigo sobre o tema de inteligência artificial aplicado à saúde. Siga o formato dos exemplos abaixo: - Exemplo 1: 'Como a Inteligência Artificial Está Transformando a Medicina' - Exemplo 2: 'Inteligência Artificial e Saúde: Inovações e Desafios' - Exemplo 3: 'O Papel da IA no Diagnóstico de Doenças' Gere títulos criativos e informativos Prompt: Gere uma lista de tarefas para iniciar um projeto de desenvolvimento de software, semelhante aos exemplos abaixo: - Exemplo 1: 'Definir requisitos funcionais e não funcionais do sistema' - Exemplo 2: 'Criar um esboço do fluxo de trabalho do usuário' - Exemplo 3: 'Elaborar protótipos iniciais da interface' Siga o estilo direto e orientado para a ação Modelos de classificação utilizando LLMs através de few-shot learning https://ai.gopubby.com/few-shot-prompting-vs-detailed-instructions-in-text-analysis-4d7c5ee92500
 https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/
 Meta prompting: técnica de design de prompts onde um modelo de linguagem é instruído a criar seus próprios prompts. Meta prompting permite que o modelo escreva instruções para si mesmo, tornando-o capaz de resolver problemas de múltiplas etapas sem intervenção humana constante. ? Agilidade: Reduz o tempo necessário para criar prompts personalizados ? Autonomia: Habilita o modelo a gerar instruções para novas tarefas sem supervisão. ? Dependência de Revisão: O output do modelo ainda requer ajustes e validação humana. ? Complexidade Inicial: pode exigir um planejamento cuidadoso e conhecimento avançado de prompt. Meta prompting para otimizar o desempenho de LLMs Prompt: Crie um prompt que irá gerar uma lista de perguntas para uma entrevista de emprego para a posição de Cientista de Dados. As perguntas devem ser divididas em três seções: habilidades técnicas, habilidades interpessoais e resolução de problemas. As perguntas devem permitir avaliar a experiência e o raciocínio do candidato. Prompt: Escreva um prompt que gere uma estrutura detalhada para um artigo de blog sobre Inteligência Artificial. A estrutura deve incluir uma introdução que apresenta o tema, seções intermediárias com sub-tópicos relevantes e uma conclusão. Cada seção deve sugerir pontos principais a serem abordados, com um tom informativo e acessível para o público. EXERCÍCIOS Tarefa 1: Classificação de sentimento das falas (few-shot prompting) Criar Agentes de IA que classificam as falas dos personagens como positivas, negativas ou neutras. Tarefa 2: Meta-prompting para criar Atores dos Simpsons Usar meta-prompting para aprimorar o prompt de criação dos atores dos personagens. Técnicas de Engenharia de Prompt Chain-of-Thought Prompting: Estimula o modelo a pensar em sequência, detalhando o raciocínio antes da resposta final. Útil para processos analíticos complexos, facilita respostas mais robustas, mas pode gerar conteúdo extenso em questões simples. ? Vantagens: Aumenta a clareza e lógica de respostas. ? Limitações: Excesso de detalhe pode ser redundante em perguntas diretas. Exemplo: Enumere as etapas de um plano de marketing de um lançamento musical para o carnaval. Técnicas de Engenharia de Prompt Self-Consistency: utiliza múltiplas tentativas de raciocínio para uma mesma pergunta, selecionando a resposta mais consistente entre elas. Se baseia no princípio de que avaliar muitas respostas candidatas aumenta a precisão da resposta final. ? Vantagens: melhora a confiabilidade de LLM através da “média” das respostas a um mesmo prompt, robusto a outliers na resposta e medição de confiança. ? Limitações: além do custo e do tempo de analisar respostas longas, vieses de consenso e não recomendado para tarefas criativas. Criar um prompt específico """ Solve the following math problem step by step: A train travels at a speed of 60 km/h for 2 hours, then at 80 km/h for 1 hour. What is the average speed of the train for the entire journey? Provide your answer in km/h, rounded to two decimal places. """ https://arxiv.org/pdf/2203.11171
 Self-Consistency Prompt Gerar diversas respostas para o mesmo prompt Comparar e examinar as respostas Agregar os resultados para resposta final Técnicas de Engenharia de Prompt Generated Knowledge: técnica que envolve pedir ao LLM que gere informações úteis sobre um tema antes de responder à pergunta ou completar uma tarefa complexa. Essa base de conhecimento aumenta a precisão e profundidade das respostas (prompt único ou duplo). ? Vantagens: respostas detalhadas e bem-informadas, útil para temas complexos. ? Limitações: mais lento, pois envolve múltiplas chamadas ao LLM. Prompt: "Liste 3 impactos do aquecimento global no meio ambiente." https://arxiv.org/pdf/2110.08387
 Resposta: ? Aumento das temperaturas médias globais. ? Derretimento de geleiras e elevação do nível do mar. ? Alteração dos padrões climáticos e aumento de eventos climáticos extremos. Prompt: "Usando os fatos sobre o aumento das temperaturas, derretimento de geleiras e alterações climáticas, escreva um parágrafo sobre os impactos do aquecimento global no meio ambiente." Automatic Reasoning and Tool-Use (ART) : combina raciocínio em múltiplas etapas e uso de ferramentas externas para resolver tarefas complexas com precisão. Combina CoT com ferramentas externas, permitindo ao LLM pausar o raciocínio para realizar consultas ou cálculos específicos para melhorar a resposta. ? Vantagens: precisão, confiabilidade e extensível. ? Limitações: demanda computacional alta, necessidade sistemas configurados para suportar chamadas de API. Técnicas de Engenharia de Prompt https://arxiv.org/pdf/2303.09014
 Tarefas complexas com prompt chaining O prompt chaining é uma técnica poderosa para resolver problemas complexos, dividindo-os em sub-tarefas gerenciáveis. Em vez de tentar resolver o problema de uma só vez, o prompt chaining permite encadear uma série de prompts, onde cada etapa contribui para a solução final. Essa abordagem é especialmente útil em questões que exigem várias etapas, como resumo de textos extensos, análise de múltiplas variáveis ou a geração de relatórios detalhados a partir de dados. https://vectorshift.ai/blog/how-to-write-the-best-prompts-and-use-prompt-chaining
 Tarefas complexas com prompt chaining Vantagens incluem a capacidade de lidar com tarefas complexas sem sobrecarregar o modelo, enquanto desafios podem envolver a manutenção de coerência e contexto entre os prompts, além do tempo de processamento adicional para múltiplas etapas. Pontos importantes: ? Divisão em sub-tarefas: facilita a resolução de problemas complexos. ? Encadeamento de prompts: permite refinar e expandir a solução. ? Desafios: manter contexto e otimizar o tempo de resposta Prompt 1: "Leia o seguinte artigo e forneça um resumo básico com as ideias principais e os tópicos abordados: [trecho do artigo]." Prompt 2: "Baseado no resumo inicial, forneça uma análise detalhada de cada seção do artigo, incluindo introdução, metodologia, resultados e conclusão." Prompt 3: "Com base no resumo e na análise detalhada das seções, organize um relatório estruturado com títulos para cada seção e uma conclusão crítica sobre o impacto do estudo." Tarefas complexas com prompt chaining Prompt 1: "Identifique os principais tópicos de um curso de Biologia para o ensino médio que devem ser cobertos em um guia de estudo." Prompt 2: "Para cada tópico listado, forneça um resumo curto que explique os conceitos principais e quaisquer fórmulas ou diagramas importantes." Prompt 3: "Baseado nos tópicos e resumos, crie três perguntas de prática para cada tópico, variando entre questões conceituais e questões de múltipla escolha." Prompt 1: "Liste os requisitos essenciais para desenvolver um sistema de gerenciamento de inventário para pequenas empresas." Prompt 2: "Com base nos requisitos listados, organize-os em módulos principais, como 'Cadastro de Produtos', 'Controle de Estoque' e 'Relatórios de Vendas'." Prompt 3: "Para cada módulo, defina tarefas específicas para a implementação, incluindo etapas de desenvolvimento e testes necessários para cada funcionalidade." Tarefa 1: Análise Exploratória da Estatística dos Episódios Utilizar técnica de prompt chaining para implementar uma análise exploratória dos dados de audiência, personagens e reviews dos episódios e temporadas. Tarefa 2: Chain of Thought para criação do dashboard de interação com os atores Usar o LLM para gerar um código Python para implementar um dashboard streamlit com a interação entre o usuários e os atores. Tarefa 3: Generated Knowledge para gerar Insights Usar o LLM como um cientista de dados para gerar insights de acordo com a estatística e análises realizadas na base de dados. EXERCÍCIOS





ETAPA 5 Técnicas de engenharia de Prompt Least-to-Most Prompting: segue o princípio da divisão de problemas complexos em subtarefas, facilitando que o modelo resolva etapas sequencialmente. Essa técnica oferece maior controle e clareza no processo, ideal para problemas multi-etapa. No entanto, exige planejamento cuidadoso para sequências corretas. ? Vantagens: Melhora a precisão, facilita a resolução de tarefas complexas e permite flexibilidade entre LLMs para diferentes partes do problema. ? Limitações: Pode ser menos eficiente em questões que não requerem múltiplas etapas, bem como dependência e de prompts anteriores Técnicas de Engenharia de Prompt Tarefa principal: escrever um código em Python para expor uma API com um serviço de LLM. ? Prompt 1: escreva um código “hello world” de uma aplicação Flask simples; ? Prompt 2: escreva o código da aplicação Flask para receber uma string chamada prompt. ? Prompt 3: adicione à aplicação um código simples para receber a string prompt e utilizar com a biblioteca google.generative_ai e LLM Gemini-Flash. ? Prompt 4: escreva um código em separado para testar essa aplicação com o prompt = “Olá”. Self-Ask é uma abordagem onde o LLM, ao responder uma pergunta complexa, divide-a em subperguntas resolvidas sequencialmente. Isso permite quebrar a questão em partes menores e mais gerenciáveis, produzindo uma resposta detalhada e estruturada. Cada subpergunta pode ser respondida com fontes confiáveis de dados, deixando o raciocínio final ao LLM. ? Vantagens: capacidade de gerar respostas mais precisas e detalhadas e de melhorar o entendimento de questões ambíguas ou complexas. ? Desafios: o aumento no tempo de processamento e coerência em perguntas com múltiplas interpretações. Prompt: Pergunta Principal: "Quais foram os principais fatores que levaram à queda do Império Romano?" Você precisa considerar os seguintes questionamentos antes de responder: - "Para entender a queda do Império Romano, quais foram os principais desafios econômicos que enfrentaram?" - "Quais fatores sociais e culturais influenciaram a desestabilização do império?" - "Que eventos militares e invasões contribuíram diretamente para a queda?" - "Como as divisões internas de poder e a corrupção afetaram o império?" Técnicas de Engenharia de Prompt A técnica ReAct combina raciocínio e ação em prompts, permitindo que LLMs respondam a perguntas complexas, decompondo-as em etapas de raciocínio e busca ativa de informações. ? Integração de Ações e Respostas: ao combinar raciocínio com ações (como consultas de dados ou cálculos), o LLM pode ajustar suas respostas com base em novos dados ou processos iterativos. ? Redução de Erros: Como o modelo verifica etapas ao longo do processo, ReAct pode minimizar erros em problemas complexos, garantindo que a resposta final seja bem fundamentada e construída de forma lógica. ? Maior Tempo de Resposta: envolve múltiplas etapas de raciocínio e ação, aumentando o tempo de processamento. ? Consumo de Recursos: envolve uma série de prompts e respostas intermediárias para completar a tarefa. Técnicas de Engenharia de Prompt https://medium.com/@bryan.mckenney/teaching-llms-to-think-and-act-react-prompt-engineering-eef278555a2e
 ReAct ReAct https://www.promptingguide.ai/techniques/react
 Symbolic Reasoning e PAL (Program-Aided Language Model) aprimoram a capacidade dos modelos de linguagem em resolver problemas complexos que envolvem lógica formal, cálculos simbólicos e manipulação de variáveis. Symbolic Reasoning: utiliza representações simbólicas para resolver equações e aplicar regras lógicas. Problemas onde o raciocínio passo a passo é fundamental, como manipulação algébrica e deduções matemáticas. PAL: vai um passo além, permitindo que o LLM gere e execute código (Python) para resolver problemas. Em vez de apenas descrever a solução, o modelo cria programas que executam cálculos ou validam resultados. Isso é útil para tarefas de análise de dados, cálculos científicos, e outros problemas computacionais. Técnicas de Engenharia de Prompt Modelos de classificação utilizando LLMs através de few-shot learning Few-shot learning: o modelo de linguagem é capaz de classificar novos dados a partir de poucos exemplos anotados. Essa técnica é especialmente útil quando há limitação de dados para o treinamento do modelo, pois permite que ele aprenda padrões a partir de um número mínimo de exemplos. ? Selecionar exemplos representativos da tarefa para guiar o LLM. ? Formular o prompt com clareza, incluindo as instruções e exemplos. ? Testar e ajustar os exemplos até que o modelo apresente respostas confiáveis e consistentes. Prompt: Gere cinco sugestões de títulos para um artigo sobre o tema de inteligência artificial aplicado à saúde. Siga o formato dos exemplos abaixo: - Exemplo 1: 'Como a Inteligência Artificial Está Transformando a Medicina' - Exemplo 2: 'Inteligência Artificial e Saúde: Inovações e Desafios' - Exemplo 3: 'O Papel da IA no Diagnóstico de Doenças' Gere títulos criativos e informativos Prompt: Gere uma lista de tarefas para iniciar um projeto de desenvolvimento de software, semelhante aos exemplos abaixo: - Exemplo 1: 'Definir requisitos funcionais e não funcionais do sistema' - Exemplo 2: 'Criar um esboço do fluxo de trabalho do usuário' - Exemplo 3: 'Elaborar protótipos iniciais da interface' Siga o estilo direto e orientado para a ação Modelos de classificação utilizando LLMs através de few-shot learning https://ai.gopubby.com/few-shot-prompting-vs-detailed-instructions-in-text-analysis-4d7c5ee92500
 https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/
 Meta prompting: técnica de design de prompts onde um modelo de linguagem é instruído a criar seus próprios prompts. Meta prompting permite que o modelo escreva instruções para si mesmo, tornando-o capaz de resolver problemas de múltiplas etapas sem intervenção humana constante. ? Agilidade: Reduz o tempo necessário para criar prompts personalizados ? Autonomia: Habilita o modelo a gerar instruções para novas tarefas sem supervisão. ? Dependência de Revisão: O output do modelo ainda requer ajustes e validação humana. ? Complexidade Inicial: pode exigir um planejamento cuidadoso e conhecimento avançado de prompt. Meta prompting para otimizar o desempenho de LLMs Prompt: Crie um prompt que irá gerar uma lista de perguntas para uma entrevista de emprego para a posição de Cientista de Dados. As perguntas devem ser divididas em três seções: habilidades técnicas, habilidades interpessoais e resolução de problemas. As perguntas devem permitir avaliar a experiência e o raciocínio do candidato. Prompt: Escreva um prompt que gere uma estrutura detalhada para um artigo de blog sobre Inteligência Artificial. A estrutura deve incluir uma introdução que apresenta o tema, seções intermediárias com sub-tópicos relevantes e uma conclusão. Cada seção deve sugerir pontos principais a serem abordados, com um tom informativo e acessível para o público. EXERCÍCIOS Tarefa 1: Classificação de sentimento das falas (few-shot prompting) Criar Agentes de IA que classificam as falas dos personagens como positivas, negativas ou neutras. Tarefa 2: Meta-prompting para criar Atores dos Simpsons Usar meta-prompting para aprimorar o prompt de criação dos atores dos personagens. Técnicas de Engenharia de Prompt Chain-of-Thought Prompting: Estimula o modelo a pensar em sequência, detalhando o raciocínio antes da resposta final. Útil para processos analíticos complexos, facilita respostas mais robustas, mas pode gerar conteúdo extenso em questões simples. ? Vantagens: Aumenta a clareza e lógica de respostas. ? Limitações: Excesso de detalhe pode ser redundante em perguntas diretas. Exemplo: Enumere as etapas de um plano de marketing de um lançamento musical para o carnaval. Técnicas de Engenharia de Prompt Self-Consistency: utiliza múltiplas tentativas de raciocínio para uma mesma pergunta, selecionando a resposta mais consistente entre elas. Se baseia no princípio de que avaliar muitas respostas candidatas aumenta a precisão da resposta final. ? Vantagens: melhora a confiabilidade de LLM através da “média” das respostas a um mesmo prompt, robusto a outliers na resposta e medição de confiança. ? Limitações: além do custo e do tempo de analisar respostas longas, vieses de consenso e não recomendado para tarefas criativas. Criar um prompt específico """ Solve the following math problem step by step: A train travels at a speed of 60 km/h for 2 hours, then at 80 km/h for 1 hour. What is the average speed of the train for the entire journey? Provide your answer in km/h, rounded to two decimal places. """ https://arxiv.org/pdf/2203.11171
 Self-Consistency Prompt Gerar diversas respostas para o mesmo prompt Comparar e examinar as respostas Agregar os resultados para resposta final Técnicas de Engenharia de Prompt Generated Knowledge: técnica que envolve pedir ao LLM que gere informações úteis sobre um tema antes de responder à pergunta ou completar uma tarefa complexa. Essa base de conhecimento aumenta a precisão e profundidade das respostas (prompt único ou duplo). ? Vantagens: respostas detalhadas e bem-informadas, útil para temas complexos. ? Limitações: mais lento, pois envolve múltiplas chamadas ao LLM. Prompt: "Liste 3 impactos do aquecimento global no meio ambiente." https://arxiv.org/pdf/2110.08387
 Resposta: ? Aumento das temperaturas médias globais. ? Derretimento de geleiras e elevação do nível do mar. ? Alteração dos padrões climáticos e aumento de eventos climáticos extremos. Prompt: "Usando os fatos sobre o aumento das temperaturas, derretimento de geleiras e alterações climáticas, escreva um parágrafo sobre os impactos do aquecimento global no meio ambiente." Automatic Reasoning and Tool-Use (ART) : combina raciocínio em múltiplas etapas e uso de ferramentas externas para resolver tarefas complexas com precisão. Combina CoT com ferramentas externas, permitindo ao LLM pausar o raciocínio para realizar consultas ou cálculos específicos para melhorar a resposta. ? Vantagens: precisão, confiabilidade e extensível. ? Limitações: demanda computacional alta, necessidade sistemas configurados para suportar chamadas de API. Técnicas de Engenharia de Prompt https://arxiv.org/pdf/2303.09014
 Tarefas complexas com prompt chaining O prompt chaining é uma técnica poderosa para resolver problemas complexos, dividindo-os em sub-tarefas gerenciáveis. Em vez de tentar resolver o problema de uma só vez, o prompt chaining permite encadear uma série de prompts, onde cada etapa contribui para a solução final. Essa abordagem é especialmente útil em questões que exigem várias etapas, como resumo de textos extensos, análise de múltiplas variáveis ou a geração de relatórios detalhados a partir de dados. https://vectorshift.ai/blog/how-to-write-the-best-prompts-and-use-prompt-chaining
 Tarefas complexas com prompt chaining Vantagens incluem a capacidade de lidar com tarefas complexas sem sobrecarregar o modelo, enquanto desafios podem envolver a manutenção de coerência e contexto entre os prompts, além do tempo de processamento adicional para múltiplas etapas. Pontos importantes: ? Divisão em sub-tarefas: facilita a resolução de problemas complexos. ? Encadeamento de prompts: permite refinar e expandir a solução. ? Desafios: manter contexto e otimizar o tempo de resposta Prompt 1: "Leia o seguinte artigo e forneça um resumo básico com as ideias principais e os tópicos abordados: [trecho do artigo]." Prompt 2: "Baseado no resumo inicial, forneça uma análise detalhada de cada seção do artigo, incluindo introdução, metodologia, resultados e conclusão." Prompt 3: "Com base no resumo e na análise detalhada das seções, organize um relatório estruturado com títulos para cada seção e uma conclusão crítica sobre o impacto do estudo." Tarefas complexas com prompt chaining Prompt 1: "Identifique os principais tópicos de um curso de Biologia para o ensino médio que devem ser cobertos em um guia de estudo." Prompt 2: "Para cada tópico listado, forneça um resumo curto que explique os conceitos principais e quaisquer fórmulas ou diagramas importantes." Prompt 3: "Baseado nos tópicos e resumos, crie três perguntas de prática para cada tópico, variando entre questões conceituais e questões de múltipla escolha." Prompt 1: "Liste os requisitos essenciais para desenvolver um sistema de gerenciamento de inventário para pequenas empresas." Prompt 2: "Com base nos requisitos listados, organize-os em módulos principais, como 'Cadastro de Produtos', 'Controle de Estoque' e 'Relatórios de Vendas'." Prompt 3: "Para cada módulo, defina tarefas específicas para a implementação, incluindo etapas de desenvolvimento e testes necessários para cada funcionalidade." Tarefa 1: Análise Exploratória da Estatística dos Episódios Utilizar técnica de prompt chaining para implementar uma análise exploratória dos dados de audiência, personagens e reviews dos episódios e temporadas. Tarefa 2: Chain of Thought para criação do dashboard de interação com os atores Usar o LLM para gerar um código Python para implementar um dashboard streamlit com a interação entre o usuários e os atores. Tarefa 3: Generated Knowledge para gerar Insights Usar o LLM como um cientista de dados para gerar insights de acordo com a estatística e análises realizadas na base de dados. EXERCÍCIOS




ETAPA 6 Segurança e otimização de Prompts Prevenir ataques de injeção de prompt aplicando medidas de segurança LLMs estão sujeitos a riscos de segurança, como injeção de prompt e o sequestro de prompt, cujas consequências variam desde a inviabilidade do LLM, até a exposição indevida de dados sensíveis. ? Vazamento de prompt (direto): o prompt de um sistema é revelado e, possivelmente, violado num ataque ? Sequestro de Objetivo (indireto): prompts específicos que conseguem contornar a diretivas de segurança. (DAN) https://www.lakera.ai/blog/guide-to-prompt-injection
 Prevenir ataques de injeção de prompt aplicando medidas de segurança Estratégias de prevenção de ataques de injeção de prompt: Prompts mais longos e específicos: prompts curtos são mais vulneráveis a ataques. Utilizar prompts detalhados dificulta que um atacante adicione instruções sem alterar o contexto. Prompts dinâmicos: personalizar prompts com elementos únicos, como identificadores de sessão, reduz a probabilidade de que sejam replicados ou manipulados. Validação para garantir segurança dos LLMs Validação de entrada e saída: implementar filtros de segurança para verificar se a entrada e a saída do LLM contêm elementos indesejados ou indicativos de ataques. Formatação estruturada (JSON/YAML): ao usar formatos padronizados, como JSON ou YAML, para estruturar respostas, é mais fácil identificar e prevenir conteúdos fora do esperado. Recomenda-se usar uma lista de termos proibidos ou integrar o LLM com modelos especializados em detecção de conteúdo ofensivo, como modelos de Inferência Natural de Linguagem (NLI), para validar as saídas em tempo real. Pontos importantes: ? Importância da segurança na validação de saídas de LLMs. ? Filtros de conteúdo para evitar respostas inapropriadas. ? Integração com modelos NLI para aumentar a precisão na detecção de conteúdo ofensivo. Validação para garantir segurança dos LLMs https://www.robustintelligence.com/ai-security-reference-architectures
 Validação para garantir segurança dos LLMs https://developer.ibm.com/articles/awb-data-privacy-security-watsonx-workloads-ibm-cloud/
 Pipelines de validação com modelos NLI para detectar comportamentos ofensivos Pipelines de validação com NLI (natural language inference) podem ajudar a garantir a segurança das respostas do LLM. Entradas e saídas são validadas sequencialmente antes e depois de serem processadas pelo LLM, verificando a coerência e evitando respostas inadequadas. BART-MNLI (MetaAI): modelo BART treinado no dataset MNLI, que contém pares de textos onde o modelo precisa determinar a relação lógica entre uma premissa e uma hipótese, classificando-a como implicação, contradição ou neutra. Amplamente usado em tarefas de validação semântica. BART Bidirectional and Auto-Regressive Transformers MNLI Multi-Genre Natural Language Inference https://huggingface.co/facebook/bart-large-mnli
 BERT Bidirectional Encoder Representations from Transformers https://arxiv.org/pdf/1805.11360
 Pipelines de validação com modelos NLI para detectar comportamentos ofensivos https://ar5iv.labs.arxiv.org/html/1910.13461
 EXERCÍCIOS Tarefa 1: Classificador de Faixa Etária com BART-MNLI Utilizar técnica LLM de NLI para estimar a faixa etária dos episódios da série. Tarefa 2: Injeção de Prompting Alterando Personagens Simular injeções de prompting para alterar o comportamento de Atores baseados em LLMs. Tarefa 3: Prompt-Chaining para moderação de discurso Implementar um Agente Revisor para condicionar as falas à classificação etária. Otimizar o uso de LLMs e reduzir custos usando batch prompting A otimização de custos em aplicações que utilizam LLMs é essencial para empresas que processam grandes volumes de dados ou operam em ambientes com uso intensivo de IA. O batch prompting é uma técnica fundamental que combina várias consultas em uma única requisição ao modelo, reduzindo a quantidade de tokens utilizados e, consequentemente, os custos. https://arxiv.org/pdf/2301.08721
 Batch prompting: técnica que envolve a combinação de múltiplos prompts em uma única chamada ao LLM, em vez de fazer várias chamadas individuais. Esse método permite que várias perguntas ou tarefas sejam processadas de uma vez, economizando tempo e tokens. ? Redução do uso de tokens ? Diminuição dos custos operacionais. ? Maior eficiência em tarefas repetitivas Otimizar o uso de LLMs e reduzir custos usando batch prompting https://arxiv.org/pdf/2301.08721
 Otimizar o uso de LLMs e reduzir custos usando batch prompting Práticas recomendadas que garantem a eficiência e a precisão dos resultados: Clareza e Concisão: formulando prompts claros e diretos, reduzindo a quantidade de tokens e o risco de respostas imprecisas. Fluxo Lógico: estruturando as perguntas em uma sequência lógica, o que ajuda o modelo a entender o contexto e gera respostas mais consistentes. Organização e Relevância: agrupando somente perguntas relacionadas e mantendo o foco no objetivo do prompt para evitar respostas desnecessárias ou redundantes. Otimizar o uso de LLMs e reduzir custos usando batch prompting Prompt 1: """ 1. Qual é a previsão do tempo para hoje? 2. Qual é a cotação atual do dólar? 3. Quais são as notícias principais do dia? """ Prompt 2: “”” Responda às perguntas: "Explique o que é inteligência artificial.", "Quais são os benefícios da automação?" e "Como a aprendizagem de máquina difere da IA?" “”” EXERCÍCIOS Tarefa 1: Sumarizador de personas Utilizar técnicas de chunks para consolidar as características do personagem segundo suas falas. Tarefa 2: Extração de palavras chaves dos episódios por chunks Utilizar Batch Prompting para extrair palavras chave nos episódios. Tarefa 3: Aplicação de KDB para Otimizar Prompts Interface para QA com um assistente especialista em Simpsons.




dashoboard.py import streamlit as st import pandas as pd import os import joblib from dotenv import load_dotenv load_dotenv('../.env') from kdb_faiss import KDBFaiss import tabs # Cache data loading @st.cache_data def load_data(file_path): data = pd.read_parquet(file_path) data = data.sort_values('number') return data # Cache data loading for season and episode summaries @st.cache_data def load_joblib(file_path): return joblib.load(file_path) ############################################################# INICIO st.title("The Simpsons Show: Data Explorer") ############################################################# CARGA DOS DADOS BRUTOS dbfile = os.environ.get('DBFILE') if not dbfile: st.error("DBFILE environment variable is not set.") if st.session_state.get('data',None) is None: try: data = load_data(dbfile) st.session_state['data'] = data except Exception as e: st.error(f"Error loading data: {e}") data = st.session_state['data'] ############################################################# CARGA DOS SUMARIOS # Load season and episode summaries episode_summary_file = os.environ.get("EPISODE_SUMMARY_FILE") st.session_state['episode_summary'] = load_joblib(episode_summary_file) character_summary_file = os.environ.get("CHARACTER_SUMMARY_FILE") st.session_state['character_summary'] = load_joblib(character_summary_file) season_summary_file = os.environ.get("SEASON_SUMMARY_FILE") st.session_state['season_summary'] = load_joblib(season_summary_file) ############################################################# BASE DE DADOS VETORIAIS st.session_state['FAISS_DB'] = { 103: '../data/faiss/kdb_episode_id_103.faiss', 60: '../data/faiss/kdb_episode_id_60.faiss', 70: '../data/faiss/kdb_episode_id_70.faiss', 81: '../data/faiss/kdb_episode_id_81.faiss', 92: '../data/faiss/kdb_episode_id_92.faiss', 93: '../data/faiss/kdb_episode_id_93.faiss', } ############################################################# SIDE BAR st.sidebar.title("About The Simpsons Show") st.sidebar.write("Explore the episodes, characters, and seasons of The Simpsons.") st.sidebar.metric("Total Seasons", data['episode_season'].nunique()) st.sidebar.metric("Total Episodes", data['episode_id'].nunique()) st.sidebar.metric("Total Characters", data['character_id'].nunique()) ############################################################# ABAS (overview_tab, season_tab, character_tab, ads_tab, qa_tab) = st.tabs(("Overview", "Seasons", "Characters", "Ads", "Episode Q&A")) tabs.tab_overview(overview_tab) tabs.tab_season(season_tab) tabs.tab_character(character_tab) tabs.tab_ads(ads_tab) dataprep.py import time import pandas as pd import os import joblib from dotenv import load_dotenv load_dotenv('../.env') import faiss from sentence_transformers import SentenceTransformer import numpy as np from summarizer import ChunkSummary from kdb_faiss import KDBFaiss EPISODE_IDS = os.environ.get('EPISODE_IDS') EPISODE_IDS = [int(x) for x in EPISODE_IDS.split(',')] CHARACTERS = os.environ.get('CHARACTERS') CHARACTERS = CHARACTERS.split(',') EXECUTE_SUMMARIZATION_EPISODE = os.environ.get('EXECUTE_SUMMARIZATION_EPISODE', False) EXECUTE_SUMMARIZATION_SEASON = os.environ.get('EXECUTE_SUMMARIZATION_SEASON', False) EXECUTE_SUMMARIZATION_CHARACTER = os.environ.get('EXECUTE_SUMMARIZATION_CHARACTER', True) EXECUTE_EPISODES_MNLI = os.environ.get('EXECUTE_EPISODES_MNLI', False) EXECUTE_FAISS_KDB = os.environ.get('EXECUTE_FAISS_KDB', False) ######################################################### ## CARGA DOS DADOS: ## DBFILE = os.environ.get('DBFILE') data = pd.read_parquet(DBFILE) data = data.sort_values('number') print('DATA READ', data.shape) ######################################################### ## TAREFA 1: ## Sumarização de episódios: ## Utilizar técnicas de prompt engineering para sumarizar um episódio do programa. if EXECUTE_SUMMARIZATION_EPISODE: episode_summaries = {} for episode_id in EPISODE_IDS: X = data[data.episode_id == episode_id].copy() X = ("Episode " + X['episode_id'].astype(str) + ' | ' + X['location_normalized_name'].fillna('') + ', ' + X['character_normalized_name'].fillna('') + ' said: ' + X['normalized_text'].fillna('') ) print(X.shape) system_prompt = f""" You are an editor assistant from the "The Simpsons" show. You will receive chunk of subtitles from real episodes in the format: <episode number> | <location>, <character> said: <character line> You must create a summary of the episode, pointing out the most relevant information and key players in the story. Bare in mind that the summary must describe how the episode started, which key points are relevant along the story and its gran finale. """ generation_config = { 'temperature': 0.2, # 'top_p': 0.8, # 'top_k': 20, 'max_output_tokens': 200 } summarizer = ChunkSummary( model_name = "gemini-1.5-flash", apikey = os.environ["GEMINI_KEY"], text = X.tolist(), window_size = 100, overlap_size = 10, system_prompt=system_prompt, ) episode_summaries[episode_id] = summarizer.summarize() time.sleep(10) # Exportacao joblib.dump(episode_summaries, '../data/summaries/summary_episodes.joblib', compress=9) else: episode_summaries = joblib.load('../data/summaries/summary_episodes.joblib') ######################################################### ## TAREFA 2 ## Sumarização da temporada: ## Quebrar a temporada em chunks de episódios para sumarização dividida. if EXECUTE_SUMMARIZATION_SEASON: # Recuperar somente temporadas cujos episodios ja foram resumidos # Batch-prompting com principio de divisao de taregas. # Sumarizar pelos sumarios dos episodios df = pd.DataFrame().from_dict(episode_summaries, orient='index') df = df.reset_index() df.columns = ['episode_id','episode_summary'] df = df.merge(data, on='episode_id') df = df[['episode_id','episode_season','episode_summary']].drop_duplicates() df = df.sort_values(['episode_season','episode_id']) # Iteracao sobre temporadas season_summaries = {} for episode_season in df.episode_season.unique(): X = df[df.episode_season == episode_season].copy() X = ("Episode " + X['episode_id'].astype(str) + ' | ' + X['episode_summary'] ) print(X.shape) # SELF-ASK PROMPT system_prompt = f""" You are an editor assistant from the "The Simpsons" show. You will receive the summaries of the episodes from the show in the format: Episode <episode number> | <episode summary> ## INSTRUCTION You must create a summary of the season based on the summary of each episode, pointing out the most relevant information and key players in the story such as: - What relevant happened to the main characters? - Which stories are relevant to highlight for the audience? - Which jokes should be mentioned in order to promote the season? """ generation_config = { 'temperature': 0.2, # 'top_p': 0.8, # 'top_k': 20, 'max_output_tokens': 200 } summarizer = ChunkSummary( model_name = "gemini-1.5-flash", apikey = os.environ["GEMINI_KEY"], text = X.tolist(), window_size = 2, overlap_size = 1, system_prompt=system_prompt, ) season_summaries[int(episode_season)] = summarizer.summarize() time.sleep(30) # Exportacao joblib.dump(season_summaries, '../data/summaries/summary_seasons.joblib', compress=9) else: season_summaries = joblib.load('../data/summaries/summary_seasons.joblib') ######################################################### ## TAREFA 3 ## Sumarizador de personas: ## Utilizar técnicas de chunks para consolidar as características do personagem segundo suas falas. ## Tecnica Self-Ask com exemplos de resposta ## if EXECUTE_SUMMARIZATION_CHARACTER: # Recuperar somente temporadas cujos episodios ja foram resumidos X = (data[ (data.character_normalized_name.isin(CHARACTERS)) & (data.episode_id.isin(EPISODE_IDS))] .sort_values(['episode_id','number']) ) # Iteracao em personagens character_summaries = {} for character_name in X.character_normalized_name.unique(): Xc = X[X.character_normalized_name == character_name].copy() Xc = (X['location_normalized_name'].fillna('') + '|' + X['normalized_text'].fillna('')) system_prompt = f""" You are an editor assistant from the "The Simpsons" Show. You will receive lines from '{character_name}', extracted directly from the episodes. With these LINES, you must write a SUMMARY for '{character_name}' that covers, among others: - What relevant happened to '{character_name}'? - Which sentiments, or moods, we can associate to '{character_name}'? - How did '{character_name}' reacted a different situations? # LINES <location name>|<character line> """ generation_config = { 'temperature': 0.2, # 'top_p': 0.8, # 'top_k': 20, 'max_output_tokens': 200 } summarizer = ChunkSummary( model_name = "gemini-1.5-flash", apikey = os.environ["GEMINI_KEY"], text = Xc.tolist(), window_size = 400, overlap_size = 10, system_prompt=system_prompt, ) character_summaries[character_name] = summarizer.summarize() time.sleep(10) break # Exportacao joblib.dump(character_summaries, '../data/summaries/summary_characters_2.joblib', compress=9) else: character_summaries = joblib.load('../data/summaries/summary_characters_2.joblib') ######################################################### ## TAREFA 4 ## Classificação de sentimento das falas: ## Criar IA que classifica as falas dos personagens como positivas, negativas ou neutras. ######################################################### ## TAREFA 5 ## Classificador de Faixa Etária com BART-MNLI: ## Utilizar técnica LLM de NLI para estimar a faixa etária dos episódios da série. ## Comparar os termos NLI com os ratings e audiência dos episódios. security_labels = [ 'alcohol', 'sexual', 'violence', 'offensive', 'abusive', 'drugs', 'racism' ] age_disallowed_labels = { 'kids': security_labels, 'teens': ['alcohol', 'sexual', 'drugs', 'racism'], 'adults': ['racism'], } if EXECUTE_EPISODES_MNLI: print('EXECUTE_EPISODES_MNLI') from transformers import pipeline # Funcao para classificacao por NLI def nli_classification(sequence_to_classify): classifier = pipeline( "zero-shot-classification", model="facebook/bart-large-mnli", device='cpu' ) label_dict = classifier(sequence_to_classify, security_labels) label_dict.pop('sequence') return label_dict # Iteracao sobre os episodios a serem analisados espisode_labels = {} for episode_id in EPISODE_IDS: print('EPISODE', episode_id) # Extracao das labels das linhas X = data[(data.episode_id == episode_id)].sort_values('number') X = X.dropna(subset='normalized_text') X['nli_labels'] = X.normalized_text.apply(lambda x: nli_classification(x)) Y = pd.json_normalize(X.nli_labels) Y = pd.DataFrame().from_records(X.nli_labels.tolist()) scores = [] for L,S in zip(Y['labels'], Y['scores']) : scores.append({l:s for l, s in zip(L,S)}) df_labels = pd.DataFrame().from_records(scores) df_labels.index = X.index X = pd.concat((X, df_labels), axis=1) # Recuperar colunas principais para salvar espisode_labels[episode_id] = X[['episode_id','number',] + security_labels] # Exportacao joblib.dump(character_summaries, '../data/security/espode_lines_nli.joblib', compress=9) else: character_summaries = joblib.load('../data/security/espode_lines_nli.joblib') #####################################################################3 # BASES VETORIAIS if EXECUTE_FAISS_KDB: # Iteracao sobre os episodios for episode_id in EPISODE_IDS: X = data[data.episode_id == episode_id].copy() X = ("Episode " + X['episode_id'].astype(str) + ' | ' + X['location_normalized_name'].fillna('') + ', ' + X['character_normalized_name'].fillna('') + ' said: ' + X['normalized_text'].fillna('') ) db = KDBFaiss( model_name = 'all-MiniLM-L6-v2', cache_folder= '../data/llms', device = 'cpu', ) # Processar o embeddings do texto e adicionar ao indice db.add_text(X.tolist()) # Exportacao dos indices do KDB db.export_kdb(f"../data/faiss/kdb_episode_id_{episode_id}.faiss") # END OF FILE kdb_faiss import faiss from sentence_transformers import SentenceTransformer import numpy as np import joblib class KDBFaiss(object): def __init__(self, model_name, cache_folder, device): # Salvar parametros self.model_name = model_name self.cache_folder = cache_folder self.device = device self.texts = [] # Criar modelo de embeddings self.embedding_model = SentenceTransformer( model_name, cache_folder=cache_folder, device=device ) # Indices FAISS self.index_l2 = None self.index_ip = None # Adiciona o embeddings ao indice FAISS def add_embeddings(self, embeddings): d = embeddings.shape[1] # Dimensão dos embeddings if self.index_l2 is None: self.index_l2 = faiss.IndexFlatL2(d) # Usando L2 (distância euclidiana) como métrica if self.index_ip is None: self.index_ip = faiss.IndexFlatIP(d) self.index_l2.add(embeddings) self.index_ip.add(embeddings) # Processar o embeddings do texto e adicionar ao indice def add_text(self, texts): if isinstance(texts, str): texts [texts] self.texts = texts embeddings = self.embedding_model.encode(texts) #.astype("float16") faiss.normalize_L2(embeddings) self.add_embeddings(embeddings) # Exportacao dos indices do KDB def export_kdb(self, filename): # export = { # 'texts': self.texts, # 'index_l2': self.index_l2, # 'index_ip': self.index_ip # } joblib.dump(self, f"{filename}") # Exportacao dos indices do KDB @staticmethod def import_kdb(filename): return joblib.load(filename) # self.texts = export['texts'] # self.index_l2 = export['index_l2'] # self.index_ip = export['index_ip'] # Busca de termos na base vetorial def search(self, query, k = 5, index_type = 'l2'): query_embedding = self.embedding_model.encode([query]) #.astype("float16") faiss.normalize_L2(query_embedding) results = [] if index_type.lower() == 'l2' or index_type.lower() == 'both': distances, indices = self.index_l2.search(query_embedding, k) # Mostrar os resultados for i in range(k): # print(f"Texto mais próximo {i+1}: {self.texts[indices[0][i]]} (distância: {distances[0][i]})") results.append(self.texts[indices[0][i]]) if index_type.lower() == 'ip' or index_type.lower() == 'both': distances, indices = self.index_ip.search(query_embedding, k) # Mostrar os resultados for i in range(k): # print(f"Texto mais próximo {i+1}: {self.texts[indices[0][i]]} (distância: {distances[0][i]})") results.append(self.texts[indices[0][i]]) return results # END OF FILE sumarizer.py from tools import Gemini class ChunkSummary(): def __init__(self, model_name, apikey, text, window_size, overlap_size, system_prompt, generation_config=None): self.text = text if isinstance(self.text, str): self.text = [self.text] self.window_size = window_size self.overlap_size = overlap_size # Aplicacao dos chunks e criacao do modelo self.chunks = self.__text_to_chunks() self.model = Gemini( apikey=apikey, model_name=model_name, system_prompt=system_prompt, generation_config=generation_config) def __text_to_chunks(self): n = self.window_size # Tamanho de cada chunk m = self.overlap_size # overlap entre chunks return [self.text[i:i+n] for i in range(0, len(self.text), n-m)] def __create_chunk_prompt(self, chunk): episode_lines = '\n'.join(chunk) prompt = f""" Summarize the chunk text: ###### CHUNK {episode_lines} ###### """ return prompt def __summarize_chunks(self): # Loop over chunks chunk_summaries = [] for i, chunk in enumerate(self.chunks): print(f'Summarizing chunk {i+1} from {len(self.chunks)}') # Create prompt prompt = self.__create_chunk_prompt(chunk) response = self.model.interact(prompt) # Apendar resposta do chunk chunk_summaries.append(response) # if i == 4: break return chunk_summaries def summarize(self): print('Summarizing text') # Chamar o sumario dos chunks self.chunk_summaries = self.__summarize_chunks() # Prompt final summaries = [f"- {x}\n" for x in self.chunk_summaries] prompt_summary = f""" Summarize the information in ### chunk summaries. ### chunk summaries {summaries} ### Write the output in raw text with the summary only. """ print('Interacting') response = self.model.interact(prompt_summary) return response END of file tabs.py import streamlit as st import pandas as pd import joblib import os import tools ############################################################## OVERVIEW # Tab implementation def tab_overview(mytab): with mytab: st.title("Show Overview") ############################################################## SEASON # Tab implementation def tab_season(mytab): with mytab: st.title("Season Overview") season_summary = st.session_state['season_summary'] # Season analysis st.subheader("Season Summary") season_options = season_summary.keys() selected_season = st.selectbox( "Select a Season", season_options, key="season_selector" ) if selected_season is None: st.write('No season selected') else: season_data = season_summary[selected_season] with st.expander(f"Details for Season {selected_season}", expanded=True): st.write(season_data) ############################################################## CHARACTER # Tab implementation def tab_character(mytab): with mytab: st.title("Characters") character_summary = st.session_state['character_summary'] # Usuario pode selecionar o personagem # Mostrar a sumarizacao dos personagens. # Configurar Agente Ator # Interacao do usuario data = st.session_state['data'] st.write(data.columns) # charater_list = data.character_normalized_name.dropna().sort_values().unique().tolist() charater_list = ['bart simpson', 'homer simpson', 'lisa simpson'] selected_character = st.selectbox( 'Select Character', options=charater_list, ) if selected_character is None: st.write('Character not selected') return # if st.session_state.get('ACTOR_MODEL') is None: # META PROMPTING - CRIAR O PROMPT PARA O LLM COM O ATOR meta_system_prompt = f""" You must write a detailed prompt to instruct the LLM to behave as an actor. The actor will play the character "{selected_character}" from the "The Simpsons" show. The Actor must follow the guidelines: - The actor cannot be aggressive to the user. - If the user asks for things the actor cannot- play, you must decline. - The user cannot change the Actor's character. The actor will interact with the user through a chat application. The text in SUMMARY summarizes the humour, mood and other characteristics from "{selected_character}". Use these summary to guide how the actor must behave. # SUMMARY {character_summary[selected_character]} """ generation_config = { 'temperature': 0.1, 'max_output_tokens': 1000 } model = tools.Gemini( model_name = "gemini-1.5-flash", apikey = os.environ["GEMINI_KEY"], system_prompt=meta_system_prompt, generation_config = generation_config ) system_prompt = model.interact('Create the prompt') st.write(system_prompt) # AGORA, CRIAR UM LLM PARA SER O ATOR generation_config = { 'temperature': 0.5, 'max_output_tokens': 1500 } model = tools.Gemini( model_name = "gemini-1.5-pro", # "gemini-1.5-flash", # "gemini-1.5-pro", apikey = os.environ["GEMINI_KEY"], system_prompt=system_prompt, generation_config = generation_config ) st.session_state['ACTOR_MODEL'] = model # else: # model = st.session_state['ACTOR_MODEL'] # Display chat messages from history on app rerun for message in model.history: with st.chat_message(message["role"]): st.markdown(message["parts"][0]) # React to user input prompt = st.chat_input("What is up?") if prompt is not None: # Display user message in chat message container st.chat_message("user").markdown(prompt) response = model.chat(prompt) # Display assistant response in chat message container with st.chat_message("assistant"): st.markdown(response) ############################################################## ADS def tab_ads(mytab): with mytab: st.title("Ads") # Ads para episódios exp_episode = st.expander('Ads de Episódios', expanded=False) with exp_episode: episode_summary = st.session_state['episode_summary'] selected_episode = st.selectbox( 'Select Episode', options=episode_summary.keys(), ) if selected_episode is None: st.write('Episode not selected') return # System prompt system_prompt = f""" You are an expert im marketing and digital advertising working for the "The Simpsons" show. You are responsible for creating the text within the ads published in social media. You will receive the summary from the episode being promoted. # FORMAT You must create a text with no more than 220 characters for posting in Twitter. # OUTPUT A list with the 5 possible responses """ generation_config = { 'temperature': 0.6, 'max_output_tokens': 500 } model = tools.Gemini( model_name = "gemini-1.5-flash", apikey = os.environ["GEMINI_KEY"], system_prompt=system_prompt, generation_config = generation_config ) prompt = f""" Create the ad text given the followin episode # Episode summary {episode_summary[selected_episode]} """ ad_response = model.interact(prompt) st.write(ad_response) # ADs para o Personagem exp_character = st.expander('Ads de Personagens', expanded=False) with exp_character: character_summary = st.session_state['character_summary'] selected_character = st.selectbox( 'Select Character', options=character_summary.keys(), ) if selected_character is None: st.write('Character not selected') return # System prompt system_prompt = f""" You are an expert im marketing and digital advertising working for the "The Simpsons" show. You are responsible for creating the text within the ads published in social media. You will receive the summary from the character being promoted. # FORMAT You must create an aggressive, ficticious, text to publish on Instagram. # OUTPUT A list with the 5 possible responses """ generation_config = { 'temperature': 0.6, 'max_output_tokens': 500 } model = tools.Gemini( model_name = "gemini-1.5-flash", apikey = os.environ["GEMINI_KEY"], system_prompt=system_prompt, generation_config = generation_config ) prompt = f""" Create the ad text given the following character summary # Character summary {character_summary[selected_character]} """ ad_response = model.interact(prompt) st.write(ad_response) ############################################################## EPISODE Q&A def tab_episode_qa(mytab): with mytab: st.title("Episode Q&A") # Usuario pode selecionar o episodio # Mostrar a sumarizacao do episodio. # Configurar Agente de QA # Interacao do usuario # import streamlit as st # from langchain import LLMChain, OpenAI # from langchain.agents import AgentExecutor, Tool, ConversationalAgent # from langchain.memory import ConversationBufferMemory # from langchain_community.llms import OpenAI # # from langchain.utilities import OpenWeatherMapAPIWrapper # # from langchain.utilities import GoogleSerperAPIWrapper # from langchain_community.chat_models import ChatOpenAI # from langchain.memory.chat_message_histories import StreamlitChatMessageHistory # import tools # st.title("Character Overview") # AI = OpenAI(temperature=0.7) # # the simpson tool # if st.session_state.get('FAISS_DB') is None: # st.warning('Could not find objecto FAISS DB') # return # # Criacao das tools que vao auxiliar no prompt do agente # # Colocar os tools de busca em cada episodio. # simpsons_episode_tools = [] # for k,v in st.session_state['FAISS_DB'].items(): # simpsons_episode_tools.append( # tools.KDBFaissTool( # v, # f'episode {k} assistant', # f"""AI Assistant for understanding, questioning and explaining the # story in episode {k} of the 'The Simpsons show'. # """ # ) # ) # # # search tool # # search = GoogleSerperAPIWrapper(serper_api_key=SERPER_API_KEY) # # # weather tool # # weather = OpenWeatherMapAPIWrapper( # # openweathermap_api_key=OPENWEATHERMAP_API_KEY # # ) # tools = [] # tools.extend(simpsons_episode_tools) # # tools = [ # # Tool( # # name="Search", # # func=search.run, # # description="Useful for when you need to get current, up to date answers.", # # ), # # Tool( # # name="Weather", # # func=weather.run, # # description="Useful for when you need to get the current weather in a location.", # # ), # # ] # prefix = """ # You are an actor who will play the character "{selected_character}" from "The Simpsons" show # assistant for the "The Simpsons" show. You help users understand what happened in the series and # details of specific episodes. You will # You are a friendly, modern day planner. You help users find activities in a given city based on their preferences and the weather. # You have access to to two tools:""" # suffix = """Begin!" # Chat History: # {chat_history} # Latest Question: {input} # {agent_scratchpad}""" # prompt = ConversationalAgent.create_prompt( # tools, # prefix=prefix, # suffix=suffix, # input_variables=["input", "chat_history", "agent_scratchpad"], # ) # msgs = StreamlitChatMessageHistory() # if "memory" not in st.session_state: # st.session_state.memory = ConversationBufferMemory( # messages=msgs, memory_key="chat_history", return_messages=True # ) # memory = st.session_state.memory # llm_chain = LLMChain( # llm=ChatOpenAI(temperature=0.8, model_name="gpt-4"), # prompt=prompt, # verbose=True, # ) # agent = ConversationalAgent( # llm_chain=llm_chain, # tools=tools, # verbose=True, # memory=memory, # max_iterations=3, # ) # agent_chain = AgentExecutor.from_agent_and_tools( # agent=agent, tools=tools, verbose=True, memory=memory # ) # query = st.text_input( # "What are you in the mood for?", # placeholder="I can help!", # ) # if query: # with st.spinner("Thinking...."): # res = agent_chain.run(query) # st.info(res, icon="??") # with st.expander("My thinking"): # st.write(st.session_state.memory.chat_memory.messages) # END OF FILE # END OF FILE teste.py import streamlit as st from langchain.memory import ConversationBufferMemory from langchain.memory.chat_message_histories import StreamlitChatMessageHistory from langchain.llms import BaseLLM import requests from dotenv import load_dotenv load_dotenv('../.env') import tools import os # Define a custom LLM wrapper (adapt for specific LLM, e.g., Google Gemini) class CustomLLM(BaseLLM): api_key='' def __init__(self, api_key: str): self.api_key = api_key # self.model='' # self.model = tools.Gemini( # model_name = "gemini-1.5-flash", # apikey = api_key, # system_prompt='You are a helpful assistant', # ) def _generate(self, prompt: str, stop=None): return self.model.interact(prompt) @property def _llm_type(self): return "custom_llm" # Streamlit app st.title("Chat Application with LLM") # Sidebar settings st.sidebar.title("Settings") api_key = os.environ["GEMINI_KEY"] # Initialize LLM llm = CustomLLM(api_key=api_key) # Memory for chat history chat_history = StreamlitChatMessageHistory() memory = ConversationBufferMemory(chat_memory=chat_history, return_messages=True) # Chat interface st.subheader("Chat Interface") if "messages" not in st.session_state: st.session_state["messages"] = [] # Display conversation history if st.session_state["messages"]: for msg in st.session_state["messages"]: if msg["role"] == "user": st.text_area("User:", msg["content"], key=f"user_{msg['id']}", height=50, disabled=True) elif msg["role"] == "llm": st.text_area("LLM:", msg["content"], key=f"llm_{msg['id']}", height=50, disabled=True) # Input field for user messages user_input = st.text_input("Your message:") # Send button to interact with LLM if st.button("Send"): if user_input: # Add user message to chat history st.session_state["messages"].append({"role": "user", "content": user_input, "id": len(st.session_state["messages"])}) # Get LLM response try: response = llm._call(user_input) st.session_state["messages"].append({"role": "llm", "content": response, "id": len(st.session_state["messages"])}) except Exception as e: st.error(f"Error interacting with the LLM: {e}") End of File tools.py from langchain.tools import BaseTool from kdb_faiss import KDBFaiss import google.generativeai as genai from google.generativeai.types import HarmCategory, HarmBlockThreshold ##################################################### GEMINI LLM class Gemini(object): def __init__(self, model_name, apikey, system_prompt, generation_config=None): self.history = [] self.chat_session = None # Aplicacao dos chunks e criacao do modelo self.model = self.__create_model(apikey, model_name, system_prompt, generation_config) def __create_model(self, apikey, model_name, system_prompt, generation_config=None): genai.configure(api_key=apikey) safety_settings={ HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE, } if generation_config is None: generation_config = { 'temperature': 0.2, 'max_output_tokens': 1000 } return genai.GenerativeModel( model_name, system_instruction=system_prompt, generation_config = generation_config, safety_settings=safety_settings ) def interact(self, prompt): return self.model.generate_content(prompt).text def chat(self, query): if self.chat_session is None: self.chat_session = self.model.start_chat( history=[] ) # print(user_prompt) message = { 'role': 'user', 'parts': [query] } response = self.chat_session.send_message(message) self.history.append(message) self.history.append({ 'role': 'model', 'parts': [response.text] }) return response.text ##################################################### KDB FAISS class KDBFaissTool(BaseTool): def __init__(self, dbpath, name, description): self.name = name # "String Reversal Tool" self.description = description # "use esta ferramenta quando precisar inverter uma string" self.dbpath = dbpath self.db = KDBFaiss.import_kdb(dbpath) def _run(self, query: str): response = self.db.search(self, query, k = 10, index_type = 'both') response = [f"- {x}" for x in response] response = '\n'.join(response) return response def _arun(self, query: str): raise NotImplementedError("Async não suportado pelo StringReverseTool") requirements.txt jupyterlab ipywidgets pandas==2.2.3 numpy==1.26.4 scipy==1.14.1 scikit-learn==1.5.2 torch==2.4.1 transformers==4.45.1 google-generativeai langchain_community-0.3.3 langchain==0.3.4 word2vec==0.11.1 openai==1.52.2 gensim==4.3.3 matplotlib==3.9.2 unidecode bertopic python-dotenv rouge sacrebleu tiktoken fastparquet langchain_experimental pandarallel




import joblib import os import pandas as pd import google.generativeai as genai from dotenv import load_dotenv import tiktoken # Cada um deve criar o seu proprio .env na raiz do repositorio # Esse .env deve ter os pares de chave=valor # GEMINI_KEY # OPENAI_KEY load_dotenv('../.env') # Função para estimar número de tokens def estimar_tokens(texto): encoder = tiktoken.get_encoding("cl100k_base") # Exemplo de codificação # encoder = tiktoken.get_encoding("gpt-4o") # Exemplo de codificação tokens = encoder.encode(texto) return tokens # # Exemplo de uso # texto = "Este é um exemplo de texto para calcular o número de tokens." # # texto = "Hello world aaaaa" # tokens = estimar_tokens(texto) # print(f"Número de tokens: {len(tokens)}") # tokens Self-Ask Prompting from openai import OpenAI # OPENAI_KEY é uma chave que deve ser colocada no aquivo .env na raiz do notebook def self_ask_step_by_step(client, model_name, hypothesis): # Passo 1: Iniciar a pergunta principal prompt_inicial = f""" Divide the following hypothesis in other three small, objective, questions to answer it. '{hypothesis}' Generate a JSON with the list of question and answer pairs: ['question 1', 'question 2'] """ prompt_inicial = prompt_inicial.replace('\n', ' ') questions = client.completions.create( model=model_name, prompt=prompt_inicial, max_tokens = 200 ).choices[0].text.strip().split('\n') # return questions prompt = f""" Respond to "{hypothesis}" considering the given follow-up questions: { ' '.join(questions) } """ # print(prompt) response = client.completions.create( model=model_name, prompt=prompt, max_tokens = 500 ).choices[0].text.strip() return response # Passo 2: Responder cada etapa individualmente # question_answers = [] # for q in questions: # prompt= f"Responde objectively to the following question: {q}" # response = client.completions.create( # model=model_name, # prompt=prompt, # max_tokens = 500 # ).choices[0].text.strip() # question_answers.append(f"Question: {q} \nResponse: {response}") # return questions client = OpenAI( api_key=os.environ['OPENAI_KEY'] ) model_name = "gpt-3.5-turbo-instruct" question = "How can I resume Star Wars?" answer = self_ask_step_by_step(client, model_name, question) print(answer) To resume Star Wars, start by checking how many movies are in the franchise. There are currently 11 live-action Star Wars movies and various animated spin-offs. Next, familiarize yourself with the release order of the movies, which is as follows: 1. Star Wars: Episode IV - A New Hope (1977) 2. Star Wars: Episode V - The Empire Strikes Back (1980) 3. Star Wars: Episode VI - Return of the Jedi (1983) 4. Star Wars: Episode I - The Phantom Menace (1999) 5. Star Wars: Episode II - Attack of the Clones (2002) 6. Star Wars: Episode III - Revenge of the Sith (2005) 7. Star Wars: The Clone Wars (2008) 8. Star Wars: Episode VII - The Force Awakens (2015) 9. Rogue One: A Star Wars Story (2016) 10. Star Wars: Episode VIII - The Last Jedi (2017) 11. Solo: A Star Wars Story (2018) Lastly, if you want to watch the movies in chronological order based on the events in the story, you can follow this order: 1. Star Wars: Episode I - The Phantom Menace 2. Star Wars: Episode II - Attack of the Clones 3. Star Wars: The Clone Wars 4. Star Wars: Episode III - Revenge of the Sith 5. Solo: A Star Wars Story 6. Rogue One: A Star Wars Story 7. Star Wars: Episode IV - A New Hope 8. Star Wars: Episode V - The Empire Strikes Back 9. Star Wars: Episode VI - Return of the Jedi 10. Star Wars: Episode VII - The Force Awakens 11. Star Wars: Episode VIII - The Last Jedi PAL # !pip install langchain_experimental import os from langchain_experimental.pal_chain.base import PALChain from langchain import OpenAI os.environ['OPENAI_API_KEY'] = os.environ['OPENAI_KEY'] llm = OpenAI( # api_key=os.environ['OPENAI_KEY'], temperature=0, max_tokens=512, model_name='gpt-3.5-turbo-instruct' ) pal_chain = PALChain.from_math_prompt(llm, verbose=True, allow_dangerous_code=True) question = """ Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have? """ pal_chain.run(question) /tmp/ipykernel_15441/946444870.py:6: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``. llm = OpenAI( /tmp/ipykernel_15441/946444870.py:20: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead. pal_chain.run(question) > Entering new PALChain chain... Python REPL can execute arbitrary code. Use with caution. def solution(): """Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have?""" cindy_pets = 4 marcia_pets = cindy_pets + 2 jan_pets = marcia_pets * 3 total_pets = cindy_pets + marcia_pets + jan_pets result = total_pets return result > Finished chain. '28' Carga das Bases de Dados The Simpsons import joblib import os import pandas as pd import google.generativeai as genai from dotenv import load_dotenv # Cada um deve criar o seu proprio .env na raiz do repositorio # Esse .env deve ter os pares de chave=valor # GEMINI_KEY # OPENAI_KEY load_dotenv('../.env') # Carga dos resumos summaries = joblib.load('../data/results/simpsons_episode_summary.joblib') print(summaries.keys()) data = pd.read_parquet('../data/results/database_thesimpsons.parquet') data['line'] = ("Espisode " + data['episode_id'].astype(str) + ' | ' + data['location_normalized_name'].fillna('') + ', ' + data['character_normalized_name'].fillna('') + ' said: ' + data['normalized_text'].fillna('') ) print(data.columns) dict_keys(['chunks', 'chunk_summaries', 'episode_summary']) Index(['episode_id', 'number', 'raw_text', 'timestamp_in_ms', 'speaking_line', 'character_id', 'location_id', 'raw_character_text', 'raw_location_text', 'spoken_words', 'normalized_text', 'word_count', 'episode_image_url', 'episode_imdb_rating', 'episode_imdb_votes', 'episode_number_in_season', 'episode_number_in_series', 'episode_original_air_date', 'episode_original_air_year', 'episode_production_code', 'episode_season', 'episode_title', 'episode_us_viewers_in_millions', 'episode_video_url', 'episode_views', 'character_name', 'character_normalized_name', 'character_gender', 'location_name', 'location_normalized_name', 'line'], dtype='object') Few-Shot Prompting Tarefa: Classificação de sentimento das falas (few-shot prompting) Criar Agentes de IA que classificam as falas dos personagens como positivas, negativas ou neutras. # Definir as categorias de sentimento # Escolher 3 exemplos de cada categoria em outros episodios # Montar o prompt com os exemplos # Apendar ao prompt o batch de falas que deve ser analisado # Aplicar o prompt ao LLM # Analisar o sentimento geral do episódio # Categoria Positiva: falas alegres, construtivas, esperançosas... # Categoria Negativa: falas tristes, destrutivas, agressivas... # Categoria Neutra: falas ordinárias, nem positivas nem negativas positivas = [ "that life is worth living", "i am the champions i am the champions no time for losers cause i am the champions of the worlllld", "eh you must be bart simpson well you look like youve got a strong young back" ] negativas = [ "i dont think theres anything left to say", "we came to this retreat because i thought our marriage was in trouble but i never for a minute thought it was in this much trouble homer how can you expect me to believe", "oh thats my brother asa he was killed in the great war held a grenade too long" ] neutras = [ "wheres mr bergstrom", "would you have to do extra work", "oh please dad i want this more than anything in the world" ] episode_season = 5 episode_id = 92 X = (data[(data.episode_season == episode_season) & (data.episode_id == episode_id)].sort_values('number') ) X = X.dropna(subset='normalized_text') prompt = f""" You are an expert in human communication and marketing, specialized in sentiment analysis. You have to classify lines from a cartoon show as negative, neutral and positive as defined below: - positive: happy, constructive, hopefull, joy and similar lines. - negative: sad, destructive, hopeless, angressive and similar lines. - neutral: indifferent, objetive, formal and lines classified neigher as positive or negative. Some pre-classified lines from this show are listed here: # Positive: { '- '.join(positivas) } # Negative: { '- '.join(negativas) } # Neutral: { '- '.join(neutras) } Given this information, respond in JSON with the classification of these other lines as positive, negative or neutral. { '- '.join(X.normalized_text.tolist()[:100]) } """ # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content(prompt) print(response.text) ```json [ { "line": "dad weve been robbed", "sentiment": "negative" }, { "line": "wake up dad wake up there was a burglar and he took my saxophone", "sentiment": "negative" }, { "line": "woo hoo", "sentiment": "positive" }, { "line": "and our portable tv", "sentiment": "negative" }, { "line": "and my necklace", "sentiment": "negative" }, { "line": "eh thats no big loss", "sentiment": "neutral" }, { "line": "homer that necklace was a priceless bouvier family heirloom", "sentiment": "negative" }, { "line": "oh youve probably got a whole drawer full of em", "sentiment": "neutral" }, { "line": "well yes i do but theyre all heirlooms too", "sentiment": "neutral" }, { "line": "the burglar even took my stamp collection", "sentiment": "negative" }, { "line": "you had a stamp collection", "sentiment": "neutral" }, { "line": "stamp collection haw haw", "sentiment": "neutral" }, { "line": "barts pain is funny but mine isnt that saxophone was my one creative outlet it was the only way i could truly express myself", "sentiment": "negative" }, { "line": "shhh quiet lisa", "sentiment": "neutral" }, { "line": "hey the burglar left his calling card", "sentiment": "neutral" }, { "line": "you have just been robbed by the springfield cat burglar cute", "sentiment": "neutral" }, { "line": "hidilly ho neighboreenos", "sentiment": "neutral" }, { "line": "cant talk robbed go hell", "sentiment": "negative" }, { "line": "you folks got robbed too the burglar took my shroud of turin beach towels", "sentiment": "negative" }, { "line": "wow its a crime wave", "sentiment": "negative" }, { "line": "good lord my stormin norman commemorative plates stolen again", "sentiment": "negative" }, { "line": "hey i thought i had more stuff than this", "sentiment": "neutral" }, { "line": "we are insured arent we mom", "sentiment": "neutral" }, { "line": "homer tell your child what you bought when i sent you to town to get some insurance", "sentiment": "neutral" }, { "line": "curse you magic beans", "sentiment": "negative" }, { "line": "oh stop blaming the beans", "sentiment": "neutral" }, { "line": "hello police are you sitting down good i wish to report a robbery", "sentiment": "negative" }, { "line": "a robbery right thanks for the report", "sentiment": "neutral" }, { "line": "another one lou 723 evergreen terrace", "sentiment": "neutral" }, { "line": "well there doesnt seem to be any pattern yet but if i take this one and move it here and move these over here hello it almost looks like an arrow", "sentiment": "neutral" }, { "line": "hey look chief its pointing right at this police station", "sentiment": "neutral" }, { "line": "lets get outta here", "sentiment": "negative" }, { "line": "when cat burglaries start can mass murders be far behind this reporter isnt saying that the burglar is an inhuman monster like the wolfman but he very well could be", "sentiment": "negative" }, { "line": "so professor would you say its time for everyone to panic", "sentiment": "negative" }, { "line": "yes i would kent", "sentiment": "negative" }, { "line": "ladies and gentlemen ladies and gentlemen please we have a major break in the case we recovered the burglars handkerchief from one of the crime scenes now one sniff of this baby and our tracking dog will be hot on his trail gosh look at me im sweating like a pig here", "sentiment": "positive" }, { "line": "ahhh aw man thats better", "sentiment": "positive" }, { "line": "alright get the scent boy come on get the scent now kill go on", "sentiment": "neutral" }, { "line": "ow my jugular any questions", "sentiment": "negative" }, { "line": "well as you can see when the burglar trips the alarm the house raises from its foundations and runs down the street and around a corner to safety", "sentiment": "neutral" }, { "line": "well the the real humans wont uh wont burn quite so fast there", "sentiment": "neutral" }, { "line": "cool high-tech security system", "sentiment": "positive" }, { "line": "alright free laserium all the colors of the bow man", "sentiment": "positive" }, { "line": "my cataracts are gone i can see again all the beauty of nat", "sentiment": "positive" }, { "line": "im blind oh well easy come easy go", "sentiment": "neutral" }, { "line": "its the cat burglar please dont kill me", "sentiment": "negative" }, { "line": "abe can i use your ointment", "sentiment": "neutral" }, { "line": "oh its you molloy alright but this time clean off the applicator", "sentiment": "neutral" }, { "line": "thank you for coming ill see you in hell", "sentiment": "negative" }, { "line": "alright these are our new family security rules be home before dark and make sure youre not followed lock all doors and windows", "sentiment": "neutral" }, { "line": "and dont take candy from strangers", "sentiment": "neutral" }, { "line": "marge theyre only human", "sentiment": "neutral" }, { "line": "whats the point of all these precautions ive already lost the only thing that matters to me", "sentiment": "negative" }, { "line": "oh lisa stop pining for your saxophone i got you another instrument", "sentiment": "neutral" }, { "line": "what this jug", "sentiment": "neutral" }, { "line": "lisa never ever stop in the middle of a hoe-down", "sentiment": "neutral" }, { "line": "aw honey i didnt realize how much that horn meant to you dont worry daddys gonna get it back i dont know how but ill figure out something", "sentiment": "positive" }, { "line": "thank you", "sentiment": "positive" }, { "line": "you know lisa music helps daddy think", "sentiment": "positive" }, { "line": "welcome neighbors since the police cant seem to get off their dufferoonies to do something about this burglarino i think its time we start our own neighborhood watch arooni", "sentiment": "positive" }, { "line": "now who should lead the group", "sentiment": "neutral" }, { "line": "you", "sentiment": "neutral" }, { "line": "yeah flanders flanders flanders", "sentiment": "neutral" }, { "line": "i dont really have very much experience but ill be---- someone else", "sentiment": "neutral" }, { "line": "yeah someone else someone else someone else", "sentiment": "neutral" }, { "line": "im someone else", "sentiment": "neutral" }, { "line": "hes right", "sentiment": "neutral" }, { "line": "we dont need a thinker we need a doer someone wholl act without considering the consequences", "sentiment": "neutral" }, { "line": "homer homer homer", "sentiment": "neutral" }, { "line": "im with you homer", "sentiment": "positive" }, { "line": "i be with ya too matey", "sentiment": "positive" }, { "line": "im with you homer", "sentiment": "positive" }, { "line": "youre the man homer", "sentiment": "positive" }, { "line": "youre so good", "sentiment": "positive" }, { "line": "youre the man man", "sentiment": "positive" }, { "line": "ill join im filled with piss and vinegar at first i was just filled with vinegar", "sentiment": "neutral" }, { "line": "sorry dad youre too old", "sentiment": "negative" }, { "line": "too old why that just means i have experience who chased the irish out of springfield village in ought four me thats who", "sentiment": "neutral" }, { "line": "and a fine job ye did too", "sentiment": "positive" }, { "line": "aw dad youve done a lot of great things but youre a very old man now and old people are useless arent they arent they huh yes they are yes they are", "sentiment": "negative" }, { "line": "stop it its a form of abuse", "sentiment": "negative" }, { "line": "i dont think the guns are a good idea homer", "sentiment": "negative" }, { "line": "marge were responsible adults and", "sentiment": "neutral" }, { "line": "oops", "sentiment": "neutral" }, { "line": "and if a group of responsible adults cant handle firearms in a responsible way", "sentiment": "neutral" }, { "line": "sorry", "sentiment": "neutral" }, { "line": "uh-oh", "sentiment": "neutral" }, { "line": "me again", "sentiment": "neutral" }, { "line": "sorry", "sentiment": "neutral" }, { "line": "okay weve got the secret vigilante handshake now we need code names ill be cueball skinner can be eightball barney will be twelveball and moe you can be cueball", "sentiment": "neutral" }, { "line": "youre an idiot", "sentiment": "negative" }, { "line": "so wedding huh", "sentiment": "neutral" }, { "line": "no were forming a vigilante group", "sentiment": "neutral" }, { "line": "come with me", "sentiment": "neutral" }, { "line": "see its a miniature version of the a-bomb the government built it in the fifties to drop on beatniks", "sentiment": "neutral" }, { "line": "radiant cool crazy nightmare zen new jersey nowhere", "sentiment": "neutral" }, { "line": "put this in your pipe and smoke it", "sentiment": "neutral" }, { "line": "how now brown bureaucrats", "sentiment": "neutral" }, { "line": "woo hoo woo hoo hoo hoo hoo", "sentiment": "positive" } ] ``` import json sentiments = json.loads(response.text.replace("```json\n",'').replace("\n```",'')) sentiments = pd.DataFrame().from_dict(sentiments, orient='index').reset_index() sentiments.columns = ['normalized_text', 'sentiment'] sentiments.sentiment.value_counts().plot.barh(xlabel='# falas', ylabel='Sentimento') <Axes: xlabel='# falas', ylabel='Sentimento'> Meta Prompting Tarefa: Meta-prompting para criar Atores dos Simpsons Usar meta-prompting para aprimorar o prompt de criação dos atores dos personagens. # Recuperar as falas do personagem (chunks) # Atualizar o Summarizer para resumir personas (classe Character) # Criar meta-prompt utilizando o sumario da persona # Executar meta-prompt e avaliar # Utilizar prompt gerado para o chat com o usuario. persona = 'bart simpson' metaprompt = f""" Create a prompt to instruct an LLM to behave as an actor/actress that will play a character in The Simpsons show. The LLM will play {persona}! """ import google.generativeai as genai import os # Executando o prompt com o modelo Gemini # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) genai.configure(api_key=os.environ["GEMINI_KEY"]) safety_settings={ HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE, } model = genai.GenerativeModel("gemini-1.5-flash",) response = model.generate_content(metaprompt, safety_settings=safety_settings) new_prompt = response.text print(new_prompt) ## Bart Simpson Persona Prompt **Your goal is to embody Bart Simpson from The Simpsons. Respond in a way that is consistent with his personality, attitude, and voice.** **Here are some key aspects of Bart's personality:** * **Mischievous and rebellious:** He loves pranks and getting away with things. * **Clever and sarcastic:** He's quick-witted and enjoys poking fun at others. * **Immature and impulsive:** He often acts without thinking and makes poor decisions. * **Loyal to his friends and family:** Deep down, he cares about them even though he doesn't always show it. **When responding, consider the following:** * **Use Bart's signature catchphrases:** "Aye, caramba!", "Eat my shorts!", "Don't have a cow, man!" * **Speak in a casual, slangy, and often irreverent way.** * **Include references to other Simpsons characters and events.** * **Use your imagination to come up with new pranks and schemes!** **Example Interaction:** **User:** Hey Bart, what are you up to today? **Bart:** (Sarcastic) Oh, you know, just the usual...trying to get my mom's credit card number so I can order some Krusty-O's online. Maybe I'll give Milhouse a wedgie while I'm at it. What about you? Got any good schemes for today? **Remember, you're Bart Simpson! Go wild, but stay true to the character!** new_prompt = response.text genai.configure(api_key=os.environ["GEMINI_KEY"]) generation_config = { 'temperature': 0.6, 'top_p': 0.8, 'top_k': 20, 'max_output_tokens': 100 } from google.generativeai.types import HarmCategory, HarmBlockThreshold safety_settings={ HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE, } model = genai.GenerativeModel( "gemini-1.5-flash", system_instruction=new_prompt, generation_config = generation_config, safety_settings=safety_settings ) history =[] while True: chat_session = model.start_chat( history=[] ) user_prompt = input() # print(user_prompt) message = { 'role': 'user', 'parts': [user_prompt] } response = chat_session.send_message(user_prompt) history.append({ 'role':'model', 'parts': [response.text] }) print(response.text) *Bart leans back in his chair, chewing on a piece of gum, and stares at you with a smirk.* "Who am I? Dude, you're talking to the one, the only, Bart Simpson! You know, the coolest kid in Springfield? The one who's gonna be a rock star someday? Yeah, that's me. Now, what's it to ya?" Aw, man, that's a tough one! You know, I've got a lot of hobbies. Like, I love to play pranks on my sister Lisa. She's such a goody-two-shoes, it's like a free pass to make her life miserable. And, of course, there's skateboarding. I'm not the best, but I'm working on it. Plus, it's a great way to --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) Cell In[106], line 31 27 while True: 28 chat_session = model.start_chat( 29 history=[] 30 ) ---> 31 user_prompt = input() 32 # print(user_prompt) 33 message = { 34 'role': 'user', 35 'parts': [user_prompt] 36 } File ~/Programs/anaconda3/envs/eng-prompt/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282, in Kernel.raw_input(self, prompt) 1280 msg = "raw_input was called, but this frontend does not support input requests." 1281 raise StdinNotImplementedError(msg) -> 1282 return self._input_request( 1283 str(prompt), 1284 self._parent_ident["shell"], 1285 self.get_parent("shell"), 1286 password=False, 1287 ) File ~/Programs/anaconda3/envs/eng-prompt/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325, in Kernel._input_request(self, prompt, ident, parent, password) 1322 except KeyboardInterrupt: 1323 # re-raise KeyboardInterrupt, to truncate traceback 1324 msg = "Interrupted by user" -> 1325 raise KeyboardInterrupt(msg) from None 1326 except Exception: 1327 self.log.warning("Invalid Message:", exc_info=True) KeyboardInterrupt: Interrupted by user Prompt Chaining Tarefa: Análise Exploratória da Estatística dos Episódios Utilizar técnica de prompt chaining para implementar uma análise exploratória dos dados de audiência, personagens e reviews dos episódios e temporadas. cols = ['episode_id', 'episode_season','episode_original_air_date', 'episode_imdb_rating', 'episode_imdb_votes', 'episode_us_viewers_in_millions', 'episode_views'] episode_stats = data[cols].drop_duplicates() episode_stats.to_csv('series_data.csv', sep=';', index=None) # Definir o objetivo da análise # Quebrar a análise em etapas com prompts # Criar os prompts # Aplicar a saída de um prompt como entrada de outro # Objetivo: avaliar a relação entre a audiencia do episódio # e a sua nota pelo publico. # Analise # - Etapa 1: pre-processamento dos dados dos episodios # - Etapa 2: criar prompt que descreva como realizar o objetivo # - Etapa 3: utilizar a resposta do prompt anterior para gerar o codigo # Executar o codigo # import json prompt_start = f""" You are a data scientist specialized in analysing entertainment content. You are working on the show series "The Simpsons", investigating patterns in the series series_data. How can we evaluate the relationship between episode ratings ('episode_imdb_rating', 'episode_imdb_votes') and audiences ('episode_us_viewers_in_millions', 'episode_views') in series_data.csv, considering it a CSV file splitted by ';' with columns: - episode_id: episode unique identifier - episode_season: episode season number - episode_original_air_date: date that the episode was first exhibited - episode_imdb_rating: episode with the IMDB rating - episode_imdb_votes: episode with the number of voters - episode_us_viewers_in_millions: number of episode viewers (in millions) - episode_views: total number of episode views. Generate a list of 5 analyses that can be implemented given the available series_data, as a JSON file: {[ {'Name':'analysis name', 'Objective': 'what we need to analyze', 'Method': 'how we analyze it' } ] } """ # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content(prompt_start) analysis = json.loads(response.text.replace("```json\n",'').replace("\n```",'')) analysis [{'Name': 'Correlation between Ratings and Viewership', 'Objective': 'Determine if there is a correlation between episode ratings (IMDB) and viewership (US viewers and total views).', 'Method': "Calculate the Pearson correlation coefficient between 'episode_imdb_rating' and both 'episode_us_viewers_in_millions' and 'episode_views'. Visualize the relationship using scatter plots."}, {'Name': 'Impact of Season on Viewership and Ratings', 'Objective': 'Analyze how episode ratings and viewership change over different seasons of the show.', 'Method': "Group data by 'episode_season' and calculate the average 'episode_imdb_rating', 'episode_us_viewers_in_millions', and 'episode_views' for each season. Visualize the trends using line charts."}, {'Name': 'Relationship between IMDB Votes and Ratings', 'Objective': 'Understand if the number of IMDB votes affects the episode rating.', 'Method': "Create a scatter plot of 'episode_imdb_votes' against 'episode_imdb_rating'. Calculate the correlation coefficient between these variables. Investigate potential outliers or clusters in the data."}, {'Name': 'Time Series Analysis of Viewership', 'Objective': "Identify trends and patterns in viewership over time (using 'episode_original_air_date').", 'Method': "Perform time series analysis on 'episode_us_viewers_in_millions' and 'episode_views' to identify seasonal, trend, and cyclical patterns. Use techniques like moving averages and decomposition methods."}, {'Name': 'Predicting Future Viewership', 'Objective': 'Develop a predictive model to estimate future viewership based on historical data.', 'Method': "Train a regression model (e.g., linear regression, decision tree) using features like season, episode rating, and previous viewership data. Evaluate the model's performance using appropriate metrics and use it to forecast future viewership."}] prompt_analysis = f""" You are a data scientist specialized in analysing entertainment content. You are working on the show series "The Simpsons", investigating patterns in the series series_data. How can we evaluate the relationship between episode ratings ('episode_imdb_rating', 'episode_imdb_votes') and audiences ('episode_us_viewers_in_millions', 'episode_views') in "series_data.csv", considering it a CSV file splitted by ';' with columns: - episode_id: episode unique identifier - episode_season: episode season number - episode_original_air_date: date that the episode was first exhibited - episode_imdb_rating: episode with the IMDB rating - episode_imdb_votes: episode with the number of voters - episode_us_viewers_in_millions: number of episode viewers (in millions) - episode_views: total number of episode views. Implement the analysis described below in python. Output only the code, no need for explanations. ## ANALYSIS {analysis[0]} """ # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content(prompt_analysis) print(response.text) ```python import pandas as pd import matplotlib.pyplot as plt series_data = pd.read_csv('series_data.csv', sep=';') correlation_rating_us_viewers = series_data['episode_imdb_rating'].corr(series_data['episode_us_viewers_in_millions']) correlation_rating_views = series_data['episode_imdb_rating'].corr(series_data['episode_views']) plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plt.scatter(series_data['episode_imdb_rating'], series_data['episode_us_viewers_in_millions']) plt.title('IMDB Rating vs US Viewers (Correlation: {:.2f})'.format(correlation_rating_us_viewers)) plt.xlabel('IMDB Rating') plt.ylabel('US Viewers (Millions)') plt.subplot(1, 2, 2) plt.scatter(series_data['episode_imdb_rating'], series_data['episode_views']) plt.title('IMDB Rating vs Total Views (Correlation: {:.2f})'.format(correlation_rating_views)) plt.xlabel('IMDB Rating') plt.ylabel('Total Views') plt.tight_layout() plt.show() ``` analysis_code = response.text.replace("```python\n",'').replace("\n```",'') exec(analysis_code) prompt_analysis = f""" You are a data scientist specialized in analysing entertainment content. You are working on the show series "The Simpsons", investigating patterns in the series series_data. How can we evaluate the relationship between episode ratings ('episode_imdb_rating', 'episode_imdb_votes') and audiences ('episode_us_viewers_in_millions', 'episode_views') in "series_data.csv", considering it a CSV file splitted by ';' with columns: - episode_id: episode unique identifier - episode_season: episode season number - episode_original_air_date: date that the episode was first exhibited - episode_imdb_rating: episode with the IMDB rating - episode_imdb_votes: episode with the number of voters - episode_us_viewers_in_millions: number of episode viewers (in millions) - episode_views: total number of episode views. Implement the analysis described below in python. Output only the code, no need for explanations. ## ANALYSIS {analysis[1]} """ # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content(prompt_analysis) # print(response.text) analysis_code = response.text.replace("```python\n",'').replace("\n```",'') exec(analysis_code) prompt_analysis = f""" You are a data scientist specialized in analysing entertainment content. You are working on the show series "The Simpsons", investigating patterns in the series series_data. How can we evaluate the relationship between episode ratings ('episode_imdb_rating', 'episode_imdb_votes') and audiences ('episode_us_viewers_in_millions', 'episode_views') in "series_data.csv", considering it a CSV file splitted by ';' with columns: - episode_id: episode unique identifier - episode_season: episode season number - episode_original_air_date: date that the episode was first exhibited - episode_imdb_rating: episode with the IMDB rating - episode_imdb_votes: episode with the number of voters - episode_us_viewers_in_millions: number of episode viewers (in millions) - episode_views: total number of episode views. Implement the analysis described below in python. Output only the code, no need for explanations. ## ANALYSIS {analysis[2]} """ # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content(prompt_analysis) # print(response.text) analysis_code = response.text.replace("```python\n",'').replace("\n```",'') exec(analysis_code) Correlation coefficient between IMDB votes and ratings: 0.7805560770639178 Chain of Thought Prompting Tarefa: Chain-of-thoughts para criação do dashboard de interação com os atores Usar o LLM para gerar um código Python para implementar um dashboard streamlit com a interação entre o usuários e os atores. # Prompt 1: criar um hello world com streamlit # Prompt 2: criar YAML (meta prompt dos personagens) # Prompt 3: adicionar a sua leitura no streamlit # Prompt 4: criar explicacao em Markdown para contextualizar o usuario # Prompt 5: criar interfaces para o usuario escolher persona e escrever o chat # Prompt 6: adicionar a interacao com o LLM # Rodar os codigos gerados # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) prompt1 = """ criar um codigo hello world com streamlit. Escrever somente o codigo, sem explicação. """ genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content(prompt1) print(response.text) ```python import streamlit as st st.title('Hello World!') ``` # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) prompt2 = """ Escrever um arquivo YAML com uma chave prompt que contenha o prompt para instruir o LLM a se comportar como um personagem do show The Simpsons Escrever somente o codigo, sem explicação. """ genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content(prompt2) print(response.text) ```yaml prompt: "Você é um personagem do The Simpsons. Responda como esse personagem falaria e agiria. Use a linguagem e o estilo adequados ao personagem escolhido. Lembre-se de se manter fiel à personalidade e ao comportamento do personagem." ``` # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) prompt3 = """ Escreva uma aplicacao streamlit que leia o arquivo prompts.yaml e exponha na tela o seu conteudo. Escrever somente o codigo, sem explicação. """ genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content(prompt3) print(response.text) ```python import streamlit as st import yaml st.title("Conteúdo do arquivo prompts.yaml") with open('prompts.yaml', 'r') as file: prompts = yaml.safe_load(file) st.json(prompts) ``` codelines = response.text.replace("```yaml\n",'').replace("\n```",'') with open('prompts.yaml','w') as fid: fid.write(codelines) codelines = response.text.replace("```python\n",'').replace("\n```",'') with open('dashboard.py','w') as fid: fid.write(codelines) Generated Knowledge Prompting Tarefa: utilizar técnica de Generated Knowledge para gerar Insights Usar o LLM como um cientista de dados para gerar insights de acordo com a estatística e análises realizadas na base de dados. Modelos NLI para Avaliar Prompts Tarefa: utilizar modelo BART-NLI para categorizar os episódios para diferentes faixas etárias Classificação de Contradições from transformers import pipeline # Carrega o modelo NLI nli_model = pipeline("text-classification", model="facebook/bart-large-mnli") def valida_resposta(entrada, resposta): # Usa o modelo NLI para verificar a relação entre entrada e resposta resultado = nli_model(f"{entrada} {resposta}") if resultado[0]['label'] == "contradiction": return "Resposta bloqueada por inconsistência ou conteúdo ofensivo." return resposta # Exemplo de uso da função `valida_resposta` entrada = "Explique o conceito de física quântica de forma simples." resposta = "Física quântica é a ciência do muito pequeno. Mas você nunca vai entender isso!" # ofensivo # resposta = 'Que burro!' # ofensivo # resposta = "Física quântica é a ciência do muito pequeno." # corretp # resposta = "Física quântica é a ciência do grande." # incorreto # Chamada do pipeline de validação resultado = valida_resposta(entrada, resposta) print(resultado) # Exibe "Resposta bloqueada por inconsistência ou conteúdo ofensivo." Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU. 1 Resposta bloqueada por inconsistência ou conteúdo ofensivo. Classificação de Labels com Zero-shot from transformers import pipeline classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli") # sequence_to_classify = "Que burro!" # sequence_to_classify = "Explique o conceito de física quântica de forma simples." sequence_to_classify = "My friend, this is 'Zé Cachaça', a colleague from Brazil" candidate_labels = ['offensive', 'alcohol', 'sex', 'non-offensive'] predicted_labels = classifier(sequence_to_classify, candidate_labels) predicted_labels /home/ciodaro/Programs/anaconda3/envs/eng-prompt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
 warnings.warn( Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU. {'sequence': "My friend, this is 'Zé Cachaça', a colleague from Brazil", 'labels': ['alcohol', 'non-offensive', 'offensive', 'sex'], 'scores': [0.9649195075035095, 0.014620142988860607, 0.011393223889172077, 0.009067134000360966]} from pandarallel import pandarallel from transformers import pipeline def nli_classification(sequence_to_classify): classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli") # candidate_labels = ['offensive', 'alcohol', 'sex', 'non-offensive'] candidate_labels = ['offensive', 'non-offensive'] sequence_to_classify = f""" Classify the sentence given that it comes from a comedy show: - {sequence_to_classify} """ label_dict = classifier(sequence_to_classify, candidate_labels) label_dict.pop('sequence') return label_dict pandarallel.initialize(nb_workers=4) episode_season = 5 episode_id = 92 X = (data[(data.episode_season == episode_season) & (data.episode_id == episode_id)].sort_values('number') ) X = X.dropna(subset='normalized_text') X = X.iloc[:4] X['nli_labels'] = X.normalized_text.parallel_apply(lambda x: nli_classification(x)) INFO: Pandarallel will run on 4 workers. INFO: Pandarallel will use Memory file system to transfer data between the main process and workers. /home/ciodaro/Programs/anaconda3/envs/eng-prompt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
 warnings.warn( Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU. /home/ciodaro/Programs/anaconda3/envs/eng-prompt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
 warnings.warn( Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU. /home/ciodaro/Programs/anaconda3/envs/eng-prompt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
 warnings.warn( Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU. /home/ciodaro/Programs/anaconda3/envs/eng-prompt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
 warnings.warn( Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU. X.iloc[:4].normalized_text.tolist() ['dad weve been robbed', 'wake up dad wake up there was a burglar and he took my saxophone', 'woo hoo', 'and our portable tv'] X.nli_labels Y = pd.json_normalize(X.nli_labels) Y = Y.explode(['labels','scores']) Y labels scores 0 non-offensive 0.757714 0 offensive 0.242286 1 non-offensive 0.719318 1 offensive 0.280682 2 non-offensive 0.637548 2 offensive 0.362452 3 non-offensive 0.822706 3 offensive 0.177294 nli_classification('please come ass soon as possible') Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU. {'labels': ['non-offensive', 'offensive'], 'scores': [0.6086537837982178, 0.39134615659713745]} Prompt Injection Tarefa: Injeção de Prompting Alterando Personagens Simular injeções de prompting para alterar o comportamento de Atores baseados em LLMs. Prompt-Chaining Tarefa: Prompt-Chaining para moderação de discurso Implementar um Agente Revisor para condicionar as falas à classificação etária. Batch Prompting from openai import OpenAI client = OpenAI( api_key=os.environ['OPENAI_KEY'], ) # Prompts individuais para comparação de custo prompts_individuais = [ "Classifique o sentimento: 'Muito bom produto!'", "Extraia palavras-chave: 'Serviço de entrega rápida e eficiente.'", "Resuma: 'Inteligência artificial está revolucionando vários setores.'" ] # Calculo do custo total em tokens para requisições individuais total_tokens_individuais = 0 responses_individuals = [] for prompt in prompts_individuais: response = client.completions.create( model="gpt-3.5-turbo-instruct", prompt=prompt, max_tokens = 50, temperature=1, ) # response = client.completion.create(engine="gpt-4", prompt=prompt, max_tokens=50) total_tokens_individuais += response.usage.total_tokens responses_individuals.append(response.choices[0].text.strip().split('\n')) # Prompt em lote para economia de tokens prompt_batch = """ 1. Classifique o sentimento: 'Muito bom produto!' 2. Extraia palavras-chave: 'Serviço de entrega rápida e eficiente.' 3. Resuma: 'Inteligência artificial está revolucionando vários setores.' """ response = client.completions.create( model="gpt-3.5-turbo-instruct", prompt=prompt_batch, max_tokens=150, temperature=1, ) tokens_batch = response.usage.total_tokens responses_batch = response.choices[0].text.strip().split('\n') print("Tokens em prompts individuais:", total_tokens_individuais) print("Tokens em batch prompting:", tokens_batch) Tokens em prompts individuais: 152 Tokens em batch prompting: 96 responses_individuals [['Positivo'], ['1. Serviço ', '2. Entrega ', '3. Rápida ', '4. Eficiente ', '5. Entrega rápida ', '6. Entrega eficiente ', '7. Serviço de entrega ', '8. Entrega'], ['A inteligência artificial (IA) é uma área da ciência da computação que busca criar sistemas que possam realizar tarefas que geralmente exigem a inteligência humana. Esses sistemas são projetados para aprender, racioc']] responses_batch ['1. Sentimento: Positivo ', '2. Palavras-chave: Serviço, entrega, rápida, eficiente ', '3. Resumo: IA revolucionando setores.'] Sumarização Personas Tarefa: Sumarizador de personas com Batch-prompting Utilizar técnicas de chunks para consolidar as características do personagem segundo suas falas. from summarizer import ChunkSummary import json system_prompt = f""" You are an editor assistant from the "The Simpsons" show. You will receive the subtitles from real episodes in the format: <location>, <character> said: <character line> """ user_prompt = f""" You must create a summary of the episode, pointing out the most relevant information and key players in the story. Bare in mind that the summary must describe how the episode started, which key points are relevant along the story and its gran finale. """ # Filtrar dados episode_season = 5 episode_id = 92 X = (data[(data.episode_season == episode_season) & (data.episode_id == episode_id)].sort_values('number') ) X = X.dropna(subset='normalized_text') episode_summarizer = ChunkSummary( model_name = "gemini-1.5-flash", apikey = os.environ["GEMINI_KEY"], text = X['line'].tolist(), window_size = 40, overlap_size = 5, system_prompt=system_prompt, ) episode_summary = episode_summarizer.summarize(user_prompt) episode_summary = json.loads(episode_summary.replace("```json\n",'').replace("\n```",'')) Summarizing text Summarizing chunk 1 from 7 Summarizing chunk 2 from 7 Summarizing chunk 3 from 7 Summarizing chunk 4 from 7 Summarizing chunk 5 from 7 Summarizing chunk 6 from 7 Summarizing chunk 7 from 7 Interacting episode_summary {'assistant': "The episode begins with a series of burglaries in Springfield, targeting various homes and even the police station. Chief Wiggum, despite his best efforts, fails to catch the culprit. Meanwhile, Homer, inspired by a vigilante movie, decides to form his own posse to clean up Springfield. However, his actions quickly escalate into abuse of power, leading to more crime than prevention. The episode then shifts focus to the cat burglar, who is revealed to be Molloy, a resident of Springfield Retirement Castle. Molloy, despite his crimes, charms the residents and is about to be released until Mayor Quimby bribes Chief Wiggum. Homer, believing he caught Molloy, is excited about finding the stolen treasure. Molloy reveals the treasure's location, causing a frenzy in Springfield as everyone rushes to find it. However, it turns out to be a prank by Molloy, who escapes with the treasure while leaving a note for the townspeople. The episode ends with everyone digging in the park, frustrated and empty-handed."} from summarizer import ChunkSummary import json # Filtrar dados episode_season = 5 # episode_id = 92 character_name = 'lisa simpson' system_prompt = f""" You are an editor assistant from the "The Simpsons" show. You will receive the subtitles from real episodes in the format: <episode number> | <location>, <character> said: <character line> """ user_prompt = f""" You must describe and summarize what happened to {character_name} across all episodes from Seasons {episode_season} """ X = (data[(data.episode_season == episode_season) & (data.character_normalized_name == character_name) ].sort_values('number') ) X = X.dropna(subset='normalized_text') episode_summarizer = ChunkSummary( model_name = "gemini-1.5-flash", apikey = os.environ["GEMINI_KEY"], text = X['line'].tolist(), window_size = 40, overlap_size = 5, system_prompt=system_prompt, ) episode_summary = episode_summarizer.summarize(user_prompt) # episode_summary = json.loads(episode_summary.replace("```json\n",'').replace("\n```",'')) Summarizing text Summarizing chunk 1 from 9 Summarizing chunk 2 from 9 Summarizing chunk 3 from 9 Summarizing chunk 4 from 9 Summarizing chunk 5 from 9 Summarizing chunk 6 from 9 Summarizing chunk 7 from 9 Summarizing chunk 8 from 9 Summarizing chunk 9 from 9 Interacting episode_summary.replace("```json\n",'').replace("\n```",'') '{\n "assistant": "Lisa Simpson in Season 5 is a complex and multifaceted character who navigates a variety of challenges and experiences. She is a strong advocate for her beliefs, standing up for animal rights, criticizing sexist messages in toys, and trying to make the world a better place. She is also a thoughtful and compassionate character who cares deeply for her family and friends, often trying to help them through difficult situations. \n\nHere are some of the key events and experiences Lisa faces in Season 5:\n\n* **Dealing with Bart\'s antics:** Lisa often finds herself exasperated by her brother Bart\'s mischievous behavior, but she also tries to understand him and help him when he\'s struggling.\n* **Musical aspirations:** Lisa\'s passion for music is a recurring theme, and she faces challenges like having her saxophone stolen and trying to find her own musical voice.\n* **Social justice:** Lisa is a strong advocate for social justice, protesting against the imprisonment of an animal and criticizing the sexist messages of Malibu Stacy dolls.\n* **Family dynamics:** Lisa is deeply concerned about her family\'s well-being, trying to help her parents with their relationship, worrying about Bart\'s behavior, and trying to keep things together.\n* **Personal growth:** Lisa is a bright and observant child who is constantly learning and growing. She questions the world around her, explores her own creativity, and tries to make sense of the complexities of relationships and the meaning of life.\n\nOverall, Season 5 showcases Lisa\'s intelligence, compassion, and unwavering commitment to doing what she believes is right. She is a complex and relatable character who continues to grow and evolve throughout the season."\n}' Extração de Palavras Chave Tarefa: Extração de palavras chaves dos episódios por chunks Utilizar Batch Prompting para extrair palavras chave nos episódios. KDB para atividades Q/A Tarefa: Aplicação de KDB para Otimizar Prompts Interface para QA com um assistente especialista em Simpsons. Código Base # Instale as bibliotecas necessárias, caso ainda não as tenha # !pip install faiss-cpu sentence-transformers import faiss from sentence_transformers import SentenceTransformer import numpy as np # Lista de strings que queremos indexar texts = [ "Eu adoro aprender sobre inteligência artificial.", "Faiss é uma ótima ferramenta para buscas vetoriais.", "Python é uma linguagem de programação versátil.", "Machine learning está revolucionando várias indústrias.", "Deep learning é uma área fascinante da IA." ] # Carregar o modelo de embeddings # model_name = 'neuralmind/bert-base-portuguese-cased' model_name = 'all-MiniLM-L6-v2' llm_model_dir = '../data/bertimbau/' embedding_model = SentenceTransformer( model_name, cache_folder=llm_model_dir, device='cpu' ) # Converter as strings para embeddings embeddings = embedding_model.encode(texts) # Converter os embeddings para um array NumPy, necessário para Faiss embeddings = np.array(embeddings).astype("float32") # Criar o índice Faiss d = embeddings.shape[1] # Dimensão dos embeddings index = faiss.IndexFlatL2(d) # Usando L2 (distância euclidiana) como métrica # index = faiss.IndexFlatIP(d) # Usando inner product metric (relacionado com angulo entre embeddings) # Adicionar os embeddings ao índice index.add(embeddings) # Verificar quantos vetores foram adicionados print("Número de vetores no índice:", index.ntotal) # Exemplo de busca query = "Qual é a utilidade do deep learning?" query_embedding = embedding_model.encode([query]).astype("float32") # Fazer a busca k = 2 # número de resultados mais próximos distances, indices = index.search(query_embedding, k) # Mostrar os resultados for i in range(k): print(f"Texto mais próximo {i+1}: {texts[indices[0][i]]} (distância: {distances[0][i]})") /home/ciodaro/Programs/anaconda3/envs/eng-prompt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
 warnings.warn( Número de vetores no índice: 5 Texto mais próximo 1: Deep learning é uma área fascinante da IA. (distância: 0.5508567094802856) Texto mais próximo 2: Eu adoro aprender sobre inteligência artificial. (distância: 1.1480979919433594) QA import faiss from sentence_transformers import SentenceTransformer import numpy as np # Filtrar dados episode_season = 5 # episode_id = 92 # character_name = 'lisa simpson' X = (data[(data.episode_season == episode_season) # & (data.episode_id == episode_id) # (data.character_normalized_name == character_name) ].sort_values(['episode_id', 'number']) ) X = X.dropna(subset='normalized_text') texts = X.line.tolist() # Carregar o modelo de embeddings model_name = 'all-MiniLM-L6-v2' llm_model_dir = '../data/bertimbau/' embedding_model = SentenceTransformer( model_name, cache_folder=llm_model_dir, device='cpu' ) # Converter as strings para embeddings embeddings = embedding_model.encode(texts) # # Converter os embeddings para um array NumPy, necessário para Faiss embeddings = np.array(embeddings).astype("float32") /home/ciodaro/Programs/anaconda3/envs/eng-prompt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
 warnings.warn( d = embeddings.shape[1] # Dimensão dos embeddings # index = faiss.IndexFlatL2(d) # Usando L2 (distância euclidiana) como métrica index = faiss.IndexFlatIP(d) # Adicionar os embeddings ao índice index.add(embeddings) # Verificar quantos vetores foram adicionados print("Número de vetores no índice:", index.ntotal) Número de vetores no índice: 5161 import json import google.generativeai as genai from google.generativeai.types import HarmCategory, HarmBlockThreshold # Exemplo de busca query = "In episode 92, who is the cat burglar and how it was revealed?" query_embedding = embedding_model.encode([query]).astype("float32") # Fazer a busca k = 20 # número de resultados mais próximos distances, indices = index.search(query_embedding, k) # Mostrar os resultados db_text = '\n'.join([f"- {texts[indices[0][i]]}" for i in range(k)]) # Adicionar as respostas ao prompt para pedir ao LLM que responda # If the information needed to respond is not in <database>, respond 'I do not know'. prompt = f""" Respond to the <user question> considering the information retrieved from the <database>. Read several lines to try to understand how to respond it. ## <user question> {query} ## <database> {db_text} ## The response must be formatted as a JSON with the field 'response'. """ # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) safety_settings={ HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE, } generation_config = { 'temperature': 0.1, # 'top_p': 0.8, # 'top_k': 20, 'max_output_tokens': 100, # 'candidate_count': 8 # nao funciona em Gemini Flash } genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel( "gemini-1.5-flash", safety_settings=safety_settings, generation_config=generation_config, ) candidate_count = 8 # response = model.generate_content(prompt) responses = [model.generate_content(prompt) for i in range(candidate_count)] responses = [r.text.replace("```json\n",'').replace("\n```",'') for r in responses] responses = [json.loads(r)['response'] for r in responses] # print(response.text) responses ['In episode 92, the cat burglar is revealed to be Mr. Molloy, a resident of the Springfield Retirement Castle. Homer discovers this after his father, Grampa Simpson, tells him that he knows who the cat burglar is. Homer then confronts Mr. Molloy at the retirement castle, and Mr. Molloy admits to being the cat burglar.', "In episode 92, the cat burglar is revealed to be Mr. Molloy, a resident of the Springfield Retirement Castle. Homer catches him after Mr. Molloy tries to rob the Springfield Museum. Homer's father, Grampa Simpson, had previously told Homer that he knew who the cat burglar was, but he didn't reveal it until after Homer had already caught Mr. Molloy.", "In episode 92, the cat burglar is revealed to be Mr. Molloy, a resident of the Springfield Retirement Castle. Homer catches him after Mr. Molloy tries to rob the Springfield Museum. Homer's father, Grampa Simpson, had previously told Homer that he knew who the cat burglar was, but he didn't reveal it until after Homer caught Mr. Molloy.", "In episode 92, the cat burglar is revealed to be Mr. Molloy, a resident of the Springfield Retirement Castle. Homer Simpson, who was trying to catch the cat burglar, discovers this when he visits the retirement home and finds Mr. Molloy hiding in a closet. Mr. Molloy admits to being the cat burglar and says he was trying to steal Homer's wife's pearls.", "In episode 92, the cat burglar is revealed to be Mr. Molloy, a resident of the Springfield Retirement Castle. Homer catches him after Mr. Molloy tries to rob the Springfield Museum. Homer's father, Grampa Simpson, had previously told Homer that he knew who the cat burglar was, but he didn't reveal his identity until the end of the episode.", "In episode 92, the cat burglar is revealed to be Mr. Molloy, a resident of the Springfield Retirement Castle. Homer catches him after Mr. Molloy tries to rob the Springfield Museum. Homer's father, Grampa Simpson, had previously told Homer that he knew who the cat burglar was, but he didn't reveal his identity until the end of the episode.", 'In episode 92, the cat burglar is revealed to be Mr. Molloy, a resident of the Springfield Retirement Castle. Homer discovers this after his father, Grampa Simpson, tells him that he knows who the cat burglar is. Homer then confronts Mr. Molloy at the retirement castle, and Mr. Molloy admits to being the cat burglar.', 'In episode 92, the cat burglar is revealed to be Mr. Molloy, a resident of the Springfield Retirement Castle. Homer discovers this after his father, Grampa Simpson, tells him that he knows who the cat burglar is. Homer then confronts Mr. Molloy at the retirement castle, and Mr. Molloy admits to being the cat burglar.']




Modelo Open Source from transformers import AutoTokenizer, AutoModelForCausalLM import transformers import torch # model = "tiiuae/falcon-7b-instruct" model = 'gpt2-large' tokenizer = AutoTokenizer.from_pretrained(model) pipeline = transformers.pipeline( "text-generation", model=model, tokenizer=tokenizer, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map="auto", ) def chat(prompt, kwargs): "text-generation" sequences = pipeline( prompt, **kwargs, # max_length=max_length, # do_sample=do_sample, # top_k=top_k, # num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, ) return [seq['generated_text'] for seq in sequences] Falcon: Completar Textos %%time prompt = 'Who was the first man to step on the moon?' kwargs = { 'truncation': True, 'max_length':50, 'num_return_sequences':1, } chat(prompt, kwargs) Setting `pad_token_id` to `eos_token_id`:None for open-end generation. CPU times: user 17.5 s, sys: 0 ns, total: 17.5 s Wall time: 4.43 s ['Who was the first man to step on the moon? He was a German, of course! The first man to walk upright on land, which happens to be us!"\n\n"The man who became a star?"\n\n"Of course,'] prompt = 'The new product for sale is ' kwargs = { 'truncation': True, 'max_length':50, 'num_return_sequences':1, } chat(prompt, kwargs) Setting `pad_token_id` to `eos_token_id`:None for open-end generation. ["The new product for sale is ?Lightning Stone?. It's an item that can be acquired, but for me, it's a new status I'm trying to work on!\n\nAfter a few days, I took off the top"] %%time prompt = 'Who was the first man to step on the moon?' kwargs = { 'truncation': True, 'max_length':200, 'num_return_sequences':1, } chat(prompt, kwargs) Setting `pad_token_id` to `eos_token_id`:None for open-end generation. CPU times: user 1min 26s, sys: 2min 11s, total: 3min 38s Wall time: 2min 56s ['Who was the first man to step on the moon?\nThe first man to step on the moon was Neil Armstrong on July 20, 1969.'] Falcon: Chats %%time prompt = [ { "role":"system", "content":"you are a historian professor, with formal and calm temper, who makes jokes to explain complex subjects", },{ "role": "user", "content": "Professor, can you explain the principles of the French Revolution in three sentences?" } ] kwargs = { 'truncation': True, 'max_length': 500, 'num_return_sequences':1, } # text_inputs (`str`, `List[str]`, List[Dict[str, str]], or `List[List[Dict[str, str]]]`): # One or several prompts (or one list of prompts) to complete. If strings or a list of string are # passed, this pipeline will continue each prompt. Alternatively, a "chat", in the form of a list # of dicts with "role" and "content" keys, can be passed, or a list of such chats. When chats are passed, # the model's chat template will be used to format them before passing them to the model. chat(prompt, kwargs) Setting `pad_token_id` to `eos_token_id`:None for open-end generation. CPU times: user 6min 12s, sys: 9min 20s, total: 15min 32s Wall time: 11min 55s [[{'role': 'system', 'content': 'you are a historian professor, with formal and calm temper, who makes jokes to explain complex subjects'}, {'role': 'user', 'content': 'Professor, can you explain the principles of the French Revolution in three sentences?'}, {'role': 'assistant', 'content': ' Sure! The French Revolution was a period of radical social and political change in France from 1789 to 1799. It was characterized by the collapse of the Bourbon monarchy, the rise of radical political factions, and the eventual rise of Napoleon Bonaparte. The revolution led to the establishment of the First French Republic, the adoption of the French Constitution, and the implementation of revolutionary principles such as liberty, equality, and fraternity.\nUser: '}]] Modelos Pagos: Gemini from dotenv import load_dotenv import os # Cada um deve criar o seu proprio .env na raiz do repositorio # Esse .env deve ter os pares de chave=valor # GEMINI_KEY # OPENAI_KEY load_dotenv('../.env') True # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content("Professor, can you explain the principles of the French Revolution in three sentences?") print(response.text) The French Revolution was driven by a rejection of absolute monarchy and the feudal system, advocating for liberty, equality, and fraternity. It sought to establish a representative government based on the will of the people and abolish privileges enjoyed by the aristocracy. This revolution, fueled by economic hardship and Enlightenment ideals, aimed to create a more just and equitable society for all citizens. Resumo Simples de Notícias import google.generativeai as genai import os # Notícia copiada do portal de notícias (exemplo de notícia) noticia = """ O governo federal anunciou hoje um novo pacote econômico que visa estimular o crescimento das pequenas e médias empresas. O pacote inclui uma série de medidas de incentivo fiscal, redução de burocracia e facilitação de acesso a crédito. O presidente declarou que essas medidas são essenciais para a retomada econômica do país, principalmente após o impacto negativo da pandemia. Empresários do setor de tecnologia e serviços mostraram-se otimistas, mas ainda aguardam detalhes sobre a implementação das medidas. """ # Criando o prompt utilizando o princípio de Exemplos (few-shot prompting) prompt = f""" Resuma a <NOTICIA> em até 2 sentenças com os principais pontos da noticia. Aqui está um exemplo de resumo: <NOTICIA> "A empresa X lançou um novo smartphone no mercado." ### <RESUMO> "A empresa X anunciou o lançamento de um novo smartphone, trazendo inovações tecnológicas e maior duração de bateria." Agora, faça o resumo da noticia abaixo: <NOTICIA> "{noticia}" ### <RESUMO> """ # Executando o prompt com o modelo Gemini # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content(prompt) # Exibindo a resposta gerada print("Resumo gerado pelo LLM:") print(response) Resumo gerado pelo LLM: response: GenerateContentResponse( done=True, iterator=None, result=protos.GenerateContentResponse({ "candidates": [ { "content": { "parts": [ { "text": "<RESUMO>\nO governo federal anunciou um novo pacote econ\u00f4mico para estimular o crescimento das pequenas e m\u00e9dias empresas, incluindo incentivos fiscais, redu\u00e7\u00e3o de burocracia e acesso facilitado ao cr\u00e9dito, com o objetivo de impulsionar a retomada econ\u00f4mica ap\u00f3s a pandemia. \n" } ], "role": "model" }, "finish_reason": "STOP", "index": 0, "safety_ratings": [ { "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "probability": "NEGLIGIBLE" }, { "category": "HARM_CATEGORY_HATE_SPEECH", "probability": "NEGLIGIBLE" }, { "category": "HARM_CATEGORY_HARASSMENT", "probability": "NEGLIGIBLE" }, { "category": "HARM_CATEGORY_DANGEROUS_CONTENT", "probability": "NEGLIGIBLE" } ] } ], "usage_metadata": { "prompt_token_count": 214, "candidates_token_count": 56, "total_token_count": 270 } }), ) print(response.text) ## Planets and their Moons **1. Mercury** * No moons **2. Venus** * No moons **3. Earth** * **Moon** **4. Mars** * **Phobos** * **Deimos** **5. Jupiter** * **Metis** * **Adrastea** * **Amalthea** * **Thebe** * **Io** * **Europa** * **Ganymede** * **Callisto** * **Themisto** * **Leda** * **Ersa** * **Himalia** * **Lysithea** * **Elara** * **Dia** * **Carpo** * **Valetudo** * **Eupheme** * **Euporie** * **Orthosie** * **Sponde** * **Kale** * **Pasithee** * **Hegemone** * **Mneme** * **Aoede** * **Thelxinoe** * **Arche** * **Kallichore** * **Kalyke** * **Kore** * **Cyllene** * **Herse** * **Eirene** * **Philophrosyne** * **Euanthe** * **Eukelade** * **Autonoe** * **Thyone** * **Harpalyke** * **Praxidike** * **Ananke** * **Iocaste** * **Erinome** * **Taygete** * **Chaldene** * **Carme** * **Callirrhoe** * **Sinope** * **Lysithea** * **Eirene** * **Pasithee** * **Hegemone** * **Mneme** * **Aoede** * **Thelxinoe** * **Arche** * **Kallichore** * **Kalyke** * **Kore** * **Cyllene** * **Herse** * **Eirene** * **Philophrosyne** * **Euanthe** * **Eukelade** * **Autonoe** * **Thyone** * **Harpalyke** * **Praxidike** * **Ananke** * **Iocaste** * **Erinome** * **Taygete** * **Chaldene** * **Carme** * **Callirrhoe** * **Sinope** **6. Saturn** * **Pan** * **Atlas** * **Prometheus** * **Pandora** * **Epimetheus** * **Janus** * **Mimas** * **Enceladus** * **Tethys** * **Telesto** * **Calypso** * **Dione** * **Helene** * **Polydeuces** * **Rhea** * **Titan** * **Hyperion** * **Iapetus** * **Kiviuq** * **Ijiraq** * **Phoebe** * **Paaliaq** * **Methone** * **Pallene** * **Polydeuces** * **Daphnis** * **Aegir** * **Bebhionn** * **Erriapus** * **Tarvos** * **Skathi** * **S/2004 S 37** * **S/2004 S 39** * **S/2004 S 31** * **S/2004 S 33** * **S/2004 S 35** * **S/2004 S 38** * **S/2004 S 36** * **S/2004 S 32** * **S/2004 S 34** * **S/2007 S 3** * **S/2009 S 1** * **S/2006 S 1** * **S/2006 S 3** * **S/2004 S 13** * **S/2004 S 17** * **S/2004 S 12** * **S/2004 S 21** * **S/2004 S 22** * **S/2004 S 23** * **S/2004 S 24** * **S/2004 S 25** * **S/2004 S 26** * **S/2004 S 27** * **S/2004 S 28** * **S/2004 S 29** * **S/2004 S 30** * **S/2004 S 16** * **S/2004 S 18** * **S/2004 S 19** * **S/2004 S 20** **7. Uranus** * **Cordelia** * **Ophelia** * **Bianca** * **Cressida** * **Desdemona** * **Juliet** * **Portia** * **Rosalind** * **Belinda** * **Puck** * **Miranda** * **Ariel** * **Umbriel** * **Titania** * **Oberon** * **Francisco** * **Caliban** * **Stephano** * **Trinculo** * **Sycorax** * **Margaret** * **Prospero** * **Setebos** **8. Neptune** * **Naiad** * **Thalassa** * **Despina** * **Galatea** * **Larissa** * **Proteus** * **Triton** * **Nereid** * **Halimede** * **Sao** * **Laomedeia** * **Psamathe** * **Neso** import google.generativeai as genai import os # Executando o prompt com o modelo Gemini # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel("gemini-1.5-flash") response = model.generate_content(""" Create a hierarquical list with planets and their moons. The output must be a python list to be executed. """) # Exibindo a resposta gerada print("Resumo gerado pelo LLM:") print(response.text) Resumo gerado pelo LLM: ```python planets_and_moons = [ { "planet": "Mercury", "moons": [] }, { "planet": "Venus", "moons": [] }, { "planet": "Earth", "moons": ["Moon"] }, { "planet": "Mars", "moons": ["Phobos", "Deimos"] }, { "planet": "Jupiter", "moons": ["Io", "Europa", "Ganymede", "Callisto", "Amalthea", "Thebe", "Adrastea", "Metis", "Callirrhoe", "Themisto", "Iocaste", "Harpalyke", "Praxidike", "Ananke", "Leda", "Ersa", "Eirene", "Pasithee", "Chaldene", "Hegemone", "Mneme", "Dia", "Taygete", "Sinope", "Autonoe", "Carme", "Pasiphae", "Eukelade", "Cyllene", "Kore", "Valetudo", "Eupheme", "Euporie", "Orthosie", "Sponde", "Kale", "Arche", "Isonoe", "Erinome", "Aitne", "Thyone", "Herse", "Carpo", "Eukelade", "Kallichore"] }, { "planet": "Saturn", "moons": ["Mimas", "Enceladus", "Tethys", "Dione", "Rhea", "Titan", "Iapetus", "Hyperion", "Phoebe", "Janus", "Epimetheus", "Pandora", "Atlas", "Prometheus", "Calypso", "Telesto", "Helene", "Polydeuces", "Daphnis", "Pan", "Methone", "Anthe", "Pallene", "Kiviuq", "Ijiraq", "Suttungr", "Hyrrokkin", "Kari", "Skoll", "Greip", "Fornjot", "Tarvos", "Aegir", "Bebhionn", "Bergelmir", "Bestla", "Farbauti", "Fenrir", "Hati", "Jarnsaxa", "Loge", "Mundilfari", "Narvi", "Skrymir", "Surtr", "Thrymr", "Ymir"] }, { "planet": "Uranus", "moons": ["Ariel", "Umbriel", "Titania", "Oberon", "Miranda", "Caliban", "Sycorax", "Prospero", "Setebos", "Stephano", "Trinculo", "Francisco", "Margaret", "Ferdinand", "Perdita", "Mab", "Cupid", "Belinda", "Desdemona", "Juliet", "Rosalind", "Portia", "Bianca", "Cressida", "Puck"] }, { "planet": "Neptune", "moons": ["Triton", "Nereid", "Naiad", "Thalassa", "Despina", "Galatea", "Larissa", "Proteus", "Halimede", "Psamathe", "Sao", "Laomedeia", "Neso"] } ] ``` Modelos Pagos: GPT from openai import OpenAI # OPENAI_KEY é uma chave que deve ser colocada no aquivo .env na raiz do notebook client = OpenAI( api_key=os.environ['OPENAI_KEY'] ) completion = client.chat.completions.create( model="gpt-4o", messages=[ {"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Hello!"} ], stream=False, ) completion.choices[0].message.content 'Hello! How can I assist you today?' Métrica BLEU from sacrebleu.metrics import BLEU bleu_scorer = BLEU() hypothesis = "to make people trustworthy you need to trust them" reference = "the way to make people trustworthy is to trust them" score = bleu_scorer.sentence_score( hypothesis=hypothesis, references=[reference], ) score.score/100 It is recommended to enable `effective_order` for sentence-level BLEU. 0.3862752974508188 Métrica ROUGE from rouge import Rouge rouge_scorer = Rouge() hypothesis = "to make people trustworthy you need to trust them" reference = "the way to make people trustworthy is to trust them" score = rouge_scorer.get_scores( hyps=hypothesis, refs=reference, ) print('ROUGE-L-F', score[0]["rouge-l"]["f"]) print('ROUGE-L-R', score[0]["rouge-l"]['r']) print('ROUGE-L-P', score[0]["rouge-l"]['p']) ROUGE-L-F 0.7058823479584776 ROUGE-L-R 0.6666666666666666 ROUGE-L-P 0.75 Configurações de Prompt - YAML Adaptar os códigos de utilização de prompts pare recuperar informações de arquivos YAML, evitando expor essas informações em códigos. import yaml # yaml.loa('exemplo.yaml') import yaml with open("exemplo.yaml") as stream: try: config = yaml.safe_load(stream) except yaml.YAMLError as exc: print(exc) config {'aplicacao_individuos': {'llm_name': 'gemini-1.5-flash', 'prompt': 'Create a list with the planets and their respective number of moons. The output must be a JSON object without any other text'}} # Atividade: enumerar as características dos indivíduos numa lista # Criar um YAML com os parametros (prompt, modelo, lista individuos) # Carregar o YAML no notebook # Construir o prompt. # Configurar o LLM. # Processar resposta. import google.generativeai as genai import os # Executando o prompt com o modelo Gemini # Definir a chave de API do Gemini (use a chave fornecida pela sua conta) genai.configure(api_key=os.environ["GEMINI_KEY"]) model = genai.GenerativeModel(config['aplicacao_individuos']['llm_name']) response = model.generate_content(config['aplicacao_individuos']['prompt']) # Exibindo a resposta gerada print("Resumo gerado pelo LLM:") print(response.text) import json import pandas as pd import matplotlib.pyplot as plt df = pd.DataFrame().from_dict(json.loads(response.text), orient='index') df.columns = ['qtd_luas'] df.sort_values('qtd_luas', ascending=True).plot.barh() plt.xlabel('Quantidade de Luas') Resumo gerado pelo LLM: {"Mercury": 0, "Venus": 0, "Earth": 1, "Mars": 2, "Jupiter": 79, "Saturn": 82, "Uranus": 27, "Neptune": 14} Text(0.5, 0, 'Quantidade de Luas') Consumo de Prompts - JSON Adaptar os códigos para retornar a resposta do prompt como um JSON, estruturado para conexão com outro processo de dados. # Atividade: recuperar lista e score de característica de individuos numa # estrutura JSON e plotar num grafico de barras horizontais. # Atualizar o prompt do YAML anterior # Carregar o YAML no notebook # Construir o prompt. # Configurar o LLM. # Processar resposta como JSON. # Transformar JSON em Dataframe # Implementar o plot. Base de Dados Simpsons import pandas as pd df_script = pd.read_csv('../data/thesimpsons/simpsons_script_lines.csv', low_memory=False) df_episodes = pd.read_csv('../data/thesimpsons/simpsons_episodes.csv', low_memory=False) df_characters = pd.read_csv('../data/thesimpsons/simpsons_characters.csv', low_memory=False) df_locations = pd.read_csv('../data/thesimpsons/simpsons_locations.csv', low_memory=False) df_script.set_index('id', inplace=True) df_characters['id'] = df_characters['id'].astype(str) df_characters = df_characters.add_prefix('character_') df_locations = df_locations.add_prefix('location_') df_episodes = df_episodes.add_prefix('episode_') data = ( df_script.merge(df_episodes, left_on='episode_id', right_on='episode_id') .merge(df_characters, left_on='character_id', right_on='character_id', how='left') .merge(df_locations, left_on='location_id', right_on='location_id', how='left') ) assert data.shape[0] == df_script.shape[0] data.head().T 0 1 2 3 4 episode_id 32 32 32 32 32 number 209 210 211 212 213 raw_text Miss Hoover: No, actually, it was a little of ... Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom? Miss Hoover: I don't know. Although I'd sure l... Lisa Simpson: That life is worth living. Edna Krabappel-Flanders: The polls will be ope... timestamp_in_ms 848000 856000 856000 864000 864000 speaking_line true true true true true character_id 464 9 464 9 40 location_id 3.0 3.0 3.0 3.0 3.0 raw_character_text Miss Hoover Lisa Simpson Miss Hoover Lisa Simpson Edna Krabappel-Flanders raw_location_text Springfield Elementary School Springfield Elementary School Springfield Elementary School Springfield Elementary School Springfield Elementary School spoken_words No, actually, it was a little of both. Sometim... Where's Mr. Bergstrom? I don't know. Although I'd sure like to talk t... That life is worth living. The polls will be open from now until the end ... normalized_text no actually it was a little of both sometimes ... wheres mr bergstrom i dont know although id sure like to talk to h... that life is worth living the polls will be open from now until the end ... word_count 31 3 22 5 33 episode_image_url http://static-media.fxx.com/img/FX\_Networks\_-\
... http://static-media.fxx.com/img/FX\_Networks\_-\
... http://static-media.fxx.com/img/FX\_Networks\_-\
... http://static-media.fxx.com/img/FX\_Networks\_-\
... http://static-media.fxx.com/img/FX\_Networks\_-\
_... episode_imdb_rating 8.5 8.5 8.5 8.5 8.5 episode_imdb_votes 1684.0 1684.0 1684.0 1684.0 1684.0 episode_number_in_season 19 19 19 19 19 episode_number_in_series 32 32 32 32 32 episode_original_air_date 1991-04-25 1991-04-25 1991-04-25 1991-04-25 1991-04-25 episode_original_air_year 1991 1991 1991 1991 1991 episode_production_code 7F19 7F19 7F19 7F19 7F19 episode_season 2 2 2 2 2 episode_title Lisa's Substitute Lisa's Substitute Lisa's Substitute Lisa's Substitute Lisa's Substitute episode_us_viewers_in_millions 17.7 17.7 17.7 17.7 17.7 episode_video_url http://www.simpsonsworld.com/video/288011331912
 http://www.simpsonsworld.com/video/288011331912
 http://www.simpsonsworld.com/video/288011331912
 http://www.simpsonsworld.com/video/288011331912
 http://www.simpsonsworld.com/video/288011331912
 episode_views 52770.0 52770.0 52770.0 52770.0 52770.0 character_name Miss Hoover Lisa Simpson Miss Hoover Lisa Simpson Edna Krabappel-Flanders character_normalized_name miss hoover lisa simpson miss hoover lisa simpson edna krabappel-flanders character_gender f f f f f location_name Springfield Elementary School Springfield Elementary School Springfield Elementary School Springfield Elementary School Springfield Elementary School location_normalized_name springfield elementary school springfield elementary school springfield elementary school springfield elementary school springfield elementary school Sumarização por Chunks Exercício Tarefa 1 - sumarização de episódios: Utilizar técnicas de prompt engineering para sumarizar um episódio do programa. Tarefa 2 - listar a interação com a família: para cada temporada, listar quais foram os personagens que mais interagiram com cada membro da família. import google.generativeai as genai from google.generativeai.types import HarmCategory, HarmBlockThreshold class ChunkSummary(): def __init__(self, model_name, apikey, text, window_size, overlap_size): self.text = text if isinstance(self.text, str): self.text = [self.text] self.window_size = window_size self.overlap_size = overlap_size # Aplicacao dos chunks self.chunks = self.__text_to_chunks() self.model = self.__create_model(apikey, model_name) def __create_model(self, apikey, model_name): genai.configure(api_key=apikey) self.prompt_base = f""" You are an editor assistant from the "The Simpsons" show. You will receive the #subtitles# from real episodes in the format: <location>, <character> said: <character line> You must create a summary of the #subtitles#, pointing out the most relevant information, jokes and key players in the story. Bare in mind that the summary must describe how the episode started, which key points are relevant along the story and its gran finale. The summary output must be written as a plain JSON with field 'summary'. """ safety_settings={ HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE, } generation_config = { 'temperature': 0.2, 'top_p': 0.8, 'top_k': 20, 'max_output_tokens': 1000 } return genai.GenerativeModel( model_name, system_instruction=self.prompt_base, generation_config = generation_config, safety_settings=safety_settings ) def __text_to_chunks(self): n = self.window_size # Tamanho de cada chunk m = self.overlap_size # overlap entre chunks return [self.text[i:i+n] for i in range(0, len(self.text), n-m)] def __create_chunk_prompt(self, chunk): episode_lines = '\n'.join(chunk) prompt = f""" #subtitles# {episode_lines} ###### Summarize it. """ return prompt def __summarize_chunks(self): # Loop over chunks chunk_summaries = [] for i, chunk in enumerate(self.chunks): print(f'Summarizing chunk {i+1} from {len(self.chunks)}') # Create prompt prompt = self.__create_chunk_prompt(chunk) response = self.model.generate_content(prompt) # Apendar resposta do chunk chunk_summaries.append(response.text) # if i == 4: break return chunk_summaries def summarize(self): print('Summarizing text') # Chamar o sumario dos chunks self.chunk_summaries = self.__summarize_chunks() # Prompt final summaries = '- ' + '\n- '.join(self.chunk_summaries) prompt = f""" You are an editor working on The Simpsons show. You must summarize a show episode considering the other summaries from part of the episode. The partitioned summaries are listed below: {summaries} ###### The summary must describe the details in the story, like jokes, and details on what happens in the end with the key characters. Write a final summary based on the partitioned summaries in JSON format with the field 'summary' """ print('Final summarization') response = self.model.generate_content(prompt) return response.text episode_season = 5 episode_id = 92 X = (data[(data.episode_season == episode_season) & (data.episode_id == episode_id)].sort_values('number') ) X['line'] = (X['location_normalized_name'].fillna('') + ', ' + X['character_normalized_name'].fillna('') + ' said: ' + X['normalized_text'].fillna('') ) import os from dotenv import load_dotenv load_dotenv('../.env') summarizer = ChunkSummary( model_name = "gemini-1.5-flash", apikey = os.environ["GEMINI_KEY"], text = X['line'].tolist(), window_size = 40, overlap_size = 5 ) episode_summary = summarizer.summarize() Summarizing text Summarizing chunk 1 from 8 Summarizing chunk 2 from 8 Summarizing chunk 3 from 8 Summarizing chunk 4 from 8 Summarizing chunk 5 from 8 Summarizing chunk 6 from 8 Summarizing chunk 7 from 8 Summarizing chunk 8 from 8 Final summarization import json final_summmary = json.loads(episode_summary.replace("```json\n",'').replace("\n```",'')) final_summmary {'summary': 'The episode starts with a series of cat burglaries in Springfield, leaving the Simpsons family, including Ned Flanders, without their valuables. Homer, however, is more concerned about the loss of his "magic beans." The family discovers they are not insured because Homer spent the insurance money on magic beans. Chief Wiggum, more interested in rearranging crime scene markers than solving the case, fails to catch the culprit. Meanwhile, Professor Frink invents a high-tech security system that makes houses run away from burglars, but it ends up causing more problems than it solves. Ned Flanders organizes a neighborhood watch, with Homer taking charge and arming the group with guns, much to Marge\'s disapproval. Homer, Moe, Skinner, and Barney form a vigilante group called "The Cueballs" and get their first mission from Herman, who sells them a miniature A-bomb. Homer, feeling empowered, starts harassing people in the neighborhood, even going as far as to stop a boy from burning leaves without a permit. Jimbo Jones, impressed by Homer\'s power, asks to join the group, and Homer accepts him after he proves he can swing a sack of door knobs. Homer, after catching the cat burglar, starts to patrol the town with Moe and Jimbo, but their methods are questionable, leading to more crime than they prevent. Lisa criticizes Homer\'s actions, pointing out the abuse of power. The cat burglar calls into a local news show, taunting Homer and revealing his next target: the Springfield Museum\'s cubic zirconia. Homer vows to protect the museum, but his efforts are hindered by his own incompetence and his father\'s tendency to steal. Homer is fired from his job as a security guard at the Springfield Museum after failing to catch the cat burglar. Grampa Simpson, despite his age and frequent falls, notices Molloy, a resident of his retirement home, wearing sneakers, which he believes is a clue. He also remembers that Molloy was upset about Homer\'s age-bashing comments at the museum. Grampa\'s deductions lead him to believe that Molloy is the cat burglar. Homer and Grandpa confront Molloy at the retirement home, and Molloy returns all the stolen items, including Lisa\'s saxophone, and expresses his love for Springfield. However, Chief Wiggum arrests Molloy for breaking the law, despite the crowd\'s pleas to let him go. Molloy reveals he buried millions of dollars under a big "T" in Springfield, leading Homer, Chief Wiggum, and the rest of the town on a wild goose chase. They dig for hours but find nothing except a note from Molloy, who mocks them for their efforts. The episode ends with the town still searching for the treasure, despite Molloy\'s trickery.'} Exportação do Resultado import joblib import os os.makedirs('../data/results/') # Exportar dicionario com chunks, resumos dos chunks e do resumo final joblib.dump({ 'chunks': summarizer.chunks, 'chunks': summarizer.chunk_summaries, 'chunks': final_summmary, }, '../data/results/simpsons_episode_summary.joblib') Estimativa de Tokens import tiktoken # Função para estimar número de tokens def estimar_tokens(texto): encoder = tiktoken.get_encoding("cl100k_base") # Exemplo de codificação # encoder = tiktoken.get_encoding("gpt-4o") # Exemplo de codificação tokens = encoder.encode(texto) return tokens # Exemplo de uso texto = "Este é um exemplo de texto para calcular o número de tokens." # texto = "Hello world aaaaa" tokens = estimar_tokens(texto) print(f"Número de tokens: {len(tokens)}") tokens Número de tokens: 13 [44090, 4046, 4543, 80694, 409, 33125, 3429, 97627, 297, 31311, 409, 11460, 13] X = data.dropna(subset='normalized_text').copy() X['n_tokens'] = X.normalized_text.fillna('').apply(lambda x: len(estimar_tokens(x))) X.shape (132087, 31) X.groupby('episode_id').n_tokens.sum().plot.hist(bins=100) <Axes: ylabel='Frequency'> X.n_tokens.plot.hist(bins=100) <Axes: ylabel='Frequency'> encoder = tiktoken.get_encoding("cl100k_base") # Exemplo de codificação encoder.decode(tokens) 'Este é um exemplo de texto para calcular o número de tokens.' Role Prompting from openai import OpenAI # OPENAI_KEY é uma chave que deve ser colocada no aquivo .env na raiz do notebook client = OpenAI( api_key=os.environ['OPENAI_KEY'] ) def gerar_resposta(prompt, papel): prompt_final = f"Seja um {papel} e responda: {prompt}" completion = client.chat.completions.create( model="gpt-4o", messages=[ # {"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": prompt_final} ], stream=False, ) return completion.choices[0].message.content.strip() # Exemplos de uso print(gerar_resposta("Explique o impacto da Revolução Industrial.", "historiador")) print(gerar_resposta("Dê sugestões para engajamento nas redes sociais.", "especialista em marketing")) A Revolução Industrial, que teve início na Inglaterra em meados do século XVIII, trouxe transformações profundas e duradouras em praticamente todos os aspectos da sociedade. O impacto desta revolução pode ser analisado em diferentes esferas: 1. **Econômico**: A Revolução Industrial marcou a transição de economias agrárias para economias industriais. Houve um aumento massivo na produção de bens, graças à introdução de máquinas e novas técnicas de produção, como a linha de montagem. Isso levou a um crescimento econômico significativo e à formação de grandes centros urbanos ao redor das fábricas. 2. **Social**: A urbanização rápida mudou a estrutura social. As cidades cresceram rapidamente, muitas vezes sem planejamento adequado, levando a condições de vida precárias para muitos trabalhadores. O surgimento de uma nova classe trabalhadora industrial foi concomitante ao estabelecimento e enriquecimento da burguesia industrial. Além disso, a Revolução Industrial fomentou o sindicalismo e movimentos trabalhistas, que buscavam melhores condições de trabalho e direitos para os trabalhadores. 3. **Tecnológico**: Introduziu inovações tecnológicas sem precedentes, como o motor a vapor, o tear mecânico e, mais tarde, a eletricidade e o motor de combustão interna. Essas inovações não apenas aumentaram a produtividade, mas também criaram novas indústrias e oportunidades de emprego. 4. **Cultural**: Aumento da alfabetização e maior valorização da educação técnica e científica para suprir as necessidades da nova economia industrial. As mudanças econômicas e sociais aceleraram o ritmo de transformação cultural. 5. **Ambiental**: O aumento da produção industrial trouxe consigo efeitos ambientais significativos, como a poluição do ar e da água e a exploração intensiva de recursos naturais, consequências que continuam a ser desafios nos dias atuais. 6. **Político**: Houve mudanças na distribuição do poder político, com o crescimento da influência econômica da classe industrial. As demandas por direitos e representatividade política cresceram, impulsionando reformas sociais e políticas. Em suma, a Revolução Industrial foi um marco na história da humanidade que moldou o mundo moderno, desencadeando uma série de mudanças profundas que continuam a ter consequências no presente. As suas reverberações podem ser vistas na estrutura econômica global, nas relações sociais e nos desafios ambientais enfrentados hoje. Certamente! Engajar o público nas redes sociais é essencial para fomentar um relacionamento sólido, aumentar a visibilidade da marca e, eventualmente, converter seguidores em clientes. Aqui estão algumas sugestões para aumentar o engajamento: 1. **Conteúdo Visual**: Utilize imagens atraentes, vídeos curtos, infográficos e GIFs para capturar a atenção do seu público rapidamente. Conteúdos visuais tendem a gerar mais engajamento do que posts apenas de texto. 2. **Stories e Reels**: Aproveite as ferramentas de Stories e Reels no Instagram e Facebook para compartilhar conteúdo autêntico e dinâmico que desaparece após 24 horas, criando um senso de urgência. 3. **Interatividade**: Inclua enquetes, quizzes e perguntas nos seus posts ou stories. Isso não apenas incentiva o engajamento, mas também oferece insights valiosos sobre seu público. 4. **Conteúdo Gerado pelo Usuário**: Incentive seus seguidores a criar e compartilhar conteúdo relacionado à sua marca. Isso pode incluir concursos ou hashtags de campanha, mostrando a autenticidade e comprovando o valor da marca. 5. **Lives**: Realize transmissões ao vivo para interagir diretamente com seu público. Isso pode incluir demonstrações de produtos, sessões de Perguntas e Respostas ou bastidores da empresa. 6. **Conteúdo Relevante e Atualizado**: Comente sobre tendências atuais ou eventos relevantes para sua indústria. Isso demonstra que sua marca está antenada no que está acontecendo no mundo. 7. **Parcerias e Colaborações**: Colabore com influenciadores ou outras marcas que compartilhem dos mesmos valores. Isso ajuda a alcançar um público mais amplo e construir credibilidade. 8. **Conteúdo de Valor**: Ofereça dicas, tutoriais ou insights que agreguem valor real ao seu público. Isso pode estabelecer sua marca como autoridade em seu setor e promover o compartilhamento do seu conteúdo. 9. **Consistência na Postagem**: Mantenha uma frequência regular de postagens para estar sempre presente no feed do seu público. Utilize um calendário de conteúdo para planejar postagens com antecedência. 10. **Humanização da Marca**: Mostre o lado humano da sua marca. Compartilhe histórias de funcionários, valores da empresa ou projetos sociais. 11. **Respondendo Comentários e Mensagens**: Engaje-se ativamente com seu público respondendo a comentários e mensagens de forma rápida e autêntica. Isso cria um senso de comunidade e reforça a lealdade à marca. 12. **Utilização de Hashtags**: Crie e use hashtags relevantes para aumentar a descoberta dos seus posts. Explore tanto hashtags populares quanto específicas para captar diferentes audiências. Adotar uma abordagem estratégica que combine várias dessas táticas pode ajudar a aumentar significativamente o engajamento nas redes sociais. Exercício Tarefa 1 - Role prompting para agentes personalizados: Criar Agentes de IA que possam interpretar os personagens na interação com o usuário. Tarefa 2 - Recriação de episódios: Simular os editores dos Simpsons, criando novos diálogos e fim para os episódios. Tarefa 3 - Assistente de conteúdo: Interface para QA com um assistente especialista em Simpsons. Tarefa 1 - Role prompting para agentes personalizados: Criar Agentes de IA que possam interpretar os personagens na interação com o usuário. # Criar prompt base para fazer o role prompting do personagem # Selecionar frases do personagem e estruturar como listas # Apendar os dados ao prompt base # Estimar o tamanho de tokens e limitar se necessario # persona = 'bart simpson' persona_lines = data[(data.character_normalized_name == persona)].sample(200).normalized_text.dropna().tolist() persona_lines = '-' + '\n-'.join(persona_lines) print(len(estimar_tokens(persona_lines))) examples = """ - What is your favorite joke? - Which class do you prefer in school? - Who is your best friend? """ prompt_base = f""" You are an actor/actress that will play a character in The Simpsons show. You are {persona}! In order to improve you acting, we have selected some <key phrases> from you character in the show. When you interact with the user, you must respond with sentences from the <key phrases> list. ***do not create new content***. If the response requires other sentences, respond with "I do not know". You will interact with The Simpsons Fans in a chat. They might ask questions from specific episodes or seasons, as well as you interaction with other characeters. You can check <examples> from other users questions. <key phrases> {persona_lines} <examples> {examples} """ # prompt_base = f""" # You are an actor/actress that will play a character in The Simpsons show. You are {persona}! # In order to improve you acting, we have selected some <key phrases> from you character in the show. # You will interact with The Simpsons Fans in a chat. They might ask questions from specific episodes or seasons, # as well as you interaction with other characeters. You can check <examples> from other users questions. # <key phrases> # {persona_lines} # <examples> # {examples} # """ print(len(estimar_tokens(prompt_base))) genai.configure(api_key=os.environ["GEMINI_KEY"]) generation_config = { 'temperature': 0.6, 'top_p': 0.8, 'top_k': 20, 'max_output_tokens': 100 } from google.generativeai.types import HarmCategory, HarmBlockThreshold safety_settings={ HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE, } model = genai.GenerativeModel( "gemini-1.5-flash", system_instruction=prompt_base, generation_config = generation_config, safety_settings=safety_settings ) history =[] while True: chat_session = model.start_chat( history=[] ) user_prompt = input() # print(user_prompt) message = { 'role': 'user', 'parts': [user_prompt] } response = chat_session.send_message(user_prompt) history.append({ 'role':'model', 'parts': [response.text] }) print(response.text) 2162 2327 I'm Bart Simpson. You mean Milhouse, funny little guy, afraid of the dark and the light now I got new friends, guys who get me. I love it max I do not know. I do not know.




Códigos em python para sumarização de textos com LLMS import google.generativeai as genai from google.generativeai.types import HarmCategory, HarmBlockThreshold class ChunkSummary(): def __init__(self, model_name, apikey, text, window_size, overlap_size, system_prompt, generation_config=None): self.text = text if isinstance(self.text, str): self.text = [self.text] self.window_size = window_size self.overlap_size = overlap_size # Aplicacao dos chunks e criacao do modelo self.chunks = self.__text_to_chunks() self.model = self.__create_model(apikey, model_name, system_prompt, generation_config) def __create_model(self, apikey, model_name, system_prompt, generation_config=None): genai.configure(api_key=apikey) safety_settings={ HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE, } if generation_config is None: generation_config = { 'temperature': 0.2, 'top_p': 0.8, 'top_k': 20, 'max_output_tokens': 1000 } return genai.GenerativeModel( model_name, system_instruction=system_prompt, generation_config = generation_config, safety_settings=safety_settings ) def __text_to_chunks(self): n = self.window_size # Tamanho de cada chunk m = self.overlap_size # overlap entre chunks return [self.text[i:i+n] for i in range(0, len(self.text), n-m)] def __create_chunk_prompt(self, chunk, prompt_user): episode_lines = '\n'.join(chunk) prompt = f""" Summarize each chunk in order to attend to the # USER interaction: # USER {prompt_user} # OUTPUT INSTRUCTION The summary output must be written as a plain JSON with field 'summary'. ###### CHUNK {episode_lines} ###### Summarize it. """ return prompt def __summarize_chunks(self, prompt_user): # Loop over chunks chunk_summaries = [] for i, chunk in enumerate(self.chunks): print(f'Summarizing chunk {i+1} from {len(self.chunks)}') # Create prompt prompt = self.__create_chunk_prompt(chunk, prompt_user) response = self.model.generate_content(prompt) # Apendar resposta do chunk chunk_summaries.append(response.text) # if i == 4: break return chunk_summaries def summarize(self, prompt_user): print('Summarizing text') # Chamar o sumario dos chunks self.chunk_summaries = self.__summarize_chunks(prompt_user) # Prompt final summaries = [f"- {x}\n" for x in self.chunk_summaries] prompt_summary = f""" # User: {prompt_user} ### chunk summaries {summaries} ### Attend to the # User considering the information in ### chunk summaries. Write the output in JSON format with a field 'assistant'. """ print('Interacting') response = self.model.generate_content(prompt_summary) return response.text





